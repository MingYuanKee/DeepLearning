{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLPM1G2oyK_V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eec0ecf2-a856-4e4f-8d12-7ff7f6177bc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/drive/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TShDSRqUnXHj"
      },
      "outputs": [],
      "source": [
        "#!cp ./drive/MyDrive/temp/outdoor_train.zip .\n",
        "\n",
        "!rm -rf reside-mix/\n",
        "!unzip  ./drive/MyDrive/temp/outdoor_train.zip -d ./reside-mix/ > log.txt\n",
        "!unzip  ./drive/MyDrive/temp/outdoor_test.zip -d ./reside-mix/ > testlog.txt\n",
        "\n",
        "!unzip  ./drive/MyDrive/temp/indoor_train.zip -d ./reside-mix/ > log.txt\n",
        "!unzip  ./drive/MyDrive/temp/indoor_test.zip -d ./reside-mix/ > testlog.txt\n",
        "\n",
        "\n",
        "#!cp -rf ./drive/MyDrive/reside-outdoor/ ./\n",
        "#path = './reside-mix/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf SOTS_data/\n",
        "!unzip  ./drive/MyDrive/temp/SOTS.zip -d ./SOTS_data/ > log.txt"
      ],
      "metadata": {
        "id": "oyFGavyjT2uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip  ./drive/MyDrive/temp/outdoor_test.zip -d ./reside-outdoor/ > testlog.txt"
      ],
      "metadata": {
        "id": "BtvpwVHfd3Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = './SOTS_data/SOTS/outdoor/'"
      ],
      "metadata": {
        "id": "vq4AFrc0XfaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "directory = r'/content/reside-mix/train/gt/'\n",
        "files = os.listdir(directory)\n",
        "# Then you rename the files\n",
        "for file_name in files:\n",
        "    # You give the full path of the file\n",
        "    old_name = os.path.join(directory, file_name)\n",
        "    # You CHANGE the extension\n",
        "    new_name = old_name.replace('.jpg', '.png')\n",
        "    os.rename(old_name, new_name)"
      ],
      "metadata": {
        "id": "gG-ESQ4KHFaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVwQ0ZIsRLEG",
        "outputId": "5a36fbe3-d245-4293-90bf-fc323497001d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_msssim\n",
            "  Downloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pytorch_msssim) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->pytorch_msssim) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->pytorch_msssim) (1.3.0)\n",
            "Installing collected packages: pytorch_msssim\n",
            "Successfully installed pytorch_msssim-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_msssim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrk7IMUeRzlk",
        "outputId": "7b1f6162-362b-46e4-b721-fcc734091605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting warmup-scheduler\n",
            "  Downloading warmup_scheduler-0.3.tar.gz (2.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: warmup-scheduler\n",
            "  Building wheel for warmup-scheduler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warmup-scheduler: filename=warmup_scheduler-0.3-py3-none-any.whl size=2968 sha256=2809b9f48ad1504b08d176472146d20fd943b9ec32f020611d6d9e7581c08f0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/01/9e/d1820991c32916e9808c940f572b462f3e46427f3e76c4d852\n",
            "Successfully built warmup-scheduler\n",
            "Installing collected packages: warmup-scheduler\n",
            "Successfully installed warmup-scheduler-0.3\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install warmup-scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h78ryOOBvc2"
      },
      "source": [
        "Data Augment\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3z9B1WKSCD5O"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as FUNCTIONAL\n",
        "\n",
        "\n",
        "class PairRandomCrop(transforms.RandomCrop):\n",
        "\n",
        "    def __call__(self, image, label):\n",
        "\n",
        "        if self.padding is not None:\n",
        "            image = FUNCTIONAL.pad(image, self.padding, self.fill, self.padding_mode)\n",
        "            label = FUNCTIONAL.pad(label, self.padding, self.fill, self.padding_mode)\n",
        "\n",
        "        # pad the width if needed\n",
        "        if self.pad_if_needed and image.size[0] < self.size[1]:\n",
        "            image = FUNCTIONAL.pad(image, (self.size[1] - image.size[0], 0), self.fill, self.padding_mode)\n",
        "            label = FUNCTIONAL.pad(label, (self.size[1] - label.size[0], 0), self.fill, self.padding_mode)\n",
        "        # pad the height if needed\n",
        "        if self.pad_if_needed and image.size[1] < self.size[0]:\n",
        "            image = FUNCTIONAL.pad(image, (0, self.size[0] - image.size[1]), self.fill, self.padding_mode)\n",
        "            label = FUNCTIONAL.pad(label, (0, self.size[0] - image.size[1]), self.fill, self.padding_mode)\n",
        "\n",
        "        i, j, h, w = self.get_params(image, self.size)\n",
        "\n",
        "        return FUNCTIONAL.crop(image, i, j, h, w), FUNCTIONAL.crop(label, i, j, h, w)\n",
        "\n",
        "\n",
        "class PairCompose(transforms.Compose):\n",
        "    def __call__(self, image, label):\n",
        "        for t in self.transforms:\n",
        "            image, label = t(image, label)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "class PairRandomHorizontalFilp(transforms.RandomHorizontalFlip):\n",
        "    def __call__(self, img, label):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image): Image to be flipped.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image: Randomly flipped image.\n",
        "        \"\"\"\n",
        "        if random.random() < self.p:\n",
        "            return FUNCTIONAL.hflip(img), FUNCTIONAL.hflip(label)\n",
        "        return img, label\n",
        "\n",
        "\n",
        "class PairToTensor(transforms.ToTensor):\n",
        "    def __call__(self, pic, label):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Converted image.\n",
        "        \"\"\"\n",
        "        return FUNCTIONAL.to_tensor(pic), FUNCTIONAL.to_tensor(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik-THSPrBomo"
      },
      "source": [
        "Data Loader\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rFIJk2JBsbA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image as Image\n",
        "from torchvision.transforms import functional as Functional\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms.functional import to_tensor, resize\n",
        "\n",
        "def my_collate_fn(batch):\n",
        "    images, labels = zip(*batch)  # Unzipping images and labels\n",
        "    # Define the target size\n",
        "    target_height = 256  # Example height\n",
        "    target_width = 256   # Example width\n",
        "\n",
        "    # Resize and ensure all tensors have the same number of channels, in this case, 3\n",
        "    resized_images = [resize(img, (target_height, target_width))[:3, :, :] if img.shape[0] > 3 else resize(img, (target_height, target_width)) for img in images]\n",
        "    resized_labels = [resize(lbl, (target_height, target_width))[:3, :, :] if lbl.shape[0] > 3 else resize(lbl, (target_height, target_width)) for lbl in labels]\n",
        "\n",
        "    # Stack all images and labels\n",
        "    images = torch.stack(resized_images)\n",
        "    labels = torch.stack(resized_labels)\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "def test_collate_fn(batch):\n",
        "    images, labels, name = zip(*batch)\n",
        "    # Define the target size\n",
        "    target_height = 256  # Example height\n",
        "    target_width = 256   # Example width\n",
        "\n",
        "    # Resize and ensure all tensors have the same number of channels, in this case, 3\n",
        "    resized_images = [resize(img, (target_height, target_width))[:3, :, :] if img.shape[0] > 3 else resize(img, (target_height, target_width)) for img in images]\n",
        "    resized_labels = [resize(lbl, (target_height, target_width))[:3, :, :] if lbl.shape[0] > 3 else resize(lbl, (target_height, target_width)) for lbl in labels]\n",
        "\n",
        "    # Stack all images and labels\n",
        "    images = torch.stack(resized_images)\n",
        "    labels = torch.stack(resized_labels)\n",
        "\n",
        "    return images, labels, name\n",
        "\n",
        "\n",
        "def train_dataloader(path, batch_size=8, num_workers=0, use_transform=True): # we change batch size = 1, but the original batch size = 64\n",
        "    image_dir = os.path.join(path, 'train')\n",
        "\n",
        "    transform = None\n",
        "    if use_transform:\n",
        "        transform = PairCompose(\n",
        "            [\n",
        "                PairRandomCrop(256),\n",
        "                PairRandomHorizontalFilp(),\n",
        "                PairToTensor()\n",
        "            ]\n",
        "        )\n",
        "    dataloader = DataLoader(\n",
        "        DeblurDataset(image_dir, transform=transform),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=my_collate_fn,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def test_dataloader(path, batch_size=1, num_workers=0):\n",
        "    image_dir = os.path.join(path)\n",
        "    dataloader = DataLoader(\n",
        "        DeblurDataset(image_dir, is_test=True),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def valid_dataloader(path, batch_size=1, num_workers=0):\n",
        "    dataloader = DataLoader(\n",
        "        DeblurDataset(os.path.join(path, 'test', ), is_valid=True),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=my_collate_fn,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "class DeblurDataset_ex(Dataset):\n",
        "    def __init__(self, image_dir, transform=None, is_test=False):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_list = os.listdir(os.path.join(image_dir, 'hazy/'))\n",
        "        self._check_image(self.image_list)\n",
        "        self.image_list.sort()\n",
        "        self.transform = transform\n",
        "        self.is_test = is_test\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(os.path.join(self.image_dir, 'hazy', self.image_list[idx]))\n",
        "        label = Image.open(os.path.join(self.image_dir, 'gt', self.image_list[idx].split('_')[0]+'.png'))\n",
        "\n",
        "        if self.transform:\n",
        "            image, label = self.transform(image, label)\n",
        "        else:\n",
        "            image = Functional.to_tensor(image)\n",
        "            label = Functional.to_tensor(label)\n",
        "        if self.is_test:\n",
        "            name = self.image_list[idx]\n",
        "            return image, label, name\n",
        "        return image, label\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_image(lst):\n",
        "        for x in lst:\n",
        "            splits = x.split('.')\n",
        "            if splits[-1] not in ['png', 'jpg', 'jpeg']:\n",
        "                raise ValueError\n",
        "\n",
        "import random\n",
        "class DeblurDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None, is_test=False, is_valid=False, ps=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_list = os.listdir(os.path.join(image_dir, 'hazy/'))\n",
        "        self._check_image(self.image_list)\n",
        "        self.image_list.sort()\n",
        "        self.transform = transform\n",
        "        self.is_test = is_test\n",
        "        self.is_valid = is_valid\n",
        "        self.ps = ps\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(os.path.join(self.image_dir, 'hazy', self.image_list[idx])).convert('RGB')\n",
        "        if self.is_valid or self.is_test:\n",
        "            label = Image.open(os.path.join(self.image_dir, 'gt', self.image_list[idx].split('_')[0]+'.png')).convert('RGB')\n",
        "        else:\n",
        "            try:\n",
        "                label = Image.open(os.path.join(self.image_dir, 'gt', self.image_list[idx].split('_')[0]+'.jpg')).convert('RGB')\n",
        "            except:\n",
        "                label = Image.open(os.path.join(self.image_dir, 'gt', self.image_list[idx].split('_')[0]+'.png')).convert('RGB')\n",
        "        ps = self.ps\n",
        "\n",
        "        if self.ps is not None:\n",
        "            image = Functional.to_tensor(image)\n",
        "            label = Functional.to_tensor(label)\n",
        "\n",
        "            hh, ww = label.shape[1], label.shape[2]\n",
        "\n",
        "            rr = random.randint(0, hh-ps)\n",
        "            cc = random.randint(0, ww-ps)\n",
        "\n",
        "            image = image[:, rr:rr+ps, cc:cc+ps]\n",
        "            label = label[:, rr:rr+ps, cc:cc+ps]\n",
        "\n",
        "            if random.random() < 0.5:\n",
        "                image = image.flip(2)\n",
        "                label = label.flip(2)\n",
        "        else:\n",
        "            image = Functional.to_tensor(image)\n",
        "            label = Functional.to_tensor(label)\n",
        "\n",
        "        if self.is_test:\n",
        "            name = self.image_list[idx]\n",
        "            return image, label, name\n",
        "        return image, label\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_image(lst):\n",
        "        for x in lst:\n",
        "            splits = x.split('.')\n",
        "            if splits[-1] not in ['png', 'jpg', 'jpeg']:\n",
        "                raise ValueError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "V41ETfpGPTs8",
        "outputId": "716ef3a3-b107-4a5e-a6d6-2dd55ce08059"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-7c328b4209c0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-bac90ee99f82>\u001b[0m in \u001b[0;36mtrain_dataloader\u001b[0;34m(path, batch_size, num_workers, use_transform)\u001b[0m\n\u001b[1;32m     80\u001b[0m         )\n\u001b[1;32m     81\u001b[0m     dataloader = DataLoader(\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mDeblurDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-bac90ee99f82>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image_dir, transform, is_test, is_valid, ps)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hazy/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './SOTS_data/indoor/train/hazy/'"
          ]
        }
      ],
      "source": [
        "train_dataloader(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZHCSwIpNEIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c6e6990-c5b9-4dae-c007-9f7dcd613231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n"
          ]
        }
      ],
      "source": [
        "#print(len(train_dataloader(path)))\n",
        "print(len(test_dataloader(path)))\n",
        "\n",
        "#print(len(valid_dataloader(path)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOmdMPEPBcqU"
      },
      "source": [
        "Utils\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3KXVt8fBgN_"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Adder(object):\n",
        "    def __init__(self):\n",
        "        self.count = 0\n",
        "        self.num = float(0)\n",
        "\n",
        "    def reset(self):\n",
        "        self.count = 0\n",
        "        self.num = float(0)\n",
        "\n",
        "    def __call__(self, num):\n",
        "        self.count += 1\n",
        "        self.num += num\n",
        "\n",
        "    def average(self):\n",
        "        return self.num / self.count\n",
        "\n",
        "\n",
        "class Timer(object):\n",
        "    def __init__(self, option='s'):\n",
        "        self.tm = 0\n",
        "        self.option = option\n",
        "        if option == 's':\n",
        "            self.devider = 1\n",
        "        elif option == 'm':\n",
        "            self.devider = 60\n",
        "        else:\n",
        "            self.devider = 3600\n",
        "\n",
        "    def tic(self):\n",
        "        self.tm = time.time()\n",
        "\n",
        "    def toc(self):\n",
        "        return (time.time() - self.tm) / self.devider\n",
        "\n",
        "\n",
        "def check_lr(optimizer):\n",
        "    for i, param_group in enumerate(optimizer.param_groups):\n",
        "        lr = param_group['lr']\n",
        "    return lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuIwak2DBSmK"
      },
      "source": [
        "Eval\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQmJ-vK5BT4n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision.transforms import functional as F\n",
        "import numpy as np\n",
        "from skimage.metrics import peak_signal_noise_ratio\n",
        "import time\n",
        "from pytorch_msssim import ssim\n",
        "import torch.nn.functional as f\n",
        "\n",
        "from skimage import img_as_ubyte\n",
        "import cv2\n",
        "\n",
        "def average(self):\n",
        "    return self.num / self.count if self.count != 0 else 0  # or return None\n",
        "\n",
        "def _eval(model, args):\n",
        "    state_dict = torch.load(args.test_model)\n",
        "    model.load_state_dict(state_dict['model'])\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    dataloader = test_dataloader(args.data_dir, batch_size=1, num_workers=8)\n",
        "    torch.cuda.empty_cache()\n",
        "    adder = Adder()\n",
        "    model.eval()\n",
        "    factor = 32\n",
        "    with torch.no_grad():\n",
        "        psnr_adder = Adder()\n",
        "        ssim_adder = Adder()\n",
        "\n",
        "        for iter_idx, data in enumerate(dataloader):\n",
        "            if iter_idx > 500:\n",
        "                break\n",
        "            input_img, label_img, name = data\n",
        "\n",
        "            input_img = input_img.to(device)\n",
        "\n",
        "            h, w = input_img.shape[2], input_img.shape[3]\n",
        "            H, W = ((h+factor)//factor)*factor, ((w+factor)//factor*factor)\n",
        "            padh = H-h if h%factor!=0 else 0\n",
        "            padw = W-w if w%factor!=0 else 0\n",
        "            input_img = f.pad(input_img, (0, padw, 0, padh), 'reflect')\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "            tm = time.time()\n",
        "\n",
        "            pred = model(input_img)[2]\n",
        "            pred = pred[:,:,:h,:w]\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "            elapsed = time.time() - tm\n",
        "            adder(elapsed)\n",
        "\n",
        "            pred_clip = torch.clamp(pred, 0, 1)\n",
        "\n",
        "            pred_numpy = pred_clip.squeeze(0).cpu().numpy()\n",
        "            label_numpy = label_img.squeeze(0).cpu().numpy()\n",
        "\n",
        "\n",
        "            label_img = (label_img).cuda()\n",
        "            psnr_val = 10 * torch.log10(1 / f.mse_loss(pred_clip, label_img))\n",
        "            down_ratio = max(1, round(min(H, W) / 256))\n",
        "            ssim_val = ssim(f.adaptive_avg_pool2d(pred_clip, (int(H / down_ratio), int(W / down_ratio))),\n",
        "                            f.adaptive_avg_pool2d(label_img, (int(H / down_ratio), int(W / down_ratio))),\n",
        "                            data_range=1, size_average=False)\n",
        "            #print('%d iter PSNR_dehazing: %.2f ssim: %f' % (iter_idx + 1, psnr_val, ssim_val))\n",
        "            print('%d iter PSNR_dehazing: %.2f ssim: %f' % (iter_idx + 1, psnr_val, ssim_val))\n",
        "            ssim_adder(ssim_val)\n",
        "            if args.save_image:\n",
        "                save_name = os.path.join(args.result_dir, name[0])\n",
        "                pred_clip += 0.5 / 255\n",
        "                pred = F.to_pil_image(pred_clip.squeeze(0).cpu(), 'RGB')\n",
        "                pred.save(save_name)\n",
        "\n",
        "            psnr_mimo = peak_signal_noise_ratio(pred_numpy, label_numpy, data_range=1)\n",
        "            psnr_adder(psnr_val)\n",
        "\n",
        "            print('%d iter PSNR: %.2f time: %f' % (iter_idx + 1, psnr_mimo, elapsed))\n",
        "\n",
        "        print('==========================================================')\n",
        "        print('The average PSNR is %.2f dB' % (psnr_adder.average()))\n",
        "        print('The average SSIM is %.5f dB' % (ssim_adder.average()))\n",
        "\n",
        "        print(\"Average time: %f\" % adder.average())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiBJcTJbCOcj"
      },
      "source": [
        "Layers\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qOrc8BLCPaH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as fl\n",
        "import math\n",
        "\n",
        "class BasicConv(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size, stride, bias=True, norm=False, relu=True, transpose=False):\n",
        "        super(BasicConv, self).__init__()\n",
        "        if bias and norm:\n",
        "            bias = False\n",
        "\n",
        "        padding = kernel_size // 2\n",
        "        layers = list()\n",
        "        if transpose:\n",
        "            padding = kernel_size // 2 -1\n",
        "            layers.append(nn.ConvTranspose2d(in_channel, out_channel, kernel_size, padding=padding, stride=stride, bias=bias))\n",
        "        else:\n",
        "            layers.append(\n",
        "                nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding, stride=stride, bias=bias))\n",
        "        if norm:\n",
        "            layers.append(nn.BatchNorm2d(out_channel))\n",
        "        if relu:\n",
        "            layers.append(nn.GELU())\n",
        "        self.main = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, filter=False):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            BasicConv(in_channel, out_channel, kernel_size=3, stride=1, relu=True),\n",
        "            DeepPoolLayer(in_channel, out_channel) if filter else nn.Identity(),\n",
        "            BasicConv(out_channel, out_channel, kernel_size=3, stride=1, relu=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x) + x\n",
        "\n",
        "\n",
        "class DeepPoolLayer(nn.Module):\n",
        "    def __init__(self, k, k_out):\n",
        "        super(DeepPoolLayer, self).__init__()\n",
        "        self.pools_sizes = [8,4,2]\n",
        "        pools, convs, dynas = [],[],[]\n",
        "        for i in self.pools_sizes:\n",
        "            pools.append(nn.AvgPool2d(kernel_size=i, stride=i))\n",
        "            convs.append(nn.Conv2d(k, k, 3, 1, 1, bias=False))\n",
        "            dynas.append(dynamic_filter(inchannels=k, kernel_size=3))\n",
        "        self.pools = nn.ModuleList(pools)\n",
        "        self.convs = nn.ModuleList(convs)\n",
        "        self.dynas = nn.ModuleList(dynas)\n",
        "        self.relu = nn.GELU()\n",
        "        self.conv_sum = nn.Conv2d(k, k_out, 3, 1, 1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_size = x.size()\n",
        "        resl = x\n",
        "        for i in range(len(self.pools_sizes)):\n",
        "            if i == 0:\n",
        "                y = self.dynas[i](self.convs[i](self.pools[i](x)))\n",
        "            else:\n",
        "                y = self.dynas[i](self.convs[i](self.pools[i](x)+y_up))\n",
        "            resl = torch.add(resl, fl.interpolate(y, x_size[2:], mode='bilinear', align_corners=True))\n",
        "            if i != len(self.pools_sizes)-1:\n",
        "                y_up = fl.interpolate(y, scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        resl = self.relu(resl)\n",
        "        resl = self.conv_sum(resl)\n",
        "\n",
        "        return resl\n",
        "\n",
        "class dynamic_filter(nn.Module):\n",
        "    def __init__(self, inchannels, kernel_size=3, stride=1, group=8):\n",
        "        super(dynamic_filter, self).__init__()\n",
        "\n",
        "        self.stride = stride\n",
        "        self.kernel_size = kernel_size\n",
        "        self.group = group\n",
        "\n",
        "        self.conv = nn.Conv2d(inchannels, group*kernel_size**2, kernel_size=1, stride=1, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(group*kernel_size**2)\n",
        "        self.act = nn.Tanh()\n",
        "\n",
        "        nn.init.kaiming_normal_(self.conv.weight, mode='fan_out', nonlinearity='relu')\n",
        "        self.lamb_l = nn.Parameter(torch.zeros(inchannels), requires_grad=True)\n",
        "        self.lamb_h = nn.Parameter(torch.zeros(inchannels), requires_grad=True)\n",
        "        self.pad = nn.ReflectionPad2d(kernel_size//2)\n",
        "\n",
        "        self.ap = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.inside_all = nn.Parameter(torch.zeros(inchannels,1,1), requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity_input = x\n",
        "        # the Conv_{3x3} layer in eq.3 is included in DeepPoolLayer.convs\n",
        "        low_filter = self.ap(x)\n",
        "        low_filter = self.conv(low_filter)\n",
        "        low_filter = self.bn(low_filter)\n",
        "\n",
        "        n, c, h, w = x.shape\n",
        "        x = fl.unfold(self.pad(x), kernel_size=self.kernel_size).reshape(n, self.group, c//self.group, self.kernel_size**2, h*w)\n",
        "\n",
        "        n,c1,p,q = low_filter.shape\n",
        "        low_filter = low_filter.reshape(n, c1//self.kernel_size**2, self.kernel_size**2, p*q).unsqueeze(2)\n",
        "        low_filter = self.act(low_filter)\n",
        "        low_part = torch.sum(x * low_filter, dim=3).reshape(n, c, h, w)\n",
        "\n",
        "        # the variables here are slightly different from the paper: (code) --> (paper)\n",
        "        # low_filter --> A (eq.3)\n",
        "        # In Eq.7, X*A'= X*(A_{l} + WA_{h})\n",
        "        #              = X*A_{l} + WX*(A - A_{l})\n",
        "        #              = X*A_{l} + WX*A - WX*A_{l}\n",
        "        #              = WX*A - X*A_{l}(W-1)\n",
        "        # we substitute gap for A_{l} for simplicity, which is a coarser low-frequency filter\n",
        "        out_low = low_part * (self.inside_all + 1.) - self.inside_all * self.gap(identity_input)\n",
        "\n",
        "        out_low = out_low * self.lamb_l[None,:,None,None]\n",
        "        out_high = (identity_input) * (self.lamb_h[None,:,None,None] + 1.)\n",
        "\n",
        "        return out_low + out_high"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muFr7px4CKua"
      },
      "source": [
        "IRNeXt\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNdQN8JdCMEh"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as ff\n",
        "\n",
        "class EBlock(nn.Module):\n",
        "    def __init__(self, out_channel, num_res=8):\n",
        "        super(EBlock, self).__init__()\n",
        "\n",
        "        layers = [ResBlock(out_channel, out_channel) for _ in range(num_res-1)]\n",
        "        layers.append(ResBlock(out_channel, out_channel, filter=True))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class DBlock(nn.Module):\n",
        "    def __init__(self, channel, num_res=8):\n",
        "        super(DBlock, self).__init__()\n",
        "\n",
        "        layers = [ResBlock(channel, channel) for _ in range(num_res-1)]\n",
        "        layers.append(ResBlock(channel, channel, filter=True))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class SCM(nn.Module):\n",
        "    def __init__(self, out_plane):\n",
        "        super(SCM, self).__init__()\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            BasicConv(3, out_plane//4, kernel_size=3, stride=1, relu=True),\n",
        "            BasicConv(out_plane // 4, out_plane // 2, kernel_size=1, stride=1, relu=True),\n",
        "            BasicConv(out_plane // 2, out_plane // 2, kernel_size=3, stride=1, relu=True),\n",
        "            BasicConv(out_plane // 2, out_plane, kernel_size=1, stride=1, relu=False),\n",
        "            nn.InstanceNorm2d(out_plane, affine=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.main(x)\n",
        "        return x\n",
        "\n",
        "class FAM(nn.Module):\n",
        "    def __init__(self, channel):\n",
        "        super(FAM, self).__init__()\n",
        "\n",
        "        self.merge = BasicConv(channel*2, channel, kernel_size=3, stride=1, relu=False)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        return self.merge(torch.cat([x1, x2], dim=1))\n",
        "\n",
        "class IRNeXt(nn.Module):\n",
        "    def __init__(self, num_res=4):\n",
        "        super(IRNeXt, self).__init__()\n",
        "\n",
        "        base_channel = 32\n",
        "        self.Encoder = nn.ModuleList([\n",
        "            EBlock(base_channel, num_res),\n",
        "            EBlock(base_channel*2, num_res),\n",
        "            EBlock(base_channel*4, num_res),\n",
        "        ])\n",
        "\n",
        "        self.feat_extract = nn.ModuleList([\n",
        "            BasicConv(3, base_channel, kernel_size=3, relu=True, stride=1),\n",
        "            BasicConv(base_channel, base_channel*2, kernel_size=3, relu=True, stride=2),\n",
        "            BasicConv(base_channel*2, base_channel*4, kernel_size=3, relu=True, stride=2),\n",
        "            BasicConv(base_channel*4, base_channel*2, kernel_size=4, relu=True, stride=2, transpose=True),\n",
        "            BasicConv(base_channel*2, base_channel, kernel_size=4, relu=True, stride=2, transpose=True),\n",
        "            BasicConv(base_channel, 3, kernel_size=3, relu=False, stride=1)\n",
        "        ])\n",
        "\n",
        "        self.Decoder = nn.ModuleList([\n",
        "            DBlock(base_channel * 4, num_res),\n",
        "            DBlock(base_channel * 2, num_res),\n",
        "            DBlock(base_channel, num_res)\n",
        "        ])\n",
        "\n",
        "        self.Convs = nn.ModuleList([\n",
        "            BasicConv(base_channel * 4, base_channel * 2, kernel_size=1, relu=True, stride=1),\n",
        "            BasicConv(base_channel * 2, base_channel, kernel_size=1, relu=True, stride=1),\n",
        "        ])\n",
        "\n",
        "        self.ConvsOut = nn.ModuleList(\n",
        "            [\n",
        "                BasicConv(base_channel * 4, 3, kernel_size=3, relu=False, stride=1),\n",
        "                BasicConv(base_channel * 2, 3, kernel_size=3, relu=False, stride=1),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.FAM1 = FAM(base_channel * 4)\n",
        "        self.SCM1 = SCM(base_channel * 4)\n",
        "        self.FAM2 = FAM(base_channel * 2)\n",
        "        self.SCM2 = SCM(base_channel * 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_2 = ff.interpolate(x, scale_factor=0.5)\n",
        "        x_4 = ff.interpolate(x_2, scale_factor=0.5)\n",
        "        z2 = self.SCM2(x_2)\n",
        "        z4 = self.SCM1(x_4)\n",
        "\n",
        "        outputs = list()\n",
        "        # 256\n",
        "        x_ = self.feat_extract[0](x)\n",
        "        res1 = self.Encoder[0](x_)\n",
        "        # 128\n",
        "        z = self.feat_extract[1](res1)\n",
        "        z = self.FAM2(z, z2)\n",
        "        res2 = self.Encoder[1](z)\n",
        "        # 64\n",
        "        z = self.feat_extract[2](res2)\n",
        "        z = self.FAM1(z, z4)\n",
        "        z = self.Encoder[2](z)\n",
        "\n",
        "        z = self.Decoder[0](z)\n",
        "        z_ = self.ConvsOut[0](z)\n",
        "        # 128\n",
        "        z = self.feat_extract[3](z)\n",
        "        outputs.append(z_+x_4)\n",
        "\n",
        "        z = torch.cat([z, res2], dim=1)\n",
        "        z = self.Convs[0](z)\n",
        "        z = self.Decoder[1](z)\n",
        "        z_ = self.ConvsOut[1](z)\n",
        "        # 256\n",
        "        z = self.feat_extract[4](z)\n",
        "        outputs.append(z_+x_2)\n",
        "\n",
        "        z = torch.cat([z, res1], dim=1)\n",
        "        z = self.Convs[1](z)\n",
        "        z = self.Decoder[2](z)\n",
        "        z = self.feat_extract[5](z)\n",
        "        outputs.append(z+x)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "def build_net():\n",
        "    return IRNeXt()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ_iPV8xBg0K"
      },
      "source": [
        "Valid\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfAK2d13Bjq9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.transforms import functional as F\n",
        "import os\n",
        "from skimage.metrics import peak_signal_noise_ratio\n",
        "import torch.nn.functional as fv\n",
        "\n",
        "\n",
        "def _valid(model, args, ep):\n",
        "    print('Load test data')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    its = valid_dataloader(args.data_dir, batch_size=1, num_workers=0)\n",
        "    model.eval()\n",
        "    psnr_adder = Adder()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        print('Start Evaluation')\n",
        "        factor = 32\n",
        "        for idx, data in enumerate(its):\n",
        "            input_img, label_img = data\n",
        "            input_img = input_img.to(device)\n",
        "\n",
        "            h, w = input_img.shape[2], input_img.shape[3]\n",
        "            H, W = ((h+factor)//factor)*factor, ((w+factor)//factor*factor)\n",
        "            padh = H-h if h%factor!=0 else 0\n",
        "            padw = W-w if w%factor!=0 else 0\n",
        "            input_img = fv.pad(input_img, (0, padw, 0, padh), 'reflect')\n",
        "\n",
        "            if not os.path.exists(os.path.join(args.result_dir, '%d' % (ep))):\n",
        "                os.mkdir(os.path.join(args.result_dir, '%d' % (ep)))\n",
        "\n",
        "            pred = model(input_img)[2]\n",
        "            pred = pred[:,:,:h,:w]\n",
        "\n",
        "            pred_clip = torch.clamp(pred, 0, 1)\n",
        "            p_numpy = pred_clip.squeeze(0).cpu().numpy()\n",
        "            label_numpy = label_img.squeeze(0).cpu().numpy()\n",
        "\n",
        "            psnr = peak_signal_noise_ratio(p_numpy, label_numpy, data_range=1)\n",
        "\n",
        "            psnr_adder(psnr)\n",
        "            print('\\r%03d'%idx, end=' ')\n",
        "\n",
        "    print('\\n')\n",
        "    model.train()\n",
        "    return psnr_adder.average()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n87ZbXeBapS"
      },
      "source": [
        "TRAIN\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt9V7QBGBcGV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn.functional as ft\n",
        "import torch.nn as nn\n",
        "from warmup_scheduler import GradualWarmupScheduler\n",
        "save_dir = '/content/drive/MyDrive/results'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "def _train(model, args):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    criterion = torch.nn.L1Loss().to(device)\n",
        "    print(\"training LR:\", args.learning_rate)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, betas=(0.9, 0.999), eps=1e-8)\n",
        "    dataloader = train_dataloader(args.data_dir, args.batch_size, args.num_worker)\n",
        "    max_iter = len(dataloader)\n",
        "    warmup_epochs=3\n",
        "    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.num_epoch-warmup_epochs, eta_min=1e-6)\n",
        "    scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=warmup_epochs, after_scheduler=scheduler_cosine)\n",
        "    scheduler.step()\n",
        "    epoch = 1\n",
        "    if args.resume:\n",
        "        state = torch.load(args.resume)\n",
        "        epoch = state['epoch']\n",
        "        optimizer.load_state_dict(state['optimizer'])\n",
        "        model.load_state_dict(state['model'])\n",
        "        print('Resume from %d'%epoch)\n",
        "        epoch += 1\n",
        "        for i in range(epoch-1):\n",
        "          scheduler.step()\n",
        "\n",
        "    writer = SummaryWriter()\n",
        "    epoch_pixel_adder = Adder()\n",
        "    epoch_fft_adder = Adder()\n",
        "    iter_pixel_adder = Adder()\n",
        "    iter_fft_adder = Adder()\n",
        "    epoch_timer = Timer('m')\n",
        "    iter_timer = Timer('m')\n",
        "    best_psnr=-1\n",
        "    epoch_floss = []\n",
        "    epoch_ploss = []\n",
        "    epoch_psnr = []\n",
        "\n",
        "    for epoch_idx in range(epoch, args.num_epoch + 1):\n",
        "\n",
        "        epoch_timer.tic()\n",
        "        iter_timer.tic()\n",
        "        for iter_idx, batch_data in enumerate(dataloader):\n",
        "            input_img, label_img = batch_data\n",
        "            input_img = input_img.to(device)\n",
        "            label_img = label_img.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pred_img = model(input_img)\n",
        "            label_img2 = ft.interpolate(label_img, scale_factor=0.5, mode='bilinear')\n",
        "            label_img4 = ft.interpolate(label_img, scale_factor=0.25, mode='bilinear')\n",
        "            l1 = criterion(pred_img[0], label_img4)\n",
        "            l2 = criterion(pred_img[1], label_img2)\n",
        "            l3 = criterion(pred_img[2], label_img)\n",
        "            loss_content = l1+l2+l3\n",
        "\n",
        "            label_fft1 = torch.fft.fft2(label_img4, dim=(-2,-1))\n",
        "            label_fft1 = torch.stack((label_fft1.real, label_fft1.imag), -1)\n",
        "\n",
        "            pred_fft1 = torch.fft.fft2(pred_img[0], dim=(-2,-1))\n",
        "            pred_fft1 = torch.stack((pred_fft1.real, pred_fft1.imag), -1)\n",
        "\n",
        "            label_fft2 = torch.fft.fft2(label_img2, dim=(-2,-1))\n",
        "            label_fft2 = torch.stack((label_fft2.real, label_fft2.imag), -1)\n",
        "\n",
        "            pred_fft2 = torch.fft.fft2(pred_img[1], dim=(-2,-1))\n",
        "            pred_fft2 = torch.stack((pred_fft2.real, pred_fft2.imag), -1)\n",
        "\n",
        "            label_fft3 = torch.fft.fft2(label_img, dim=(-2,-1))\n",
        "            label_fft3 = torch.stack((label_fft3.real, label_fft3.imag), -1)\n",
        "\n",
        "            pred_fft3 = torch.fft.fft2(pred_img[2], dim=(-2,-1))\n",
        "            pred_fft3 = torch.stack((pred_fft3.real, pred_fft3.imag), -1)\n",
        "\n",
        "            f1 = criterion(pred_fft1, label_fft1)\n",
        "            f2 = criterion(pred_fft2, label_fft2)\n",
        "            f3 = criterion(pred_fft3, label_fft3)\n",
        "            loss_fft = f1+f2+f3\n",
        "\n",
        "            loss = loss_content + 0.1 * loss_fft\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.001)\n",
        "            optimizer.step()\n",
        "\n",
        "            iter_pixel_adder(loss_content.item())\n",
        "            iter_fft_adder(loss_fft.item())\n",
        "\n",
        "            epoch_pixel_adder(loss_content.item())\n",
        "            epoch_fft_adder(loss_fft.item())\n",
        "\n",
        "            if (iter_idx + 1) % args.print_freq == 0:\n",
        "                print(\"Time: %7.4f Epoch: %03d Iter: %4d/%4d LR: %.10f Loss content: %7.4f Loss fft: %7.4f\" % (\n",
        "                    iter_timer.toc(), epoch_idx, iter_idx + 1, max_iter, scheduler.get_lr()[0], iter_pixel_adder.average(),\n",
        "                    iter_fft_adder.average()))\n",
        "                writer.add_scalar('Pixel Loss', iter_pixel_adder.average(), iter_idx + (epoch_idx-1)* max_iter)\n",
        "                writer.add_scalar('FFT Loss', iter_fft_adder.average(), iter_idx + (epoch_idx - 1) * max_iter)\n",
        "\n",
        "                iter_timer.tic()\n",
        "                iter_pixel_adder.reset()\n",
        "                iter_fft_adder.reset()\n",
        "        overwrite_name = os.path.join(args.model_save_dir, 'model.pkl')\n",
        "        torch.save({'model': model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'epoch': epoch_idx}, overwrite_name)\n",
        "\n",
        "        if epoch_idx % args.save_freq == 0:\n",
        "            save_name = os.path.join(args.model_save_dir, 'model_%d.pkl' % epoch_idx)\n",
        "            torch.save({'model': model.state_dict()}, save_name)\n",
        "        print(\"EPOCH: %02d\\nElapsed time: %4.2f Epoch Pixel Loss: %7.4f Epoch FFT Loss: %7.4f\" % (\n",
        "            epoch_idx, epoch_timer.toc(), epoch_pixel_adder.average(), epoch_fft_adder.average()))\n",
        "        epoch_floss.append(epoch_fft_adder.average())\n",
        "        epoch_ploss.append(epoch_pixel_adder.average())\n",
        "        epoch_fft_adder.reset()\n",
        "        epoch_pixel_adder.reset()\n",
        "        scheduler.step()\n",
        "        if epoch_idx % args.valid_freq == 0:\n",
        "            val = _valid(model, args, epoch_idx)\n",
        "            print('%03d epoch \\n Average PSNR %.2f dB' % (epoch_idx, val))\n",
        "            epoch_psnr.append(val)\n",
        "            writer.add_scalar('PSNR', val, epoch_idx)\n",
        "            if val >= best_psnr:\n",
        "                torch.save({'model': model.state_dict()}, os.path.join(args.model_save_dir, 'Best.pkl'))\n",
        "        if epoch_idx % 10 == 0:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(epoch_ploss, label='Pixel Loss')\n",
        "            plt.plot(epoch_floss, label='FFT Loss')\n",
        "            #plt.plot(epoch_psnr, label='PSNR')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Metrics')\n",
        "            plt.title('Training Metrics Over Epochs')\n",
        "            plt.legend()\n",
        "            file_path = os.path.join(save_dir, args.figname)\n",
        "            plt.savefig(file_path)\n",
        "    save_name = os.path.join(args.model_save_dir, 'Final.pkl')\n",
        "    torch.save({'model': model.state_dict()}, save_name)\n",
        "    print(\"PSNR = \", epoch_psnr)\n",
        "    print(\"Pixel Loss =\", epoch_ploss)\n",
        "    print(\"FFT Loss =\", epoch_floss)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Mix Up Train\n",
        "\n"
      ],
      "metadata": {
        "id": "KkZX62DbuysV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#### Applies MixUp augmentation $####\n",
        "## Input parameters:\n",
        "## 1) x: Input data (features)\n",
        "## 2) y: Target data (labels)\n",
        "## 3) alpha: hyperparameter to control the mixup rate\n",
        "##\n",
        "## Output parameters\n",
        "## output data after mixed\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1.0\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size).cuda()\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    '''Compute the mixup loss'''\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn.functional as ft\n",
        "import torch.nn as nn\n",
        "from warmup_scheduler import GradualWarmupScheduler\n",
        "save_dir = '/content/drive/MyDrive/results'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "def _train(model, args):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    criterion = torch.nn.L1Loss().to(device)\n",
        "    print(\"training LR:\", args.learning_rate)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, betas=(0.9, 0.999), eps=1e-8)\n",
        "    dataloader = train_dataloader(args.data_dir, args.batch_size, args.num_worker)\n",
        "    max_iter = len(dataloader)\n",
        "    warmup_epochs=3\n",
        "    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.num_epoch-warmup_epochs, eta_min=1e-6)\n",
        "    scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=warmup_epochs, after_scheduler=scheduler_cosine)\n",
        "    scheduler.step()\n",
        "    epoch = 1\n",
        "    if args.resume:\n",
        "        state = torch.load(args.resume)\n",
        "        epoch = state['epoch']\n",
        "        optimizer.load_state_dict(state['optimizer'])\n",
        "        model.load_state_dict(state['model'])\n",
        "        print('Resume from %d'%epoch)\n",
        "        epoch += 1\n",
        "        for i in range(epoch-1):\n",
        "          scheduler.step()\n",
        "\n",
        "    writer = SummaryWriter()\n",
        "    epoch_pixel_adder = Adder()\n",
        "    epoch_fft_adder = Adder()\n",
        "    iter_pixel_adder = Adder()\n",
        "    iter_fft_adder = Adder()\n",
        "    epoch_timer = Timer('m')\n",
        "    iter_timer = Timer('m')\n",
        "    best_psnr=-1\n",
        "    epoch_floss = []\n",
        "    epoch_ploss = []\n",
        "    epoch_psnr = []\n",
        "\n",
        "    for epoch_idx in range(epoch, args.num_epoch + 1):\n",
        "\n",
        "        epoch_timer.tic()\n",
        "        iter_timer.tic()\n",
        "        for iter_idx, batch_data in enumerate(dataloader):\n",
        "            input_img, label_img = batch_data\n",
        "            input_img, label_img = input_img.to(device), label_img.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Apply Mixup Data Augmentation\n",
        "            mixed_input, mixed_label_a, mixed_label_b, lam = mixup_data(input_img, label_img, alpha=0.2)\n",
        "\n",
        "\n",
        "            # Forward pass with mixed inputs\n",
        "            pred_img = model(mixed_input)\n",
        "\n",
        "            label_img2_a = ft.interpolate(mixed_label_a, scale_factor=0.5, mode='bilinear')\n",
        "            label_img2_b = ft.interpolate(mixed_label_b, scale_factor=0.5, mode='bilinear')\n",
        "            label_img4_a = ft.interpolate(mixed_label_a, scale_factor=0.25, mode='bilinear')\n",
        "            label_img4_b = ft.interpolate(mixed_label_b, scale_factor=0.25, mode='bilinear')\n",
        "            #l1 = criterion(pred_img[0], label_img4)\n",
        "            #l2 = criterion(pred_img[1], label_img2)\n",
        "            #l3 = criterion(pred_img[2], label_img)\n",
        "\n",
        "            l1 = mixup_criterion(criterion, pred_img[0], label_img4_a, label_img4_b, lam)\n",
        "            l2 = mixup_criterion(criterion, pred_img[1], label_img2_a, label_img2_b, lam)\n",
        "            l3 = mixup_criterion(criterion, pred_img[2], mixed_label_a, mixed_label_b, lam)\n",
        "            loss_content = l1+l2+l3\n",
        "\n",
        "            label_fft1_a = torch.fft.fft2(label_img4_a, dim=(-2,-1))\n",
        "            label_fft1_a = torch.stack((label_fft1_a.real, label_fft1_a.imag), -1)\n",
        "            label_fft1_b = torch.fft.fft2(label_img4_b, dim=(-2,-1))\n",
        "            label_fft1_b = torch.stack((label_fft1_b.real, label_fft1_b.imag), -1)\n",
        "\n",
        "            pred_fft1 = torch.fft.fft2(pred_img[0], dim=(-2,-1))\n",
        "            pred_fft1 = torch.stack((pred_fft1.real, pred_fft1.imag), -1)\n",
        "\n",
        "            label_fft2_a = torch.fft.fft2(label_img2_a, dim=(-2,-1))\n",
        "            label_fft2_a = torch.stack((label_fft2_a.real, label_fft2_a.imag), -1)\n",
        "            label_fft2_b = torch.fft.fft2(label_img2_b, dim=(-2,-1))\n",
        "            label_fft2_b = torch.stack((label_fft2_b.real, label_fft2_b.imag), -1)\n",
        "\n",
        "            pred_fft2 = torch.fft.fft2(pred_img[1], dim=(-2,-1))\n",
        "            pred_fft2 = torch.stack((pred_fft2.real, pred_fft2.imag), -1)\n",
        "\n",
        "            label_fft3_a = torch.fft.fft2(mixed_label_a, dim=(-2,-1))\n",
        "            label_fft3_a = torch.stack((label_fft3_a.real, label_fft3_a.imag), -1)\n",
        "            label_fft3_b = torch.fft.fft2(mixed_label_b, dim=(-2,-1))\n",
        "            label_fft3_b = torch.stack((label_fft3_b.real, label_fft3_b.imag), -1)\n",
        "\n",
        "            pred_fft3 = torch.fft.fft2(pred_img[2], dim=(-2,-1))\n",
        "            pred_fft3 = torch.stack((pred_fft3.real, pred_fft3.imag), -1)\n",
        "\n",
        "            #f1 = criterion(pred_fft1, label_fft1)\n",
        "            #f2 = criterion(pred_fft2, label_fft2)\n",
        "            #f3 = criterion(pred_fft3, label_fft3)\n",
        "            f1 = mixup_criterion(criterion, pred_fft1, label_fft1_a, label_fft1_b, lam)\n",
        "            f2 = mixup_criterion(criterion, pred_fft2, label_fft2_a, label_fft2_b, lam)\n",
        "            f3 = mixup_criterion(criterion, pred_fft3, label_fft3_a, label_fft3_b, lam)\n",
        "            loss_fft = f1+f2+f3\n",
        "\n",
        "            loss = loss_content + 0.1 * loss_fft\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.001)\n",
        "            optimizer.step()\n",
        "\n",
        "            iter_pixel_adder(loss_content.item())\n",
        "            iter_fft_adder(loss_fft.item())\n",
        "\n",
        "            epoch_pixel_adder(loss_content.item())\n",
        "            epoch_fft_adder(loss_fft.item())\n",
        "\n",
        "            if (iter_idx + 1) % args.print_freq == 0:\n",
        "                print(\"Time: %7.4f Epoch: %03d Iter: %4d/%4d LR: %.10f Loss content: %7.4f Loss fft: %7.4f\" % (\n",
        "                    iter_timer.toc(), epoch_idx, iter_idx + 1, max_iter, scheduler.get_lr()[0], iter_pixel_adder.average(),\n",
        "                    iter_fft_adder.average()))\n",
        "                writer.add_scalar('Pixel Loss', iter_pixel_adder.average(), iter_idx + (epoch_idx-1)* max_iter)\n",
        "                writer.add_scalar('FFT Loss', iter_fft_adder.average(), iter_idx + (epoch_idx - 1) * max_iter)\n",
        "\n",
        "                iter_timer.tic()\n",
        "                iter_pixel_adder.reset()\n",
        "                iter_fft_adder.reset()\n",
        "        overwrite_name = os.path.join(args.model_save_dir, 'model.pkl')\n",
        "        torch.save({'model': model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'epoch': epoch_idx}, overwrite_name)\n",
        "\n",
        "        if epoch_idx % args.save_freq == 0:\n",
        "            save_name = os.path.join(args.model_save_dir, 'model_%d.pkl' % epoch_idx)\n",
        "            torch.save({'model': model.state_dict()}, save_name)\n",
        "        print(\"EPOCH: %02d\\nElapsed time: %4.2f Epoch Pixel Loss: %7.4f Epoch FFT Loss: %7.4f\" % (\n",
        "            epoch_idx, epoch_timer.toc(), epoch_pixel_adder.average(), epoch_fft_adder.average()))\n",
        "        epoch_floss.append(epoch_fft_adder.average())\n",
        "        epoch_ploss.append(epoch_pixel_adder.average())\n",
        "        epoch_fft_adder.reset()\n",
        "        epoch_pixel_adder.reset()\n",
        "        scheduler.step()\n",
        "        if epoch_idx % args.valid_freq == 0:\n",
        "            val = _valid(model, args, epoch_idx)\n",
        "            print('%03d epoch \\n Average PSNR %.2f dB' % (epoch_idx, val))\n",
        "            epoch_psnr.append(val)\n",
        "            writer.add_scalar('PSNR', val, epoch_idx)\n",
        "            if val >= best_psnr:\n",
        "                torch.save({'model': model.state_dict()}, os.path.join(args.model_save_dir, 'Best.pkl'))\n",
        "        if epoch_idx % 10 == 0:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(epoch_ploss, label='Pixel Loss')\n",
        "            plt.plot(epoch_floss, label='FFT Loss')\n",
        "            #plt.plot(epoch_psnr, label='PSNR')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Metrics')\n",
        "            plt.title('Training Metrics Over Epochs')\n",
        "            plt.legend()\n",
        "            file_path = os.path.join(save_dir, args.figname)\n",
        "            plt.savefig(file_path)\n",
        "    save_name = os.path.join(args.model_save_dir, 'Final.pkl')\n",
        "    torch.save({'model': model.state_dict()}, save_name)\n",
        "    print(\"PSNR = \", epoch_psnr)\n",
        "    print(\"Pixel Loss =\", epoch_ploss)\n",
        "    print(\"FFT Loss =\", epoch_floss)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "G3mf_d2suxgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoDUXGBNBWr7"
      },
      "source": [
        "Main\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "kcF3qG8RSAcd",
        "outputId": "901ac59c-41fd-45d0-fa07-948fcfce457d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.Args object at 0x786cf4254a90>\n",
            "training LR: 0.0001\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-87bc951fece7>\u001b[0m in \u001b[0;36m<cell line: 70>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-87bc951fece7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Make sure to define _train or import it if it's defined elsewhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-6ad4ac1369d2>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training LR:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_worker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mwarmup_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-dd93d3b1067d>\u001b[0m in \u001b[0;36mtrain_dataloader\u001b[0;34m(path, batch_size, num_workers, use_transform)\u001b[0m\n\u001b[1;32m     80\u001b[0m         )\n\u001b[1;32m     81\u001b[0m     dataloader = DataLoader(\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mDeblurDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-dd93d3b1067d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image_dir, transform, is_test, is_valid, ps)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hazy/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './reside-mixup/train/train/hazy/'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "# from torch.backends import cudnn # Uncomment if you need it\n",
        "\n",
        "class Args:\n",
        "    model_name = 'IRNeXt'\n",
        "    mode = 'train'\n",
        "    data_dir = './reside-mixup/train/'\n",
        "\n",
        "    # Train\n",
        "    batch_size = 8\n",
        "    learning_rate = 1e-4\n",
        "    weight_decay = 0\n",
        "    num_epoch = 30\n",
        "    print_freq = 500\n",
        "    num_worker = 8\n",
        "    save_freq = 10\n",
        "    valid_freq = 10\n",
        "    figname = 'outdoor_30epoch.jpg'\n",
        "    resume = '/content/drive/MyDrive/results/IRNeXt/OTS/model.pkl'\n",
        "\n",
        "    # Test\n",
        "    test_model = '/content/drive/MyDrive/results/IRNeXt/OTS/Final.pkl'\n",
        "    # save_image = False\n",
        "\n",
        "    # Directories (set these as per your requirement)\n",
        "    model_save_dir = os.path.join('/content/drive/MyDrive/results/', 'IRNeXt', 'OTS/')\n",
        "    result_dir = os.path.join('/content/drive/MyDrive/results/', model_name, 'test')\n",
        "\n",
        "def main(args):\n",
        "    # CUDNN\n",
        "    # cudnn.benchmark = True # Uncomment if you need it\n",
        "\n",
        "    if not os.path.exists('/content/drive/MyDrive/results/'):\n",
        "        os.makedirs(args.model_save_dir)\n",
        "    if not os.path.exists('/content/drive/MyDrive/results/' + args.model_name + '/'):\n",
        "        os.makedirs('/content/drive/MyDrive/results/' + args.model_name + '/')\n",
        "    if not os.path.exists(args.model_save_dir):\n",
        "        os.makedirs(args.model_save_dir)\n",
        "    if not os.path.exists(args.result_dir):\n",
        "        os.makedirs(args.result_dir)\n",
        "\n",
        "    model = build_net()  # Make sure to define build_net or import it if it's defined elsewhere\n",
        "    #print(model)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "    if args.mode == 'train':\n",
        "        _train(model, args)  # Make sure to define _train or import it if it's defined elsewhere\n",
        "\n",
        "    elif args.mode == 'test':\n",
        "        _eval(model, args)   # Make sure to define _eval or import it if it's defined elsewhere\n",
        "\n",
        "# Replace parser.parse_args() with an instance of the Args class\n",
        "args = Args()\n",
        "if not os.path.exists(args.model_save_dir):\n",
        "    os.makedirs(args.model_save_dir)\n",
        "# Copying files (make sure these paths are correct)\n",
        "command = 'cp ' + 'models/layers.py ' + args.model_save_dir\n",
        "os.system(command)\n",
        "command = 'cp ' + 'models/IRNeXt.py ' + args.model_save_dir\n",
        "os.system(command)\n",
        "command = 'cp ' + 'train.py ' + args.model_save_dir\n",
        "os.system(command)\n",
        "command = 'cp ' + 'main.py ' + args.model_save_dir\n",
        "os.system(command)\n",
        "print(args)\n",
        "main(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5aGTHRISo4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e083307f-632a-4618-ec41-19f365b6f15b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.Args object at 0x786be8907c70>\n",
            "1 iter PSNR_dehazing: 35.07 ssim: 0.996222\n",
            "1 iter PSNR: 35.07 time: 0.152913\n",
            "2 iter PSNR_dehazing: 33.69 ssim: 0.995272\n",
            "2 iter PSNR: 33.69 time: 0.134114\n",
            "3 iter PSNR_dehazing: 32.40 ssim: 0.994331\n",
            "3 iter PSNR: 32.40 time: 0.109591\n",
            "4 iter PSNR_dehazing: 31.53 ssim: 0.993857\n",
            "4 iter PSNR: 31.53 time: 0.107489\n",
            "5 iter PSNR_dehazing: 30.25 ssim: 0.993107\n",
            "5 iter PSNR: 30.25 time: 0.107862\n",
            "6 iter PSNR_dehazing: 29.23 ssim: 0.992002\n",
            "6 iter PSNR: 29.23 time: 0.107474\n",
            "7 iter PSNR_dehazing: 27.94 ssim: 0.990103\n",
            "7 iter PSNR: 27.94 time: 0.108520\n",
            "8 iter PSNR_dehazing: 35.26 ssim: 0.996260\n",
            "8 iter PSNR: 35.26 time: 0.106550\n",
            "9 iter PSNR_dehazing: 33.81 ssim: 0.995291\n",
            "9 iter PSNR: 33.81 time: 0.109317\n",
            "10 iter PSNR_dehazing: 32.86 ssim: 0.994550\n",
            "10 iter PSNR: 32.86 time: 0.110607\n",
            "11 iter PSNR_dehazing: 31.84 ssim: 0.994100\n",
            "11 iter PSNR: 31.84 time: 0.110312\n",
            "12 iter PSNR_dehazing: 31.26 ssim: 0.993683\n",
            "12 iter PSNR: 31.26 time: 0.108314\n",
            "13 iter PSNR_dehazing: 30.08 ssim: 0.992418\n",
            "13 iter PSNR: 30.08 time: 0.111465\n",
            "14 iter PSNR_dehazing: 28.71 ssim: 0.990463\n",
            "14 iter PSNR: 28.71 time: 0.108208\n",
            "15 iter PSNR_dehazing: 33.73 ssim: 0.995901\n",
            "15 iter PSNR: 33.73 time: 0.108039\n",
            "16 iter PSNR_dehazing: 32.53 ssim: 0.995008\n",
            "16 iter PSNR: 32.53 time: 0.107984\n",
            "17 iter PSNR_dehazing: 31.33 ssim: 0.993927\n",
            "17 iter PSNR: 31.33 time: 0.109671\n",
            "18 iter PSNR_dehazing: 30.99 ssim: 0.993520\n",
            "18 iter PSNR: 30.99 time: 0.108341\n",
            "19 iter PSNR_dehazing: 30.02 ssim: 0.992465\n",
            "19 iter PSNR: 30.02 time: 0.110792\n",
            "20 iter PSNR_dehazing: 28.75 ssim: 0.990967\n",
            "20 iter PSNR: 28.75 time: 0.108180\n",
            "21 iter PSNR_dehazing: 26.72 ssim: 0.988822\n",
            "21 iter PSNR: 26.72 time: 0.111505\n",
            "22 iter PSNR_dehazing: 34.85 ssim: 0.996060\n",
            "22 iter PSNR: 34.85 time: 0.109172\n",
            "23 iter PSNR_dehazing: 33.22 ssim: 0.995026\n",
            "23 iter PSNR: 33.22 time: 0.109896\n",
            "24 iter PSNR_dehazing: 32.13 ssim: 0.994280\n",
            "24 iter PSNR: 32.13 time: 0.107945\n",
            "25 iter PSNR_dehazing: 31.08 ssim: 0.993582\n",
            "25 iter PSNR: 31.08 time: 0.110013\n",
            "26 iter PSNR_dehazing: 30.15 ssim: 0.992945\n",
            "26 iter PSNR: 30.15 time: 0.107338\n",
            "27 iter PSNR_dehazing: 28.81 ssim: 0.991645\n",
            "27 iter PSNR: 28.81 time: 0.110148\n",
            "28 iter PSNR_dehazing: 27.19 ssim: 0.989439\n",
            "28 iter PSNR: 27.19 time: 0.109896\n",
            "29 iter PSNR_dehazing: 32.88 ssim: 0.995824\n",
            "29 iter PSNR: 32.88 time: 0.111491\n",
            "30 iter PSNR_dehazing: 31.13 ssim: 0.994658\n",
            "30 iter PSNR: 31.13 time: 0.108819\n",
            "31 iter PSNR_dehazing: 30.00 ssim: 0.993809\n",
            "31 iter PSNR: 30.00 time: 0.109590\n",
            "32 iter PSNR_dehazing: 29.00 ssim: 0.992846\n",
            "32 iter PSNR: 29.00 time: 0.109057\n",
            "33 iter PSNR_dehazing: 28.11 ssim: 0.991893\n",
            "33 iter PSNR: 28.11 time: 0.109939\n",
            "34 iter PSNR_dehazing: 26.97 ssim: 0.990436\n",
            "34 iter PSNR: 26.97 time: 0.106652\n",
            "35 iter PSNR_dehazing: 25.38 ssim: 0.987597\n",
            "35 iter PSNR: 25.38 time: 0.109924\n",
            "36 iter PSNR_dehazing: 33.89 ssim: 0.994648\n",
            "36 iter PSNR: 33.89 time: 0.110000\n",
            "37 iter PSNR_dehazing: 34.08 ssim: 0.994557\n",
            "37 iter PSNR: 34.08 time: 0.113177\n",
            "38 iter PSNR_dehazing: 34.67 ssim: 0.994814\n",
            "38 iter PSNR: 34.67 time: 0.108628\n",
            "39 iter PSNR_dehazing: 34.61 ssim: 0.994324\n",
            "39 iter PSNR: 34.61 time: 0.109541\n",
            "40 iter PSNR_dehazing: 35.07 ssim: 0.994305\n",
            "40 iter PSNR: 35.07 time: 0.109748\n",
            "41 iter PSNR_dehazing: 35.42 ssim: 0.993978\n",
            "41 iter PSNR: 35.42 time: 0.111050\n",
            "42 iter PSNR_dehazing: 34.57 ssim: 0.992714\n",
            "42 iter PSNR: 34.57 time: 0.108819\n",
            "43 iter PSNR_dehazing: 34.10 ssim: 0.994733\n",
            "43 iter PSNR: 34.10 time: 0.109454\n",
            "44 iter PSNR_dehazing: 33.83 ssim: 0.994602\n",
            "44 iter PSNR: 33.83 time: 0.108266\n",
            "45 iter PSNR_dehazing: 35.33 ssim: 0.995129\n",
            "45 iter PSNR: 35.33 time: 0.111847\n",
            "46 iter PSNR_dehazing: 34.66 ssim: 0.994730\n",
            "46 iter PSNR: 34.66 time: 0.109997\n",
            "47 iter PSNR_dehazing: 35.02 ssim: 0.994368\n",
            "47 iter PSNR: 35.02 time: 0.109709\n",
            "48 iter PSNR_dehazing: 35.65 ssim: 0.994325\n",
            "48 iter PSNR: 35.65 time: 0.108674\n",
            "49 iter PSNR_dehazing: 34.94 ssim: 0.993203\n",
            "49 iter PSNR: 34.94 time: 0.111656\n",
            "50 iter PSNR_dehazing: 33.45 ssim: 0.994349\n",
            "50 iter PSNR: 33.45 time: 0.109940\n",
            "51 iter PSNR_dehazing: 33.39 ssim: 0.993995\n",
            "51 iter PSNR: 33.39 time: 0.110276\n",
            "52 iter PSNR_dehazing: 34.03 ssim: 0.994545\n",
            "52 iter PSNR: 34.03 time: 0.108862\n",
            "53 iter PSNR_dehazing: 33.89 ssim: 0.994132\n",
            "53 iter PSNR: 33.89 time: 0.110576\n",
            "54 iter PSNR_dehazing: 33.86 ssim: 0.993551\n",
            "54 iter PSNR: 33.86 time: 0.111105\n",
            "55 iter PSNR_dehazing: 34.43 ssim: 0.993165\n",
            "55 iter PSNR: 34.43 time: 0.110647\n",
            "56 iter PSNR_dehazing: 33.95 ssim: 0.992107\n",
            "56 iter PSNR: 33.95 time: 0.107439\n",
            "57 iter PSNR_dehazing: 34.06 ssim: 0.994982\n",
            "57 iter PSNR: 34.06 time: 0.110094\n",
            "58 iter PSNR_dehazing: 33.93 ssim: 0.994663\n",
            "58 iter PSNR: 33.93 time: 0.110683\n",
            "59 iter PSNR_dehazing: 34.80 ssim: 0.994917\n",
            "59 iter PSNR: 34.80 time: 0.111177\n",
            "60 iter PSNR_dehazing: 34.10 ssim: 0.993994\n",
            "60 iter PSNR: 34.10 time: 0.108123\n",
            "61 iter PSNR_dehazing: 34.79 ssim: 0.994219\n",
            "61 iter PSNR: 34.79 time: 0.110132\n",
            "62 iter PSNR_dehazing: 35.19 ssim: 0.993782\n",
            "62 iter PSNR: 35.19 time: 0.109843\n",
            "63 iter PSNR_dehazing: 34.48 ssim: 0.992649\n",
            "63 iter PSNR: 34.48 time: 0.113005\n",
            "64 iter PSNR_dehazing: 34.66 ssim: 0.995150\n",
            "64 iter PSNR: 34.66 time: 0.109990\n",
            "65 iter PSNR_dehazing: 34.32 ssim: 0.994594\n",
            "65 iter PSNR: 34.32 time: 0.109492\n",
            "66 iter PSNR_dehazing: 34.30 ssim: 0.994473\n",
            "66 iter PSNR: 34.30 time: 0.109556\n",
            "67 iter PSNR_dehazing: 34.30 ssim: 0.994082\n",
            "67 iter PSNR: 34.30 time: 0.111416\n",
            "68 iter PSNR_dehazing: 34.07 ssim: 0.993569\n",
            "68 iter PSNR: 34.07 time: 0.110157\n",
            "69 iter PSNR_dehazing: 33.90 ssim: 0.992964\n",
            "69 iter PSNR: 33.90 time: 0.110414\n",
            "70 iter PSNR_dehazing: 32.86 ssim: 0.991469\n",
            "70 iter PSNR: 32.86 time: 0.108313\n",
            "71 iter PSNR_dehazing: 38.50 ssim: 0.996963\n",
            "71 iter PSNR: 38.50 time: 0.093754\n",
            "72 iter PSNR_dehazing: 35.67 ssim: 0.996203\n",
            "72 iter PSNR: 35.67 time: 0.091634\n",
            "73 iter PSNR_dehazing: 33.84 ssim: 0.995581\n",
            "73 iter PSNR: 33.84 time: 0.092715\n",
            "74 iter PSNR_dehazing: 32.07 ssim: 0.994797\n",
            "74 iter PSNR: 32.07 time: 0.092628\n",
            "75 iter PSNR_dehazing: 31.05 ssim: 0.993839\n",
            "75 iter PSNR: 31.05 time: 0.092957\n",
            "76 iter PSNR_dehazing: 29.25 ssim: 0.992238\n",
            "76 iter PSNR: 29.25 time: 0.094472\n",
            "77 iter PSNR_dehazing: 27.47 ssim: 0.989833\n",
            "77 iter PSNR: 27.47 time: 0.092142\n",
            "78 iter PSNR_dehazing: 40.05 ssim: 0.997054\n",
            "78 iter PSNR: 40.05 time: 0.092538\n",
            "79 iter PSNR_dehazing: 37.20 ssim: 0.996381\n",
            "79 iter PSNR: 37.20 time: 0.093173\n",
            "80 iter PSNR_dehazing: 35.15 ssim: 0.995843\n",
            "80 iter PSNR: 35.15 time: 0.088731\n",
            "81 iter PSNR_dehazing: 33.49 ssim: 0.995150\n",
            "81 iter PSNR: 33.49 time: 0.093641\n",
            "82 iter PSNR_dehazing: 32.17 ssim: 0.994267\n",
            "82 iter PSNR: 32.17 time: 0.093997\n",
            "83 iter PSNR_dehazing: 30.50 ssim: 0.992940\n",
            "83 iter PSNR: 30.50 time: 0.093547\n",
            "84 iter PSNR_dehazing: 28.49 ssim: 0.990706\n",
            "84 iter PSNR: 28.49 time: 0.092219\n",
            "85 iter PSNR_dehazing: 36.95 ssim: 0.996793\n",
            "85 iter PSNR: 36.95 time: 0.091521\n",
            "86 iter PSNR_dehazing: 34.57 ssim: 0.995960\n",
            "86 iter PSNR: 34.57 time: 0.093474\n",
            "87 iter PSNR_dehazing: 32.34 ssim: 0.995072\n",
            "87 iter PSNR: 32.34 time: 0.092747\n",
            "88 iter PSNR_dehazing: 30.86 ssim: 0.994241\n",
            "88 iter PSNR: 30.86 time: 0.094076\n",
            "89 iter PSNR_dehazing: 29.50 ssim: 0.992965\n",
            "89 iter PSNR: 29.50 time: 0.093477\n",
            "90 iter PSNR_dehazing: 27.42 ssim: 0.990661\n",
            "90 iter PSNR: 27.42 time: 0.092391\n",
            "91 iter PSNR_dehazing: 25.70 ssim: 0.987692\n",
            "91 iter PSNR: 25.70 time: 0.093333\n",
            "92 iter PSNR_dehazing: 38.26 ssim: 0.996953\n",
            "92 iter PSNR: 38.26 time: 0.094416\n",
            "93 iter PSNR_dehazing: 35.25 ssim: 0.996103\n",
            "93 iter PSNR: 35.25 time: 0.094890\n",
            "94 iter PSNR_dehazing: 33.47 ssim: 0.995441\n",
            "94 iter PSNR: 33.47 time: 0.093236\n",
            "95 iter PSNR_dehazing: 31.27 ssim: 0.994387\n",
            "95 iter PSNR: 31.27 time: 0.093331\n",
            "96 iter PSNR_dehazing: 30.02 ssim: 0.993317\n",
            "96 iter PSNR: 30.02 time: 0.093422\n",
            "97 iter PSNR_dehazing: 27.97 ssim: 0.991192\n",
            "97 iter PSNR: 27.97 time: 0.093091\n",
            "98 iter PSNR_dehazing: 26.41 ssim: 0.988638\n",
            "98 iter PSNR: 26.41 time: 0.094362\n",
            "99 iter PSNR_dehazing: 36.77 ssim: 0.996729\n",
            "99 iter PSNR: 36.77 time: 0.093267\n",
            "100 iter PSNR_dehazing: 33.63 ssim: 0.995775\n",
            "100 iter PSNR: 33.63 time: 0.092735\n",
            "101 iter PSNR_dehazing: 31.59 ssim: 0.994815\n",
            "101 iter PSNR: 31.59 time: 0.093922\n",
            "102 iter PSNR_dehazing: 29.77 ssim: 0.993571\n",
            "102 iter PSNR: 29.77 time: 0.094203\n",
            "103 iter PSNR_dehazing: 28.54 ssim: 0.992270\n",
            "103 iter PSNR: 28.54 time: 0.093048\n",
            "104 iter PSNR_dehazing: 26.69 ssim: 0.989890\n",
            "104 iter PSNR: 26.69 time: 0.093457\n",
            "105 iter PSNR_dehazing: 24.96 ssim: 0.986645\n",
            "105 iter PSNR: 24.96 time: 0.093552\n",
            "106 iter PSNR_dehazing: 38.19 ssim: 0.997252\n",
            "106 iter PSNR: 38.19 time: 0.112466\n",
            "107 iter PSNR_dehazing: 35.34 ssim: 0.996224\n",
            "107 iter PSNR: 35.34 time: 0.111399\n",
            "108 iter PSNR_dehazing: 34.66 ssim: 0.995719\n",
            "108 iter PSNR: 34.66 time: 0.111670\n",
            "109 iter PSNR_dehazing: 34.05 ssim: 0.995155\n",
            "109 iter PSNR: 34.05 time: 0.109984\n",
            "110 iter PSNR_dehazing: 34.07 ssim: 0.995128\n",
            "110 iter PSNR: 34.07 time: 0.111401\n",
            "111 iter PSNR_dehazing: 32.16 ssim: 0.993772\n",
            "111 iter PSNR: 32.16 time: 0.110833\n",
            "112 iter PSNR_dehazing: 30.43 ssim: 0.992241\n",
            "112 iter PSNR: 30.43 time: 0.112224\n",
            "113 iter PSNR_dehazing: 37.16 ssim: 0.997144\n",
            "113 iter PSNR: 37.16 time: 0.111879\n",
            "114 iter PSNR_dehazing: 35.52 ssim: 0.996299\n",
            "114 iter PSNR: 35.52 time: 0.110866\n",
            "115 iter PSNR_dehazing: 33.96 ssim: 0.995698\n",
            "115 iter PSNR: 33.96 time: 0.109910\n",
            "116 iter PSNR_dehazing: 34.38 ssim: 0.995510\n",
            "116 iter PSNR: 34.38 time: 0.111872\n",
            "117 iter PSNR_dehazing: 33.42 ssim: 0.995000\n",
            "117 iter PSNR: 33.42 time: 0.108786\n",
            "118 iter PSNR_dehazing: 31.37 ssim: 0.993701\n",
            "118 iter PSNR: 31.37 time: 0.112749\n",
            "119 iter PSNR_dehazing: 29.93 ssim: 0.992363\n",
            "119 iter PSNR: 29.93 time: 0.111386\n",
            "120 iter PSNR_dehazing: 39.31 ssim: 0.997298\n",
            "120 iter PSNR: 39.31 time: 0.112026\n",
            "121 iter PSNR_dehazing: 37.12 ssim: 0.996704\n",
            "121 iter PSNR: 37.12 time: 0.109446\n",
            "122 iter PSNR_dehazing: 35.93 ssim: 0.996166\n",
            "122 iter PSNR: 35.93 time: 0.112599\n",
            "123 iter PSNR_dehazing: 35.14 ssim: 0.995679\n",
            "123 iter PSNR: 35.14 time: 0.110884\n",
            "124 iter PSNR_dehazing: 34.16 ssim: 0.994991\n",
            "124 iter PSNR: 34.16 time: 0.114026\n",
            "125 iter PSNR_dehazing: 32.62 ssim: 0.993882\n",
            "125 iter PSNR: 32.62 time: 0.112369\n",
            "126 iter PSNR_dehazing: 31.16 ssim: 0.992375\n",
            "126 iter PSNR: 31.16 time: 0.112245\n",
            "127 iter PSNR_dehazing: 39.33 ssim: 0.997375\n",
            "127 iter PSNR: 39.33 time: 0.111627\n",
            "128 iter PSNR_dehazing: 36.98 ssim: 0.996458\n",
            "128 iter PSNR: 36.98 time: 0.110632\n",
            "129 iter PSNR_dehazing: 35.34 ssim: 0.995747\n",
            "129 iter PSNR: 35.34 time: 0.110481\n",
            "130 iter PSNR_dehazing: 33.87 ssim: 0.995194\n",
            "130 iter PSNR: 33.87 time: 0.112198\n",
            "131 iter PSNR_dehazing: 32.71 ssim: 0.994405\n",
            "131 iter PSNR: 32.71 time: 0.110831\n",
            "132 iter PSNR_dehazing: 32.64 ssim: 0.993811\n",
            "132 iter PSNR: 32.64 time: 0.113021\n",
            "133 iter PSNR_dehazing: 31.00 ssim: 0.992200\n",
            "133 iter PSNR: 31.00 time: 0.111620\n",
            "134 iter PSNR_dehazing: 37.82 ssim: 0.997304\n",
            "134 iter PSNR: 37.82 time: 0.111544\n",
            "135 iter PSNR_dehazing: 35.49 ssim: 0.996615\n",
            "135 iter PSNR: 35.49 time: 0.108851\n",
            "136 iter PSNR_dehazing: 34.30 ssim: 0.996106\n",
            "136 iter PSNR: 34.30 time: 0.111696\n",
            "137 iter PSNR_dehazing: 33.36 ssim: 0.995695\n",
            "137 iter PSNR: 33.36 time: 0.110967\n",
            "138 iter PSNR_dehazing: 32.42 ssim: 0.995131\n",
            "138 iter PSNR: 32.42 time: 0.113286\n",
            "139 iter PSNR_dehazing: 31.11 ssim: 0.994343\n",
            "139 iter PSNR: 31.11 time: 0.112190\n",
            "140 iter PSNR_dehazing: 29.69 ssim: 0.992753\n",
            "140 iter PSNR: 29.69 time: 0.111986\n",
            "141 iter PSNR_dehazing: 35.40 ssim: 0.994581\n",
            "141 iter PSNR: 35.40 time: 0.111036\n",
            "142 iter PSNR_dehazing: 33.62 ssim: 0.992381\n",
            "142 iter PSNR: 33.62 time: 0.111718\n",
            "143 iter PSNR_dehazing: 33.25 ssim: 0.992014\n",
            "143 iter PSNR: 33.25 time: 0.109978\n",
            "144 iter PSNR_dehazing: 33.01 ssim: 0.991730\n",
            "144 iter PSNR: 33.01 time: 0.113043\n",
            "145 iter PSNR_dehazing: 32.30 ssim: 0.990974\n",
            "145 iter PSNR: 32.30 time: 0.112850\n",
            "146 iter PSNR_dehazing: 31.11 ssim: 0.988787\n",
            "146 iter PSNR: 31.11 time: 0.113068\n",
            "147 iter PSNR_dehazing: 30.24 ssim: 0.987074\n",
            "147 iter PSNR: 30.24 time: 0.110767\n",
            "148 iter PSNR_dehazing: 35.76 ssim: 0.994873\n",
            "148 iter PSNR: 35.76 time: 0.111158\n",
            "149 iter PSNR_dehazing: 33.96 ssim: 0.992829\n",
            "149 iter PSNR: 33.96 time: 0.110817\n",
            "150 iter PSNR_dehazing: 33.65 ssim: 0.992611\n",
            "150 iter PSNR: 33.65 time: 0.111990\n",
            "151 iter PSNR_dehazing: 33.12 ssim: 0.991938\n",
            "151 iter PSNR: 33.12 time: 0.112188\n",
            "152 iter PSNR_dehazing: 32.42 ssim: 0.991049\n",
            "152 iter PSNR: 32.42 time: 0.112523\n",
            "153 iter PSNR_dehazing: 31.23 ssim: 0.988998\n",
            "153 iter PSNR: 31.23 time: 0.111909\n",
            "154 iter PSNR_dehazing: 30.41 ssim: 0.987263\n",
            "154 iter PSNR: 30.41 time: 0.112530\n",
            "155 iter PSNR_dehazing: 34.74 ssim: 0.994220\n",
            "155 iter PSNR: 34.74 time: 0.110631\n",
            "156 iter PSNR_dehazing: 33.13 ssim: 0.992205\n",
            "156 iter PSNR: 33.13 time: 0.112336\n",
            "157 iter PSNR_dehazing: 33.11 ssim: 0.992294\n",
            "157 iter PSNR: 33.11 time: 0.110036\n",
            "158 iter PSNR_dehazing: 32.22 ssim: 0.990911\n",
            "158 iter PSNR: 32.22 time: 0.111774\n",
            "159 iter PSNR_dehazing: 31.90 ssim: 0.990611\n",
            "159 iter PSNR: 31.90 time: 0.112002\n",
            "160 iter PSNR_dehazing: 31.04 ssim: 0.988879\n",
            "160 iter PSNR: 31.04 time: 0.113095\n",
            "161 iter PSNR_dehazing: 29.79 ssim: 0.986243\n",
            "161 iter PSNR: 29.79 time: 0.112045\n",
            "162 iter PSNR_dehazing: 35.58 ssim: 0.994689\n",
            "162 iter PSNR: 35.58 time: 0.113069\n",
            "163 iter PSNR_dehazing: 33.61 ssim: 0.992437\n",
            "163 iter PSNR: 33.61 time: 0.112129\n",
            "164 iter PSNR_dehazing: 33.35 ssim: 0.992143\n",
            "164 iter PSNR: 33.35 time: 0.112164\n",
            "165 iter PSNR_dehazing: 32.92 ssim: 0.991600\n",
            "165 iter PSNR: 32.92 time: 0.110828\n",
            "166 iter PSNR_dehazing: 32.07 ssim: 0.990467\n",
            "166 iter PSNR: 32.07 time: 0.112233\n",
            "167 iter PSNR_dehazing: 31.21 ssim: 0.988909\n",
            "167 iter PSNR: 31.21 time: 0.110939\n",
            "168 iter PSNR_dehazing: 30.12 ssim: 0.986884\n",
            "168 iter PSNR: 30.12 time: 0.113770\n",
            "169 iter PSNR_dehazing: 33.97 ssim: 0.993969\n",
            "169 iter PSNR: 33.97 time: 0.112546\n",
            "170 iter PSNR_dehazing: 32.18 ssim: 0.991801\n",
            "170 iter PSNR: 32.18 time: 0.113735\n",
            "171 iter PSNR_dehazing: 31.35 ssim: 0.990822\n",
            "171 iter PSNR: 31.35 time: 0.112448\n",
            "172 iter PSNR_dehazing: 30.71 ssim: 0.990085\n",
            "172 iter PSNR: 30.71 time: 0.113671\n",
            "173 iter PSNR_dehazing: 30.03 ssim: 0.989470\n",
            "173 iter PSNR: 30.03 time: 0.110928\n",
            "174 iter PSNR_dehazing: 29.00 ssim: 0.988094\n",
            "174 iter PSNR: 29.00 time: 0.110968\n",
            "175 iter PSNR_dehazing: 27.75 ssim: 0.985275\n",
            "175 iter PSNR: 27.75 time: 0.110483\n",
            "176 iter PSNR_dehazing: 36.67 ssim: 0.995089\n",
            "176 iter PSNR: 36.67 time: 0.112668\n",
            "177 iter PSNR_dehazing: 35.46 ssim: 0.993691\n",
            "177 iter PSNR: 35.46 time: 0.112326\n",
            "178 iter PSNR_dehazing: 34.48 ssim: 0.992693\n",
            "178 iter PSNR: 34.48 time: 0.113325\n",
            "179 iter PSNR_dehazing: 33.96 ssim: 0.992093\n",
            "179 iter PSNR: 33.96 time: 0.112967\n",
            "180 iter PSNR_dehazing: 33.90 ssim: 0.991427\n",
            "180 iter PSNR: 33.90 time: 0.113097\n",
            "181 iter PSNR_dehazing: 32.75 ssim: 0.989540\n",
            "181 iter PSNR: 32.75 time: 0.112723\n",
            "182 iter PSNR_dehazing: 31.55 ssim: 0.987081\n",
            "182 iter PSNR: 31.55 time: 0.111817\n",
            "183 iter PSNR_dehazing: 36.96 ssim: 0.995371\n",
            "183 iter PSNR: 36.96 time: 0.109947\n",
            "184 iter PSNR_dehazing: 35.45 ssim: 0.993783\n",
            "184 iter PSNR: 35.45 time: 0.112553\n",
            "185 iter PSNR_dehazing: 34.64 ssim: 0.993311\n",
            "185 iter PSNR: 34.64 time: 0.111775\n",
            "186 iter PSNR_dehazing: 34.09 ssim: 0.991908\n",
            "186 iter PSNR: 34.09 time: 0.114155\n",
            "187 iter PSNR_dehazing: 33.75 ssim: 0.991405\n",
            "187 iter PSNR: 33.75 time: 0.114460\n",
            "188 iter PSNR_dehazing: 32.84 ssim: 0.989780\n",
            "188 iter PSNR: 32.84 time: 0.113863\n",
            "189 iter PSNR_dehazing: 31.84 ssim: 0.987770\n",
            "189 iter PSNR: 31.84 time: 0.114653\n",
            "190 iter PSNR_dehazing: 36.55 ssim: 0.994620\n",
            "190 iter PSNR: 36.55 time: 0.116027\n",
            "191 iter PSNR_dehazing: 34.82 ssim: 0.992756\n",
            "191 iter PSNR: 34.82 time: 0.111982\n",
            "192 iter PSNR_dehazing: 34.39 ssim: 0.992250\n",
            "192 iter PSNR: 34.39 time: 0.112839\n",
            "193 iter PSNR_dehazing: 34.03 ssim: 0.991791\n",
            "193 iter PSNR: 34.03 time: 0.111542\n",
            "194 iter PSNR_dehazing: 33.36 ssim: 0.990790\n",
            "194 iter PSNR: 33.36 time: 0.111859\n",
            "195 iter PSNR_dehazing: 32.46 ssim: 0.989181\n",
            "195 iter PSNR: 32.46 time: 0.111147\n",
            "196 iter PSNR_dehazing: 31.75 ssim: 0.987446\n",
            "196 iter PSNR: 31.75 time: 0.113017\n",
            "197 iter PSNR_dehazing: 36.71 ssim: 0.994764\n",
            "197 iter PSNR: 36.71 time: 0.111952\n",
            "198 iter PSNR_dehazing: 34.98 ssim: 0.993258\n",
            "198 iter PSNR: 34.98 time: 0.112706\n",
            "199 iter PSNR_dehazing: 34.95 ssim: 0.992999\n",
            "199 iter PSNR: 34.95 time: 0.112521\n",
            "200 iter PSNR_dehazing: 34.29 ssim: 0.992294\n",
            "200 iter PSNR: 34.29 time: 0.113120\n",
            "201 iter PSNR_dehazing: 33.30 ssim: 0.990637\n",
            "201 iter PSNR: 33.30 time: 0.112682\n",
            "202 iter PSNR_dehazing: 32.52 ssim: 0.989059\n",
            "202 iter PSNR: 32.52 time: 0.113323\n",
            "203 iter PSNR_dehazing: 31.74 ssim: 0.987606\n",
            "203 iter PSNR: 31.74 time: 0.113278\n",
            "204 iter PSNR_dehazing: 35.76 ssim: 0.994340\n",
            "204 iter PSNR: 35.76 time: 0.113828\n",
            "205 iter PSNR_dehazing: 34.17 ssim: 0.992783\n",
            "205 iter PSNR: 34.17 time: 0.114640\n",
            "206 iter PSNR_dehazing: 33.12 ssim: 0.991649\n",
            "206 iter PSNR: 33.12 time: 0.114748\n",
            "207 iter PSNR_dehazing: 32.72 ssim: 0.991178\n",
            "207 iter PSNR: 32.72 time: 0.113837\n",
            "208 iter PSNR_dehazing: 32.02 ssim: 0.990130\n",
            "208 iter PSNR: 32.02 time: 0.114874\n",
            "209 iter PSNR_dehazing: 31.05 ssim: 0.988925\n",
            "209 iter PSNR: 31.05 time: 0.112837\n",
            "210 iter PSNR_dehazing: 29.66 ssim: 0.985546\n",
            "210 iter PSNR: 29.66 time: 0.114050\n",
            "211 iter PSNR_dehazing: 33.62 ssim: 0.991248\n",
            "211 iter PSNR: 33.62 time: 0.133496\n",
            "212 iter PSNR_dehazing: 31.52 ssim: 0.988017\n",
            "212 iter PSNR: 31.52 time: 0.120745\n",
            "213 iter PSNR_dehazing: 30.95 ssim: 0.987370\n",
            "213 iter PSNR: 30.95 time: 0.119056\n",
            "214 iter PSNR_dehazing: 30.98 ssim: 0.987621\n",
            "214 iter PSNR: 30.98 time: 0.119789\n",
            "215 iter PSNR_dehazing: 30.56 ssim: 0.985776\n",
            "215 iter PSNR: 30.56 time: 0.120480\n",
            "216 iter PSNR_dehazing: 29.84 ssim: 0.983507\n",
            "216 iter PSNR: 29.84 time: 0.122726\n",
            "217 iter PSNR_dehazing: 28.79 ssim: 0.981673\n",
            "217 iter PSNR: 28.79 time: 0.119942\n",
            "218 iter PSNR_dehazing: 34.57 ssim: 0.992161\n",
            "218 iter PSNR: 34.57 time: 0.124887\n",
            "219 iter PSNR_dehazing: 32.88 ssim: 0.989764\n",
            "219 iter PSNR: 32.88 time: 0.118203\n",
            "220 iter PSNR_dehazing: 31.88 ssim: 0.987768\n",
            "220 iter PSNR: 31.88 time: 0.121085\n",
            "221 iter PSNR_dehazing: 32.06 ssim: 0.988109\n",
            "221 iter PSNR: 32.06 time: 0.120609\n",
            "222 iter PSNR_dehazing: 31.31 ssim: 0.986303\n",
            "222 iter PSNR: 31.31 time: 0.121316\n",
            "223 iter PSNR_dehazing: 30.74 ssim: 0.984806\n",
            "223 iter PSNR: 30.74 time: 0.119578\n",
            "224 iter PSNR_dehazing: 29.55 ssim: 0.982556\n",
            "224 iter PSNR: 29.55 time: 0.122147\n",
            "225 iter PSNR_dehazing: 32.93 ssim: 0.990725\n",
            "225 iter PSNR: 32.93 time: 0.121295\n",
            "226 iter PSNR_dehazing: 31.72 ssim: 0.988255\n",
            "226 iter PSNR: 31.72 time: 0.121255\n",
            "227 iter PSNR_dehazing: 31.04 ssim: 0.987577\n",
            "227 iter PSNR: 31.04 time: 0.121116\n",
            "228 iter PSNR_dehazing: 31.04 ssim: 0.987136\n",
            "228 iter PSNR: 31.04 time: 0.121932\n",
            "229 iter PSNR_dehazing: 30.38 ssim: 0.985627\n",
            "229 iter PSNR: 30.38 time: 0.119612\n",
            "230 iter PSNR_dehazing: 29.61 ssim: 0.982639\n",
            "230 iter PSNR: 29.61 time: 0.120313\n",
            "231 iter PSNR_dehazing: 28.53 ssim: 0.980523\n",
            "231 iter PSNR: 28.53 time: 0.120799\n",
            "232 iter PSNR_dehazing: 32.74 ssim: 0.989936\n",
            "232 iter PSNR: 32.74 time: 0.120950\n",
            "233 iter PSNR_dehazing: 31.87 ssim: 0.988430\n",
            "233 iter PSNR: 31.87 time: 0.121571\n",
            "234 iter PSNR_dehazing: 30.93 ssim: 0.986914\n",
            "234 iter PSNR: 30.93 time: 0.120790\n",
            "235 iter PSNR_dehazing: 31.36 ssim: 0.987453\n",
            "235 iter PSNR: 31.36 time: 0.120764\n",
            "236 iter PSNR_dehazing: 30.76 ssim: 0.986259\n",
            "236 iter PSNR: 30.76 time: 0.122282\n",
            "237 iter PSNR_dehazing: 29.42 ssim: 0.982356\n",
            "237 iter PSNR: 29.42 time: 0.119734\n",
            "238 iter PSNR_dehazing: 28.99 ssim: 0.981818\n",
            "238 iter PSNR: 28.99 time: 0.120298\n",
            "239 iter PSNR_dehazing: 32.39 ssim: 0.989806\n",
            "239 iter PSNR: 32.39 time: 0.121875\n",
            "240 iter PSNR_dehazing: 31.35 ssim: 0.988338\n",
            "240 iter PSNR: 31.35 time: 0.120930\n",
            "241 iter PSNR_dehazing: 30.50 ssim: 0.986714\n",
            "241 iter PSNR: 30.50 time: 0.119803\n",
            "242 iter PSNR_dehazing: 30.58 ssim: 0.986532\n",
            "242 iter PSNR: 30.58 time: 0.123321\n",
            "243 iter PSNR_dehazing: 29.89 ssim: 0.984540\n",
            "243 iter PSNR: 29.89 time: 0.120022\n",
            "244 iter PSNR_dehazing: 29.22 ssim: 0.981582\n",
            "244 iter PSNR: 29.22 time: 0.121821\n",
            "245 iter PSNR_dehazing: 27.85 ssim: 0.978766\n",
            "245 iter PSNR: 27.85 time: 0.120662\n",
            "246 iter PSNR_dehazing: 35.23 ssim: 0.995700\n",
            "246 iter PSNR: 35.23 time: 0.112747\n",
            "247 iter PSNR_dehazing: 32.98 ssim: 0.993673\n",
            "247 iter PSNR: 32.98 time: 0.112532\n",
            "248 iter PSNR_dehazing: 31.36 ssim: 0.991739\n",
            "248 iter PSNR: 31.36 time: 0.113765\n",
            "249 iter PSNR_dehazing: 30.02 ssim: 0.989535\n",
            "249 iter PSNR: 30.02 time: 0.112554\n",
            "250 iter PSNR_dehazing: 29.04 ssim: 0.987781\n",
            "250 iter PSNR: 29.04 time: 0.114369\n",
            "251 iter PSNR_dehazing: 28.43 ssim: 0.985379\n",
            "251 iter PSNR: 28.43 time: 0.113213\n",
            "252 iter PSNR_dehazing: 27.81 ssim: 0.982917\n",
            "252 iter PSNR: 27.81 time: 0.114518\n",
            "253 iter PSNR_dehazing: 36.18 ssim: 0.996060\n",
            "253 iter PSNR: 36.18 time: 0.112412\n",
            "254 iter PSNR_dehazing: 34.34 ssim: 0.994676\n",
            "254 iter PSNR: 34.34 time: 0.114807\n",
            "255 iter PSNR_dehazing: 32.98 ssim: 0.993286\n",
            "255 iter PSNR: 32.98 time: 0.113735\n",
            "256 iter PSNR_dehazing: 32.07 ssim: 0.991961\n",
            "256 iter PSNR: 32.07 time: 0.113498\n",
            "257 iter PSNR_dehazing: 31.15 ssim: 0.990448\n",
            "257 iter PSNR: 31.15 time: 0.113579\n",
            "258 iter PSNR_dehazing: 29.93 ssim: 0.987606\n",
            "258 iter PSNR: 29.93 time: 0.115026\n",
            "259 iter PSNR_dehazing: 28.96 ssim: 0.984917\n",
            "259 iter PSNR: 28.96 time: 0.113275\n",
            "260 iter PSNR_dehazing: 32.27 ssim: 0.994029\n",
            "260 iter PSNR: 32.27 time: 0.115398\n",
            "261 iter PSNR_dehazing: 30.98 ssim: 0.991980\n",
            "261 iter PSNR: 30.98 time: 0.114294\n",
            "262 iter PSNR_dehazing: 29.71 ssim: 0.989915\n",
            "262 iter PSNR: 29.71 time: 0.115050\n",
            "263 iter PSNR_dehazing: 29.12 ssim: 0.988188\n",
            "263 iter PSNR: 29.12 time: 0.113156\n",
            "264 iter PSNR_dehazing: 28.12 ssim: 0.985886\n",
            "264 iter PSNR: 28.12 time: 0.114972\n",
            "265 iter PSNR_dehazing: 26.66 ssim: 0.981516\n",
            "265 iter PSNR: 26.66 time: 0.113779\n",
            "266 iter PSNR_dehazing: 26.24 ssim: 0.978969\n",
            "266 iter PSNR: 26.24 time: 0.114471\n",
            "267 iter PSNR_dehazing: 33.75 ssim: 0.995051\n",
            "267 iter PSNR: 33.75 time: 0.113939\n",
            "268 iter PSNR_dehazing: 31.19 ssim: 0.992229\n",
            "268 iter PSNR: 31.19 time: 0.114325\n",
            "269 iter PSNR_dehazing: 30.91 ssim: 0.991484\n",
            "269 iter PSNR: 30.91 time: 0.113081\n",
            "270 iter PSNR_dehazing: 29.69 ssim: 0.989168\n",
            "270 iter PSNR: 29.69 time: 0.114590\n",
            "271 iter PSNR_dehazing: 28.51 ssim: 0.986695\n",
            "271 iter PSNR: 28.51 time: 0.111976\n",
            "272 iter PSNR_dehazing: 26.57 ssim: 0.981542\n",
            "272 iter PSNR: 26.57 time: 0.112677\n",
            "273 iter PSNR_dehazing: 26.86 ssim: 0.980442\n",
            "273 iter PSNR: 26.86 time: 0.110693\n",
            "274 iter PSNR_dehazing: 31.89 ssim: 0.993656\n",
            "274 iter PSNR: 31.89 time: 0.113408\n",
            "275 iter PSNR_dehazing: 30.66 ssim: 0.991673\n",
            "275 iter PSNR: 30.66 time: 0.111522\n",
            "276 iter PSNR_dehazing: 28.99 ssim: 0.988795\n",
            "276 iter PSNR: 28.99 time: 0.114173\n",
            "277 iter PSNR_dehazing: 28.75 ssim: 0.987706\n",
            "277 iter PSNR: 28.75 time: 0.112712\n",
            "278 iter PSNR_dehazing: 27.78 ssim: 0.985176\n",
            "278 iter PSNR: 27.78 time: 0.115391\n",
            "279 iter PSNR_dehazing: 26.23 ssim: 0.980425\n",
            "279 iter PSNR: 26.23 time: 0.113686\n",
            "280 iter PSNR_dehazing: 25.71 ssim: 0.977410\n",
            "280 iter PSNR: 25.71 time: 0.115286\n",
            "281 iter PSNR_dehazing: 31.04 ssim: 0.982957\n",
            "281 iter PSNR: 31.04 time: 0.113256\n",
            "282 iter PSNR_dehazing: 29.59 ssim: 0.978244\n",
            "282 iter PSNR: 29.59 time: 0.115728\n",
            "283 iter PSNR_dehazing: 28.68 ssim: 0.975075\n",
            "283 iter PSNR: 28.68 time: 0.112515\n",
            "284 iter PSNR_dehazing: 27.67 ssim: 0.971732\n",
            "284 iter PSNR: 27.67 time: 0.113709\n",
            "285 iter PSNR_dehazing: 27.26 ssim: 0.969261\n",
            "285 iter PSNR: 27.26 time: 0.113157\n",
            "286 iter PSNR_dehazing: 26.27 ssim: 0.963766\n",
            "286 iter PSNR: 26.27 time: 0.112846\n",
            "287 iter PSNR_dehazing: 25.17 ssim: 0.956647\n",
            "287 iter PSNR: 25.17 time: 0.111551\n",
            "288 iter PSNR_dehazing: 31.35 ssim: 0.983811\n",
            "288 iter PSNR: 31.35 time: 0.112503\n",
            "289 iter PSNR_dehazing: 29.83 ssim: 0.979292\n",
            "289 iter PSNR: 29.83 time: 0.111044\n",
            "290 iter PSNR_dehazing: 28.50 ssim: 0.974963\n",
            "290 iter PSNR: 28.50 time: 0.113076\n",
            "291 iter PSNR_dehazing: 28.00 ssim: 0.972641\n",
            "291 iter PSNR: 28.00 time: 0.112470\n",
            "292 iter PSNR_dehazing: 27.63 ssim: 0.970528\n",
            "292 iter PSNR: 27.63 time: 0.113820\n",
            "293 iter PSNR_dehazing: 26.51 ssim: 0.964691\n",
            "293 iter PSNR: 26.51 time: 0.112235\n",
            "294 iter PSNR_dehazing: 25.72 ssim: 0.959100\n",
            "294 iter PSNR: 25.72 time: 0.115505\n",
            "295 iter PSNR_dehazing: 30.26 ssim: 0.981434\n",
            "295 iter PSNR: 30.26 time: 0.113944\n",
            "296 iter PSNR_dehazing: 28.78 ssim: 0.976561\n",
            "296 iter PSNR: 28.78 time: 0.114852\n",
            "297 iter PSNR_dehazing: 28.20 ssim: 0.973481\n",
            "297 iter PSNR: 28.20 time: 0.113005\n",
            "298 iter PSNR_dehazing: 27.48 ssim: 0.969973\n",
            "298 iter PSNR: 27.48 time: 0.113102\n",
            "299 iter PSNR_dehazing: 27.03 ssim: 0.967086\n",
            "299 iter PSNR: 27.03 time: 0.110821\n",
            "300 iter PSNR_dehazing: 25.36 ssim: 0.959827\n",
            "300 iter PSNR: 25.36 time: 0.112756\n",
            "301 iter PSNR_dehazing: 23.99 ssim: 0.950079\n",
            "301 iter PSNR: 23.99 time: 0.110717\n",
            "302 iter PSNR_dehazing: 30.56 ssim: 0.982147\n",
            "302 iter PSNR: 30.56 time: 0.112649\n",
            "303 iter PSNR_dehazing: 29.22 ssim: 0.977315\n",
            "303 iter PSNR: 29.22 time: 0.113701\n",
            "304 iter PSNR_dehazing: 28.53 ssim: 0.974521\n",
            "304 iter PSNR: 28.53 time: 0.113854\n",
            "305 iter PSNR_dehazing: 27.75 ssim: 0.971112\n",
            "305 iter PSNR: 27.75 time: 0.113684\n",
            "306 iter PSNR_dehazing: 27.00 ssim: 0.968653\n",
            "306 iter PSNR: 27.00 time: 0.114677\n",
            "307 iter PSNR_dehazing: 25.62 ssim: 0.961022\n",
            "307 iter PSNR: 25.62 time: 0.113894\n",
            "308 iter PSNR_dehazing: 24.53 ssim: 0.953074\n",
            "308 iter PSNR: 24.53 time: 0.113959\n",
            "309 iter PSNR_dehazing: 29.76 ssim: 0.981012\n",
            "309 iter PSNR: 29.76 time: 0.111634\n",
            "310 iter PSNR_dehazing: 28.01 ssim: 0.974998\n",
            "310 iter PSNR: 28.01 time: 0.112296\n",
            "311 iter PSNR_dehazing: 27.31 ssim: 0.971806\n",
            "311 iter PSNR: 27.31 time: 0.110878\n",
            "312 iter PSNR_dehazing: 26.69 ssim: 0.968729\n",
            "312 iter PSNR: 26.69 time: 0.114434\n",
            "313 iter PSNR_dehazing: 26.05 ssim: 0.964862\n",
            "313 iter PSNR: 26.05 time: 0.111402\n",
            "314 iter PSNR_dehazing: 24.70 ssim: 0.956614\n",
            "314 iter PSNR: 24.70 time: 0.112436\n",
            "315 iter PSNR_dehazing: 23.64 ssim: 0.948601\n",
            "315 iter PSNR: 23.64 time: 0.112964\n",
            "316 iter PSNR_dehazing: 33.19 ssim: 0.979458\n",
            "316 iter PSNR: 33.19 time: 0.115051\n",
            "317 iter PSNR_dehazing: 32.16 ssim: 0.975489\n",
            "317 iter PSNR: 32.16 time: 0.112918\n",
            "318 iter PSNR_dehazing: 31.53 ssim: 0.973760\n",
            "318 iter PSNR: 31.53 time: 0.113226\n",
            "319 iter PSNR_dehazing: 30.79 ssim: 0.970564\n",
            "319 iter PSNR: 30.79 time: 0.110541\n",
            "320 iter PSNR_dehazing: 30.04 ssim: 0.967356\n",
            "320 iter PSNR: 30.04 time: 0.111804\n",
            "321 iter PSNR_dehazing: 29.14 ssim: 0.962916\n",
            "321 iter PSNR: 29.14 time: 0.109864\n",
            "322 iter PSNR_dehazing: 28.01 ssim: 0.953584\n",
            "322 iter PSNR: 28.01 time: 0.112167\n",
            "323 iter PSNR_dehazing: 33.43 ssim: 0.979976\n",
            "323 iter PSNR: 33.43 time: 0.111051\n",
            "324 iter PSNR_dehazing: 32.26 ssim: 0.975755\n",
            "324 iter PSNR: 32.26 time: 0.114913\n",
            "325 iter PSNR_dehazing: 31.75 ssim: 0.974400\n",
            "325 iter PSNR: 31.75 time: 0.113306\n",
            "326 iter PSNR_dehazing: 31.24 ssim: 0.972199\n",
            "326 iter PSNR: 31.24 time: 0.114138\n",
            "327 iter PSNR_dehazing: 30.83 ssim: 0.970120\n",
            "327 iter PSNR: 30.83 time: 0.113332\n",
            "328 iter PSNR_dehazing: 30.10 ssim: 0.966022\n",
            "328 iter PSNR: 30.10 time: 0.112641\n",
            "329 iter PSNR_dehazing: 28.59 ssim: 0.956621\n",
            "329 iter PSNR: 28.59 time: 0.110861\n",
            "330 iter PSNR_dehazing: 33.04 ssim: 0.979274\n",
            "330 iter PSNR: 33.04 time: 0.111512\n",
            "331 iter PSNR_dehazing: 32.21 ssim: 0.975640\n",
            "331 iter PSNR: 32.21 time: 0.110234\n",
            "332 iter PSNR_dehazing: 31.13 ssim: 0.971642\n",
            "332 iter PSNR: 31.13 time: 0.113100\n",
            "333 iter PSNR_dehazing: 30.34 ssim: 0.969158\n",
            "333 iter PSNR: 30.34 time: 0.112382\n",
            "334 iter PSNR_dehazing: 29.66 ssim: 0.965755\n",
            "334 iter PSNR: 29.66 time: 0.113200\n",
            "335 iter PSNR_dehazing: 28.27 ssim: 0.958071\n",
            "335 iter PSNR: 28.27 time: 0.112837\n",
            "336 iter PSNR_dehazing: 26.91 ssim: 0.947107\n",
            "336 iter PSNR: 26.91 time: 0.115109\n",
            "337 iter PSNR_dehazing: 32.90 ssim: 0.978412\n",
            "337 iter PSNR: 32.90 time: 0.112719\n",
            "338 iter PSNR_dehazing: 32.39 ssim: 0.976478\n",
            "338 iter PSNR: 32.39 time: 0.113146\n",
            "339 iter PSNR_dehazing: 31.19 ssim: 0.972826\n",
            "339 iter PSNR: 31.19 time: 0.111275\n",
            "340 iter PSNR_dehazing: 30.42 ssim: 0.969495\n",
            "340 iter PSNR: 30.42 time: 0.110702\n",
            "341 iter PSNR_dehazing: 29.94 ssim: 0.967580\n",
            "341 iter PSNR: 29.94 time: 0.109490\n",
            "342 iter PSNR_dehazing: 28.98 ssim: 0.962783\n",
            "342 iter PSNR: 28.98 time: 0.112035\n",
            "343 iter PSNR_dehazing: 27.57 ssim: 0.952781\n",
            "343 iter PSNR: 27.57 time: 0.111027\n",
            "344 iter PSNR_dehazing: 32.12 ssim: 0.977367\n",
            "344 iter PSNR: 32.12 time: 0.112948\n",
            "345 iter PSNR_dehazing: 31.24 ssim: 0.974040\n",
            "345 iter PSNR: 31.24 time: 0.112737\n",
            "346 iter PSNR_dehazing: 30.38 ssim: 0.970731\n",
            "346 iter PSNR: 30.38 time: 0.111806\n",
            "347 iter PSNR_dehazing: 29.65 ssim: 0.967335\n",
            "347 iter PSNR: 29.65 time: 0.111429\n",
            "348 iter PSNR_dehazing: 28.98 ssim: 0.964150\n",
            "348 iter PSNR: 28.98 time: 0.111977\n",
            "349 iter PSNR_dehazing: 27.78 ssim: 0.957908\n",
            "349 iter PSNR: 27.78 time: 0.110329\n",
            "350 iter PSNR_dehazing: 26.14 ssim: 0.942931\n",
            "350 iter PSNR: 26.14 time: 0.113010\n",
            "351 iter PSNR_dehazing: 35.40 ssim: 0.989267\n",
            "351 iter PSNR: 35.40 time: 0.111128\n",
            "352 iter PSNR_dehazing: 34.15 ssim: 0.986850\n",
            "352 iter PSNR: 34.15 time: 0.112371\n",
            "353 iter PSNR_dehazing: 33.19 ssim: 0.984608\n",
            "353 iter PSNR: 33.19 time: 0.111381\n",
            "354 iter PSNR_dehazing: 32.65 ssim: 0.983903\n",
            "354 iter PSNR: 32.65 time: 0.111978\n",
            "355 iter PSNR_dehazing: 32.21 ssim: 0.983301\n",
            "355 iter PSNR: 32.21 time: 0.110128\n",
            "356 iter PSNR_dehazing: 33.26 ssim: 0.984825\n",
            "356 iter PSNR: 33.26 time: 0.112281\n",
            "357 iter PSNR_dehazing: 33.71 ssim: 0.984149\n",
            "357 iter PSNR: 33.71 time: 0.110919\n",
            "358 iter PSNR_dehazing: 36.10 ssim: 0.989928\n",
            "358 iter PSNR: 36.10 time: 0.112085\n",
            "359 iter PSNR_dehazing: 35.04 ssim: 0.987497\n",
            "359 iter PSNR: 35.04 time: 0.109805\n",
            "360 iter PSNR_dehazing: 34.65 ssim: 0.986708\n",
            "360 iter PSNR: 34.65 time: 0.110720\n",
            "361 iter PSNR_dehazing: 34.55 ssim: 0.986707\n",
            "361 iter PSNR: 34.55 time: 0.110299\n",
            "362 iter PSNR_dehazing: 34.85 ssim: 0.987688\n",
            "362 iter PSNR: 34.85 time: 0.112065\n",
            "363 iter PSNR_dehazing: 34.84 ssim: 0.986820\n",
            "363 iter PSNR: 34.84 time: 0.111834\n",
            "364 iter PSNR_dehazing: 33.95 ssim: 0.983911\n",
            "364 iter PSNR: 33.95 time: 0.113727\n",
            "365 iter PSNR_dehazing: 34.91 ssim: 0.988414\n",
            "365 iter PSNR: 34.91 time: 0.110806\n",
            "366 iter PSNR_dehazing: 34.25 ssim: 0.986624\n",
            "366 iter PSNR: 34.25 time: 0.111419\n",
            "367 iter PSNR_dehazing: 33.95 ssim: 0.986161\n",
            "367 iter PSNR: 33.95 time: 0.109755\n",
            "368 iter PSNR_dehazing: 33.55 ssim: 0.985116\n",
            "368 iter PSNR: 33.55 time: 0.112575\n",
            "369 iter PSNR_dehazing: 33.07 ssim: 0.983740\n",
            "369 iter PSNR: 33.07 time: 0.111618\n",
            "370 iter PSNR_dehazing: 33.46 ssim: 0.983311\n",
            "370 iter PSNR: 33.46 time: 0.111598\n",
            "371 iter PSNR_dehazing: 33.08 ssim: 0.981912\n",
            "371 iter PSNR: 33.08 time: 0.112780\n",
            "372 iter PSNR_dehazing: 35.12 ssim: 0.988991\n",
            "372 iter PSNR: 35.12 time: 0.113085\n",
            "373 iter PSNR_dehazing: 33.59 ssim: 0.984945\n",
            "373 iter PSNR: 33.59 time: 0.110806\n",
            "374 iter PSNR_dehazing: 33.64 ssim: 0.985743\n",
            "374 iter PSNR: 33.64 time: 0.110692\n",
            "375 iter PSNR_dehazing: 32.91 ssim: 0.984264\n",
            "375 iter PSNR: 32.91 time: 0.109092\n",
            "376 iter PSNR_dehazing: 32.39 ssim: 0.982473\n",
            "376 iter PSNR: 32.39 time: 0.111920\n",
            "377 iter PSNR_dehazing: 33.33 ssim: 0.983915\n",
            "377 iter PSNR: 33.33 time: 0.110269\n",
            "378 iter PSNR_dehazing: 33.22 ssim: 0.982543\n",
            "378 iter PSNR: 33.22 time: 0.112553\n",
            "379 iter PSNR_dehazing: 34.78 ssim: 0.988281\n",
            "379 iter PSNR: 34.78 time: 0.111026\n",
            "380 iter PSNR_dehazing: 34.04 ssim: 0.986071\n",
            "380 iter PSNR: 34.04 time: 0.111105\n",
            "381 iter PSNR_dehazing: 34.26 ssim: 0.986619\n",
            "381 iter PSNR: 34.26 time: 0.110506\n",
            "382 iter PSNR_dehazing: 33.55 ssim: 0.984491\n",
            "382 iter PSNR: 33.55 time: 0.112532\n",
            "383 iter PSNR_dehazing: 33.51 ssim: 0.984868\n",
            "383 iter PSNR: 33.51 time: 0.110112\n",
            "384 iter PSNR_dehazing: 33.38 ssim: 0.983525\n",
            "384 iter PSNR: 33.38 time: 0.112885\n",
            "385 iter PSNR_dehazing: 32.43 ssim: 0.980167\n",
            "385 iter PSNR: 32.43 time: 0.110783\n",
            "386 iter PSNR_dehazing: 40.81 ssim: 0.997570\n",
            "386 iter PSNR: 40.81 time: 0.111259\n",
            "387 iter PSNR_dehazing: 38.63 ssim: 0.996398\n",
            "387 iter PSNR: 38.63 time: 0.110370\n",
            "388 iter PSNR_dehazing: 37.43 ssim: 0.995599\n",
            "388 iter PSNR: 37.43 time: 0.110832\n",
            "389 iter PSNR_dehazing: 36.01 ssim: 0.994530\n",
            "389 iter PSNR: 36.01 time: 0.111051\n",
            "390 iter PSNR_dehazing: 35.01 ssim: 0.993638\n",
            "390 iter PSNR: 35.01 time: 0.112457\n",
            "391 iter PSNR_dehazing: 33.45 ssim: 0.991671\n",
            "391 iter PSNR: 33.45 time: 0.111562\n",
            "392 iter PSNR_dehazing: 31.84 ssim: 0.989575\n",
            "392 iter PSNR: 31.84 time: 0.111238\n",
            "393 iter PSNR_dehazing: 41.56 ssim: 0.997721\n",
            "393 iter PSNR: 41.56 time: 0.110188\n",
            "394 iter PSNR_dehazing: 39.37 ssim: 0.996671\n",
            "394 iter PSNR: 39.37 time: 0.111608\n",
            "395 iter PSNR_dehazing: 37.84 ssim: 0.995705\n",
            "395 iter PSNR: 37.84 time: 0.110451\n",
            "396 iter PSNR_dehazing: 36.25 ssim: 0.994915\n",
            "396 iter PSNR: 36.25 time: 0.113693\n",
            "397 iter PSNR_dehazing: 35.53 ssim: 0.993966\n",
            "397 iter PSNR: 35.53 time: 0.110778\n",
            "398 iter PSNR_dehazing: 33.51 ssim: 0.991814\n",
            "398 iter PSNR: 33.51 time: 0.111626\n",
            "399 iter PSNR_dehazing: 32.62 ssim: 0.990357\n",
            "399 iter PSNR: 32.62 time: 0.110295\n",
            "400 iter PSNR_dehazing: 40.26 ssim: 0.997293\n",
            "400 iter PSNR: 40.26 time: 0.112223\n",
            "401 iter PSNR_dehazing: 38.07 ssim: 0.996047\n",
            "401 iter PSNR: 38.07 time: 0.109800\n",
            "402 iter PSNR_dehazing: 36.48 ssim: 0.994975\n",
            "402 iter PSNR: 36.48 time: 0.112490\n",
            "403 iter PSNR_dehazing: 34.99 ssim: 0.993622\n",
            "403 iter PSNR: 34.99 time: 0.111823\n",
            "404 iter PSNR_dehazing: 34.37 ssim: 0.993244\n",
            "404 iter PSNR: 34.37 time: 0.111138\n",
            "405 iter PSNR_dehazing: 32.52 ssim: 0.990702\n",
            "405 iter PSNR: 32.52 time: 0.109589\n",
            "406 iter PSNR_dehazing: 31.35 ssim: 0.988649\n",
            "406 iter PSNR: 31.35 time: 0.111883\n",
            "407 iter PSNR_dehazing: 40.65 ssim: 0.997515\n",
            "407 iter PSNR: 40.65 time: 0.110145\n",
            "408 iter PSNR_dehazing: 38.56 ssim: 0.996322\n",
            "408 iter PSNR: 38.56 time: 0.112680\n",
            "409 iter PSNR_dehazing: 36.80 ssim: 0.995298\n",
            "409 iter PSNR: 36.80 time: 0.111684\n",
            "410 iter PSNR_dehazing: 35.52 ssim: 0.994125\n",
            "410 iter PSNR: 35.52 time: 0.112594\n",
            "411 iter PSNR_dehazing: 34.58 ssim: 0.993158\n",
            "411 iter PSNR: 34.58 time: 0.110956\n",
            "412 iter PSNR_dehazing: 32.96 ssim: 0.991325\n",
            "412 iter PSNR: 32.96 time: 0.110105\n",
            "413 iter PSNR_dehazing: 31.72 ssim: 0.989403\n",
            "413 iter PSNR: 31.72 time: 0.110321\n",
            "414 iter PSNR_dehazing: 39.87 ssim: 0.997344\n",
            "414 iter PSNR: 39.87 time: 0.111926\n",
            "415 iter PSNR_dehazing: 37.43 ssim: 0.995835\n",
            "415 iter PSNR: 37.43 time: 0.111548\n",
            "416 iter PSNR_dehazing: 35.80 ssim: 0.994659\n",
            "416 iter PSNR: 35.80 time: 0.112137\n",
            "417 iter PSNR_dehazing: 34.85 ssim: 0.993584\n",
            "417 iter PSNR: 34.85 time: 0.110489\n",
            "418 iter PSNR_dehazing: 33.87 ssim: 0.992599\n",
            "418 iter PSNR: 33.87 time: 0.110167\n",
            "419 iter PSNR_dehazing: 31.87 ssim: 0.989944\n",
            "419 iter PSNR: 31.87 time: 0.110533\n",
            "420 iter PSNR_dehazing: 30.58 ssim: 0.987902\n",
            "420 iter PSNR: 30.58 time: 0.113200\n",
            "421 iter PSNR_dehazing: 32.39 ssim: 0.969446\n",
            "421 iter PSNR: 32.39 time: 0.110770\n",
            "422 iter PSNR_dehazing: 31.16 ssim: 0.961158\n",
            "422 iter PSNR: 31.16 time: 0.113163\n",
            "423 iter PSNR_dehazing: 30.28 ssim: 0.956484\n",
            "423 iter PSNR: 30.28 time: 0.110659\n",
            "424 iter PSNR_dehazing: 29.69 ssim: 0.952692\n",
            "424 iter PSNR: 29.69 time: 0.110805\n",
            "425 iter PSNR_dehazing: 29.83 ssim: 0.955389\n",
            "425 iter PSNR: 29.83 time: 0.109396\n",
            "426 iter PSNR_dehazing: 29.36 ssim: 0.954103\n",
            "426 iter PSNR: 29.36 time: 0.111269\n",
            "427 iter PSNR_dehazing: 28.56 ssim: 0.948942\n",
            "427 iter PSNR: 28.56 time: 0.110707\n",
            "428 iter PSNR_dehazing: 33.08 ssim: 0.972516\n",
            "428 iter PSNR: 33.08 time: 0.112244\n",
            "429 iter PSNR_dehazing: 31.03 ssim: 0.960021\n",
            "429 iter PSNR: 31.03 time: 0.109846\n",
            "430 iter PSNR_dehazing: 30.30 ssim: 0.955523\n",
            "430 iter PSNR: 30.30 time: 0.110613\n",
            "431 iter PSNR_dehazing: 29.94 ssim: 0.953860\n",
            "431 iter PSNR: 29.94 time: 0.110965\n",
            "432 iter PSNR_dehazing: 29.69 ssim: 0.953332\n",
            "432 iter PSNR: 29.69 time: 0.111487\n",
            "433 iter PSNR_dehazing: 29.33 ssim: 0.952543\n",
            "433 iter PSNR: 29.33 time: 0.110206\n",
            "434 iter PSNR_dehazing: 28.92 ssim: 0.950794\n",
            "434 iter PSNR: 28.92 time: 0.112259\n",
            "435 iter PSNR_dehazing: 32.04 ssim: 0.967749\n",
            "435 iter PSNR: 32.04 time: 0.110426\n",
            "436 iter PSNR_dehazing: 30.84 ssim: 0.959774\n",
            "436 iter PSNR: 30.84 time: 0.109385\n",
            "437 iter PSNR_dehazing: 30.45 ssim: 0.958463\n",
            "437 iter PSNR: 30.45 time: 0.110176\n",
            "438 iter PSNR_dehazing: 30.01 ssim: 0.956518\n",
            "438 iter PSNR: 30.01 time: 0.111645\n",
            "439 iter PSNR_dehazing: 29.79 ssim: 0.956014\n",
            "439 iter PSNR: 29.79 time: 0.111688\n",
            "440 iter PSNR_dehazing: 29.26 ssim: 0.953551\n",
            "440 iter PSNR: 29.26 time: 0.112971\n",
            "441 iter PSNR_dehazing: 28.72 ssim: 0.950724\n",
            "441 iter PSNR: 28.72 time: 0.109052\n",
            "442 iter PSNR_dehazing: 32.19 ssim: 0.968484\n",
            "442 iter PSNR: 32.19 time: 0.111243\n",
            "443 iter PSNR_dehazing: 30.87 ssim: 0.959376\n",
            "443 iter PSNR: 30.87 time: 0.110034\n",
            "444 iter PSNR_dehazing: 30.25 ssim: 0.956579\n",
            "444 iter PSNR: 30.25 time: 0.111901\n",
            "445 iter PSNR_dehazing: 29.84 ssim: 0.955208\n",
            "445 iter PSNR: 29.84 time: 0.110704\n",
            "446 iter PSNR_dehazing: 29.48 ssim: 0.953025\n",
            "446 iter PSNR: 29.48 time: 0.110214\n",
            "447 iter PSNR_dehazing: 29.05 ssim: 0.951543\n",
            "447 iter PSNR: 29.05 time: 0.109638\n",
            "448 iter PSNR_dehazing: 28.43 ssim: 0.947842\n",
            "448 iter PSNR: 28.43 time: 0.111337\n",
            "449 iter PSNR_dehazing: 31.77 ssim: 0.965954\n",
            "449 iter PSNR: 31.77 time: 0.110759\n",
            "450 iter PSNR_dehazing: 30.48 ssim: 0.958367\n",
            "450 iter PSNR: 30.48 time: 0.111812\n",
            "451 iter PSNR_dehazing: 29.85 ssim: 0.955140\n",
            "451 iter PSNR: 29.85 time: 0.109599\n",
            "452 iter PSNR_dehazing: 29.34 ssim: 0.952765\n",
            "452 iter PSNR: 29.34 time: 0.110031\n",
            "453 iter PSNR_dehazing: 29.28 ssim: 0.953800\n",
            "453 iter PSNR: 29.28 time: 0.109283\n",
            "454 iter PSNR_dehazing: 28.77 ssim: 0.951036\n",
            "454 iter PSNR: 28.77 time: 0.112214\n",
            "455 iter PSNR_dehazing: 28.25 ssim: 0.948140\n",
            "455 iter PSNR: 28.25 time: 0.111025\n",
            "456 iter PSNR_dehazing: 33.85 ssim: 0.996288\n",
            "456 iter PSNR: 33.85 time: 0.111082\n",
            "457 iter PSNR_dehazing: 31.95 ssim: 0.994799\n",
            "457 iter PSNR: 31.95 time: 0.109116\n",
            "458 iter PSNR_dehazing: 31.28 ssim: 0.993712\n",
            "458 iter PSNR: 31.28 time: 0.111097\n",
            "459 iter PSNR_dehazing: 30.78 ssim: 0.992730\n",
            "459 iter PSNR: 30.78 time: 0.110403\n",
            "460 iter PSNR_dehazing: 30.20 ssim: 0.991595\n",
            "460 iter PSNR: 30.20 time: 0.112071\n",
            "461 iter PSNR_dehazing: 28.76 ssim: 0.988787\n",
            "461 iter PSNR: 28.76 time: 0.110908\n",
            "462 iter PSNR_dehazing: 28.00 ssim: 0.986784\n",
            "462 iter PSNR: 28.00 time: 0.111180\n",
            "463 iter PSNR_dehazing: 35.50 ssim: 0.996806\n",
            "463 iter PSNR: 35.50 time: 0.108606\n",
            "464 iter PSNR_dehazing: 33.23 ssim: 0.995334\n",
            "464 iter PSNR: 33.23 time: 0.111919\n",
            "465 iter PSNR_dehazing: 32.24 ssim: 0.994390\n",
            "465 iter PSNR: 32.24 time: 0.110163\n",
            "466 iter PSNR_dehazing: 31.41 ssim: 0.993349\n",
            "466 iter PSNR: 31.41 time: 0.111491\n",
            "467 iter PSNR_dehazing: 30.56 ssim: 0.991974\n",
            "467 iter PSNR: 30.56 time: 0.110827\n",
            "468 iter PSNR_dehazing: 29.55 ssim: 0.990006\n",
            "468 iter PSNR: 29.55 time: 0.111036\n",
            "469 iter PSNR_dehazing: 28.49 ssim: 0.987740\n",
            "469 iter PSNR: 28.49 time: 0.108759\n",
            "470 iter PSNR_dehazing: 32.70 ssim: 0.995692\n",
            "470 iter PSNR: 32.70 time: 0.111888\n",
            "471 iter PSNR_dehazing: 31.91 ssim: 0.994498\n",
            "471 iter PSNR: 31.91 time: 0.111256\n",
            "472 iter PSNR_dehazing: 30.95 ssim: 0.993341\n",
            "472 iter PSNR: 30.95 time: 0.113346\n",
            "473 iter PSNR_dehazing: 29.85 ssim: 0.991754\n",
            "473 iter PSNR: 29.85 time: 0.110799\n",
            "474 iter PSNR_dehazing: 29.27 ssim: 0.990381\n",
            "474 iter PSNR: 29.27 time: 0.111556\n",
            "475 iter PSNR_dehazing: 28.11 ssim: 0.987795\n",
            "475 iter PSNR: 28.11 time: 0.107445\n",
            "476 iter PSNR_dehazing: 26.69 ssim: 0.984215\n",
            "476 iter PSNR: 26.69 time: 0.111006\n",
            "477 iter PSNR_dehazing: 32.85 ssim: 0.995845\n",
            "477 iter PSNR: 32.85 time: 0.110469\n",
            "478 iter PSNR_dehazing: 31.64 ssim: 0.994463\n",
            "478 iter PSNR: 31.64 time: 0.112058\n",
            "479 iter PSNR_dehazing: 31.23 ssim: 0.993580\n",
            "479 iter PSNR: 31.23 time: 0.109268\n",
            "480 iter PSNR_dehazing: 30.32 ssim: 0.992272\n",
            "480 iter PSNR: 30.32 time: 0.110431\n",
            "481 iter PSNR_dehazing: 29.68 ssim: 0.991051\n",
            "481 iter PSNR: 29.68 time: 0.109536\n",
            "482 iter PSNR_dehazing: 28.76 ssim: 0.988759\n",
            "482 iter PSNR: 28.76 time: 0.110958\n",
            "483 iter PSNR_dehazing: 27.27 ssim: 0.985603\n",
            "483 iter PSNR: 27.27 time: 0.110888\n",
            "484 iter PSNR_dehazing: 32.47 ssim: 0.995507\n",
            "484 iter PSNR: 32.47 time: 0.110958\n",
            "485 iter PSNR_dehazing: 31.39 ssim: 0.994124\n",
            "485 iter PSNR: 31.39 time: 0.108372\n",
            "486 iter PSNR_dehazing: 30.55 ssim: 0.992922\n",
            "486 iter PSNR: 30.55 time: 0.110715\n",
            "487 iter PSNR_dehazing: 29.70 ssim: 0.991522\n",
            "487 iter PSNR: 29.70 time: 0.109870\n",
            "488 iter PSNR_dehazing: 29.11 ssim: 0.990205\n",
            "488 iter PSNR: 29.11 time: 0.112830\n",
            "489 iter PSNR_dehazing: 27.69 ssim: 0.986946\n",
            "489 iter PSNR: 27.69 time: 0.110575\n",
            "490 iter PSNR_dehazing: 26.43 ssim: 0.983719\n",
            "490 iter PSNR: 26.43 time: 0.109678\n",
            "491 iter PSNR_dehazing: 40.32 ssim: 0.997602\n",
            "491 iter PSNR: 40.32 time: 0.109205\n",
            "492 iter PSNR_dehazing: 38.22 ssim: 0.997350\n",
            "492 iter PSNR: 38.22 time: 0.110934\n",
            "493 iter PSNR_dehazing: 36.05 ssim: 0.996811\n",
            "493 iter PSNR: 36.05 time: 0.110667\n",
            "494 iter PSNR_dehazing: 34.30 ssim: 0.996006\n",
            "494 iter PSNR: 34.30 time: 0.110588\n",
            "495 iter PSNR_dehazing: 32.83 ssim: 0.995477\n",
            "495 iter PSNR: 32.83 time: 0.108711\n",
            "496 iter PSNR_dehazing: 30.54 ssim: 0.993899\n",
            "496 iter PSNR: 30.54 time: 0.111352\n",
            "497 iter PSNR_dehazing: 28.52 ssim: 0.991627\n",
            "497 iter PSNR: 28.52 time: 0.109827\n",
            "498 iter PSNR_dehazing: 40.76 ssim: 0.997513\n",
            "498 iter PSNR: 40.76 time: 0.112764\n",
            "499 iter PSNR_dehazing: 39.55 ssim: 0.997302\n",
            "499 iter PSNR: 39.55 time: 0.110352\n",
            "500 iter PSNR_dehazing: 38.38 ssim: 0.997081\n",
            "500 iter PSNR: 38.38 time: 0.109745\n",
            "501 iter PSNR_dehazing: 37.11 ssim: 0.996562\n",
            "501 iter PSNR: 37.11 time: 0.108036\n",
            "==========================================================\n",
            "The average PSNR is 31.97 dB\n",
            "The average SSIM is 0.98581 dB\n",
            "Average time: 0.111060\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "# from torch.backends import cudnn # Uncomment if you need it\n",
        "\n",
        "class Args:\n",
        "    model_name = 'IRNeXt'\n",
        "    mode = 'test'\n",
        "    data_dir = './reside-mix/train/'\n",
        "\n",
        "    # Train\n",
        "    batch_size = 8\n",
        "    learning_rate = 1e-4\n",
        "    weight_decay = 0\n",
        "    num_epoch = 30\n",
        "    print_freq = 500\n",
        "    num_worker = 8\n",
        "    save_freq = 10\n",
        "    valid_freq = 10\n",
        "    figname = 'outdoor_30epoch.jpg'\n",
        "    resume = ''\n",
        "\n",
        "    # Test\n",
        "    test_model = '/content/drive/MyDrive/temp/simon_best.pkl'\n",
        "    save_image = False\n",
        "\n",
        "    # Directories (set these as per your requirement)\n",
        "    model_save_dir = os.path.join('/content/drive/MyDrive/results/', 'IRNeXt', 'OTS/')\n",
        "    result_dir = os.path.join('/content/drive/MyDrive/results/', model_name, 'test')\n",
        "\n",
        "def main(args):\n",
        "    # CUDNN\n",
        "    # cudnn.benchmark = True # Uncomment if you need it\n",
        "\n",
        "    if not os.path.exists('/content/drive/MyDrive/results/'):\n",
        "        os.makedirs(args.model_save_dir)\n",
        "    if not os.path.exists('/content/drive/MyDrive/results/' + args.model_name + '/'):\n",
        "        os.makedirs('/content/drive/MyDrive/results/' + args.model_name + '/')\n",
        "    if not os.path.exists(args.model_save_dir):\n",
        "        os.makedirs(args.model_save_dir)\n",
        "    if not os.path.exists(args.result_dir):\n",
        "        os.makedirs(args.result_dir)\n",
        "\n",
        "    model = build_net()  # Make sure to define build_net or import it if it's defined elsewhere\n",
        "    #print(model)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "    if args.mode == 'train':\n",
        "        _train(model, args)  # Make sure to define _train or import it if it's defined elsewhere\n",
        "\n",
        "    elif args.mode == 'test':\n",
        "        _eval(model, args)   # Make sure to define _eval or import it if it's defined elsewhere\n",
        "\n",
        "# Replace parser.parse_args() with an instance of the Args class\n",
        "args = Args()\n",
        "if not os.path.exists(args.model_save_dir):\n",
        "    os.makedirs(args.model_save_dir)\n",
        "# Copying files (make sure these paths are correct)\n",
        "command = 'cp ' + 'models/layers.py ' + args.model_save_dir\n",
        "os.system(command)\n",
        "command = 'cp ' + 'models/IRNeXt.py ' + args.model_save_dir\n",
        "os.system(command)\n",
        "command = 'cp ' + 'train.py ' + args.model_save_dir\n",
        "os.system(command)\n",
        "command = 'cp ' + 'main.py ' + args.model_save_dir\n",
        "os.system(command)\n",
        "print(args)\n",
        "main(args)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main (Mixup)"
      ],
      "metadata": {
        "id": "RvTwi00y3xG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "# from torch.backends import cudnn # Uncomment if you need it\n",
        "\n",
        "class Args:\n",
        "    model_name = 'IRNeXt'\n",
        "    mode = 'train'\n",
        "    data_dir = './reside-mix/'\n",
        "\n",
        "    # Train\n",
        "    batch_size = 8\n",
        "    learning_rate = 1e-4\n",
        "    weight_decay = 0\n",
        "    num_epoch = 30\n",
        "    print_freq = 500\n",
        "    num_worker = 8\n",
        "    save_freq = 10\n",
        "    valid_freq = 10\n",
        "    figname = 'mix_30epoch.jpg'\n",
        "    resume = ''\n",
        "    #'/content/drive/MyDrive/results_mix/IRNeXt/OTS/model.pkl'\n",
        "\n",
        "    # Test\n",
        "    test_model = '/content/drive/MyDrive/results_mix/IRNeXt/OTS/Final.pkl'\n",
        "    # save_image = False\n",
        "\n",
        "    # Directories (set these as per your requirement)\n",
        "    model_save_dir = os.path.join('/content/drive/MyDrive/results_mix/', 'IRNeXt', 'OTS/')\n",
        "    result_dir = os.path.join('/content/drive/MyDrive/results_mix/', model_name, 'test')\n",
        "\n",
        "def main(args):\n",
        "    # CUDNN\n",
        "    # cudnn.benchmark = True # Uncomment if you need it\n",
        "\n",
        "    if not os.path.exists('/content/drive/MyDrive/results_mix/'):\n",
        "        os.makedirs(args.model_save_dir)\n",
        "    if not os.path.exists('/content/drive/MyDrive/results_mix/' + args.model_name + '/'):\n",
        "        os.makedirs('/content/drive/MyDrive/results_mix/' + args.model_name + '/')\n",
        "    if not os.path.exists(args.model_save_dir):\n",
        "        os.makedirs(args.model_save_dir)\n",
        "    if not os.path.exists(args.result_dir):\n",
        "        os.makedirs(args.result_dir)\n",
        "\n",
        "    model = build_net()  # Make sure to define build_net or import it if it's defined elsewhere\n",
        "    #print(model)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "    if args.mode == 'train':\n",
        "        _train(model, args)  # Make sure to define _train or import it if it's defined elsewhere\n",
        "\n",
        "    elif args.mode == 'test':\n",
        "        _eval(model, args)   # Make sure to define _eval or import it if it's defined elsewhere\n",
        "\n",
        "# Replace parser.parse_args() with an instance of the Args class\n",
        "args = Args()\n",
        "if not os.path.exists(args.model_save_dir):\n",
        "    os.makedirs(args.model_save_dir)\n",
        "# Copying files (make sure these paths are correct)\n",
        "command = 'cp ' + 'models/layers.py ' + args.model_save_dir\n",
        "os.system(command)\n",
        "command = 'cp ' + 'models/IRNeXt.py ' + args.model_save_dir\n",
        "os.system(command)\n",
        "command = 'cp ' + 'train.py ' + args.model_save_dir\n",
        "os.system(command)\n",
        "command = 'cp ' + 'main.py ' + args.model_save_dir\n",
        "os.system(command)\n",
        "print(args)\n",
        "main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywmyq21qr0Pb",
        "outputId": "4dc03ffc-c698-454a-ef10-7ae3c4433d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<__main__.Args object at 0x7b917e6b74f0>\n",
            "training LR: 0.0001\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time:  1.0974 Epoch: 001 Iter:  500/40967 LR: 0.0000333333 Loss content:  0.2463 Loss fft:  8.0278\n",
            "Time:  1.0686 Epoch: 001 Iter: 1000/40967 LR: 0.0000333333 Loss content:  0.2181 Loss fft:  6.4606\n",
            "Time:  1.0682 Epoch: 001 Iter: 1500/40967 LR: 0.0000333333 Loss content:  0.2167 Loss fft:  6.3689\n",
            "Time:  1.0683 Epoch: 001 Iter: 2000/40967 LR: 0.0000333333 Loss content:  0.2049 Loss fft:  6.1052\n",
            "Time:  1.0683 Epoch: 001 Iter: 2500/40967 LR: 0.0000333333 Loss content:  0.2054 Loss fft:  6.1183\n",
            "Time:  1.0736 Epoch: 001 Iter: 3000/40967 LR: 0.0000333333 Loss content:  0.2015 Loss fft:  5.7990\n",
            "Time:  1.0683 Epoch: 001 Iter: 3500/40967 LR: 0.0000333333 Loss content:  0.1994 Loss fft:  5.7666\n",
            "Time:  1.0680 Epoch: 001 Iter: 4000/40967 LR: 0.0000333333 Loss content:  0.1965 Loss fft:  5.7144\n",
            "Time:  1.0679 Epoch: 001 Iter: 4500/40967 LR: 0.0000333333 Loss content:  0.1910 Loss fft:  5.5271\n",
            "Time:  1.0732 Epoch: 001 Iter: 5000/40967 LR: 0.0000333333 Loss content:  0.1893 Loss fft:  5.4750\n",
            "Time:  1.0680 Epoch: 001 Iter: 5500/40967 LR: 0.0000333333 Loss content:  0.1794 Loss fft:  5.2215\n",
            "Time:  1.0694 Epoch: 001 Iter: 6000/40967 LR: 0.0000333333 Loss content:  0.1866 Loss fft:  5.5109\n",
            "Time:  1.0699 Epoch: 001 Iter: 6500/40967 LR: 0.0000333333 Loss content:  0.1734 Loss fft:  5.0018\n",
            "Time:  1.0695 Epoch: 001 Iter: 7000/40967 LR: 0.0000333333 Loss content:  0.1836 Loss fft:  5.3694\n",
            "Time:  1.0721 Epoch: 001 Iter: 7500/40967 LR: 0.0000333333 Loss content:  0.1864 Loss fft:  5.4662\n",
            "Time:  1.0691 Epoch: 001 Iter: 8000/40967 LR: 0.0000333333 Loss content:  0.1742 Loss fft:  5.1590\n",
            "Time:  1.0712 Epoch: 001 Iter: 8500/40967 LR: 0.0000333333 Loss content:  0.1848 Loss fft:  5.5859\n",
            "Time:  1.0673 Epoch: 001 Iter: 9000/40967 LR: 0.0000333333 Loss content:  0.1850 Loss fft:  5.5366\n",
            "Time:  1.0720 Epoch: 001 Iter: 9500/40967 LR: 0.0000333333 Loss content:  0.1855 Loss fft:  5.6025\n",
            "Time:  1.0666 Epoch: 001 Iter: 10000/40967 LR: 0.0000333333 Loss content:  0.1851 Loss fft:  5.6813\n",
            "Time:  1.0683 Epoch: 001 Iter: 10500/40967 LR: 0.0000333333 Loss content:  0.1800 Loss fft:  5.3515\n",
            "Time:  1.0694 Epoch: 001 Iter: 11000/40967 LR: 0.0000333333 Loss content:  0.1785 Loss fft:  5.3924\n",
            "Time:  1.0692 Epoch: 001 Iter: 11500/40967 LR: 0.0000333333 Loss content:  0.1944 Loss fft:  5.9113\n",
            "Time:  1.0717 Epoch: 001 Iter: 12000/40967 LR: 0.0000333333 Loss content:  0.1821 Loss fft:  5.5113\n",
            "Time:  1.0688 Epoch: 001 Iter: 12500/40967 LR: 0.0000333333 Loss content:  0.1819 Loss fft:  5.4672\n",
            "Time:  1.0688 Epoch: 001 Iter: 13000/40967 LR: 0.0000333333 Loss content:  0.1774 Loss fft:  5.3040\n",
            "Time:  1.0688 Epoch: 001 Iter: 13500/40967 LR: 0.0000333333 Loss content:  0.1790 Loss fft:  5.2947\n",
            "Time:  1.0724 Epoch: 001 Iter: 14000/40967 LR: 0.0000333333 Loss content:  0.1745 Loss fft:  5.3008\n",
            "Time:  1.0677 Epoch: 001 Iter: 14500/40967 LR: 0.0000333333 Loss content:  0.1647 Loss fft:  4.9163\n",
            "Time:  1.0683 Epoch: 001 Iter: 15000/40967 LR: 0.0000333333 Loss content:  0.1796 Loss fft:  5.4396\n",
            "Time:  1.0702 Epoch: 001 Iter: 15500/40967 LR: 0.0000333333 Loss content:  0.1738 Loss fft:  5.1649\n",
            "Time:  1.0689 Epoch: 001 Iter: 16000/40967 LR: 0.0000333333 Loss content:  0.1719 Loss fft:  5.1993\n",
            "Time:  1.0728 Epoch: 001 Iter: 16500/40967 LR: 0.0000333333 Loss content:  0.1763 Loss fft:  5.3425\n",
            "Time:  1.0704 Epoch: 001 Iter: 17000/40967 LR: 0.0000333333 Loss content:  0.1683 Loss fft:  5.0804\n",
            "Time:  1.0695 Epoch: 001 Iter: 17500/40967 LR: 0.0000333333 Loss content:  0.1744 Loss fft:  5.3345\n",
            "Time:  1.0683 Epoch: 001 Iter: 18000/40967 LR: 0.0000333333 Loss content:  0.1811 Loss fft:  5.5837\n",
            "Time:  1.0719 Epoch: 001 Iter: 18500/40967 LR: 0.0000333333 Loss content:  0.1733 Loss fft:  5.3512\n",
            "Time:  1.0690 Epoch: 001 Iter: 19000/40967 LR: 0.0000333333 Loss content:  0.1742 Loss fft:  5.3078\n",
            "Time:  1.0703 Epoch: 001 Iter: 19500/40967 LR: 0.0000333333 Loss content:  0.1815 Loss fft:  5.5961\n",
            "Time:  1.0688 Epoch: 001 Iter: 20000/40967 LR: 0.0000333333 Loss content:  0.1709 Loss fft:  5.2626\n",
            "Time:  1.0676 Epoch: 001 Iter: 20500/40967 LR: 0.0000333333 Loss content:  0.1668 Loss fft:  5.0801\n",
            "Time:  1.0726 Epoch: 001 Iter: 21000/40967 LR: 0.0000333333 Loss content:  0.1725 Loss fft:  5.3059\n",
            "Time:  1.0703 Epoch: 001 Iter: 21500/40967 LR: 0.0000333333 Loss content:  0.1712 Loss fft:  5.1502\n",
            "Time:  1.0681 Epoch: 001 Iter: 22000/40967 LR: 0.0000333333 Loss content:  0.1663 Loss fft:  5.0564\n",
            "Time:  1.0690 Epoch: 001 Iter: 22500/40967 LR: 0.0000333333 Loss content:  0.1672 Loss fft:  5.1540\n",
            "Time:  1.0692 Epoch: 001 Iter: 23000/40967 LR: 0.0000333333 Loss content:  0.1691 Loss fft:  5.2439\n",
            "Time:  1.0728 Epoch: 001 Iter: 23500/40967 LR: 0.0000333333 Loss content:  0.1636 Loss fft:  4.9889\n",
            "Time:  1.0672 Epoch: 001 Iter: 24000/40967 LR: 0.0000333333 Loss content:  0.1698 Loss fft:  5.2227\n",
            "Time:  1.0668 Epoch: 001 Iter: 24500/40967 LR: 0.0000333333 Loss content:  0.1698 Loss fft:  5.2918\n",
            "Time:  1.0685 Epoch: 001 Iter: 25000/40967 LR: 0.0000333333 Loss content:  0.1685 Loss fft:  5.2056\n",
            "Time:  1.0704 Epoch: 001 Iter: 25500/40967 LR: 0.0000333333 Loss content:  0.1725 Loss fft:  5.3093\n",
            "Time:  1.0672 Epoch: 001 Iter: 26000/40967 LR: 0.0000333333 Loss content:  0.1677 Loss fft:  5.2138\n",
            "Time:  1.0673 Epoch: 001 Iter: 26500/40967 LR: 0.0000333333 Loss content:  0.1716 Loss fft:  5.3165\n",
            "Time:  1.0666 Epoch: 001 Iter: 27000/40967 LR: 0.0000333333 Loss content:  0.1654 Loss fft:  5.1436\n",
            "Time:  1.0664 Epoch: 001 Iter: 27500/40967 LR: 0.0000333333 Loss content:  0.1765 Loss fft:  5.5108\n",
            "Time:  1.0709 Epoch: 001 Iter: 28000/40967 LR: 0.0000333333 Loss content:  0.1679 Loss fft:  5.1401\n",
            "Time:  1.0665 Epoch: 001 Iter: 28500/40967 LR: 0.0000333333 Loss content:  0.1625 Loss fft:  5.0278\n",
            "Time:  1.0675 Epoch: 001 Iter: 29000/40967 LR: 0.0000333333 Loss content:  0.1567 Loss fft:  4.8331\n",
            "Time:  1.0672 Epoch: 001 Iter: 29500/40967 LR: 0.0000333333 Loss content:  0.1689 Loss fft:  5.2845\n",
            "Time:  1.0705 Epoch: 001 Iter: 30000/40967 LR: 0.0000333333 Loss content:  0.1657 Loss fft:  5.1382\n",
            "Time:  1.0677 Epoch: 001 Iter: 30500/40967 LR: 0.0000333333 Loss content:  0.1652 Loss fft:  5.1926\n",
            "Time:  1.0675 Epoch: 001 Iter: 31000/40967 LR: 0.0000333333 Loss content:  0.1603 Loss fft:  4.9722\n",
            "Time:  1.0660 Epoch: 001 Iter: 31500/40967 LR: 0.0000333333 Loss content:  0.1620 Loss fft:  5.0530\n",
            "Time:  1.0669 Epoch: 001 Iter: 32000/40967 LR: 0.0000333333 Loss content:  0.1644 Loss fft:  5.2066\n",
            "Time:  1.0715 Epoch: 001 Iter: 32500/40967 LR: 0.0000333333 Loss content:  0.1590 Loss fft:  4.9163\n",
            "Time:  1.0682 Epoch: 001 Iter: 33000/40967 LR: 0.0000333333 Loss content:  0.1667 Loss fft:  5.2167\n",
            "Time:  1.0668 Epoch: 001 Iter: 33500/40967 LR: 0.0000333333 Loss content:  0.1559 Loss fft:  4.8610\n",
            "Time:  1.0662 Epoch: 001 Iter: 34000/40967 LR: 0.0000333333 Loss content:  0.1609 Loss fft:  5.0139\n",
            "Time:  1.0707 Epoch: 001 Iter: 34500/40967 LR: 0.0000333333 Loss content:  0.1532 Loss fft:  4.7617\n",
            "Time:  1.0659 Epoch: 001 Iter: 35000/40967 LR: 0.0000333333 Loss content:  0.1652 Loss fft:  5.2532\n",
            "Time:  1.0663 Epoch: 001 Iter: 35500/40967 LR: 0.0000333333 Loss content:  0.1643 Loss fft:  5.1710\n",
            "Time:  1.0669 Epoch: 001 Iter: 36000/40967 LR: 0.0000333333 Loss content:  0.1616 Loss fft:  5.0712\n",
            "Time:  1.0660 Epoch: 001 Iter: 36500/40967 LR: 0.0000333333 Loss content:  0.1581 Loss fft:  4.9066\n",
            "Time:  1.0708 Epoch: 001 Iter: 37000/40967 LR: 0.0000333333 Loss content:  0.1624 Loss fft:  5.1827\n",
            "Time:  1.0659 Epoch: 001 Iter: 37500/40967 LR: 0.0000333333 Loss content:  0.1641 Loss fft:  5.1866\n",
            "Time:  1.0671 Epoch: 001 Iter: 38000/40967 LR: 0.0000333333 Loss content:  0.1611 Loss fft:  5.0847\n",
            "Time:  1.0670 Epoch: 001 Iter: 38500/40967 LR: 0.0000333333 Loss content:  0.1617 Loss fft:  5.1600\n",
            "Time:  1.0706 Epoch: 001 Iter: 39000/40967 LR: 0.0000333333 Loss content:  0.1584 Loss fft:  4.9655\n",
            "Time:  1.0669 Epoch: 001 Iter: 39500/40967 LR: 0.0000333333 Loss content:  0.1558 Loss fft:  4.9170\n",
            "Time:  1.0669 Epoch: 001 Iter: 40000/40967 LR: 0.0000333333 Loss content:  0.1570 Loss fft:  4.8952\n",
            "Time:  1.0654 Epoch: 001 Iter: 40500/40967 LR: 0.0000333333 Loss content:  0.1564 Loss fft:  4.8914\n",
            "EPOCH: 01\n",
            "Elapsed time: 87.61 Epoch Pixel Loss:  0.1747 Epoch FFT Loss:  5.3333\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time:  1.0896 Epoch: 002 Iter:  500/40967 LR: 0.0000666667 Loss content:  0.1591 Loss fft:  4.9934\n",
            "Time:  1.0675 Epoch: 002 Iter: 1000/40967 LR: 0.0000666667 Loss content:  0.1651 Loss fft:  5.1704\n",
            "Time:  1.0670 Epoch: 002 Iter: 1500/40967 LR: 0.0000666667 Loss content:  0.1520 Loss fft:  4.6706\n",
            "Time:  1.0672 Epoch: 002 Iter: 2000/40967 LR: 0.0000666667 Loss content:  0.1668 Loss fft:  5.1652\n",
            "Time:  1.0713 Epoch: 002 Iter: 2500/40967 LR: 0.0000666667 Loss content:  0.1601 Loss fft:  5.0085\n",
            "Time:  1.0672 Epoch: 002 Iter: 3000/40967 LR: 0.0000666667 Loss content:  0.1678 Loss fft:  5.2460\n",
            "Time:  1.0679 Epoch: 002 Iter: 3500/40967 LR: 0.0000666667 Loss content:  0.1609 Loss fft:  5.0463\n",
            "Time:  1.0669 Epoch: 002 Iter: 4000/40967 LR: 0.0000666667 Loss content:  0.1590 Loss fft:  4.9635\n",
            "Time:  1.0665 Epoch: 002 Iter: 4500/40967 LR: 0.0000666667 Loss content:  0.1640 Loss fft:  5.2001\n",
            "Time:  1.0711 Epoch: 002 Iter: 5000/40967 LR: 0.0000666667 Loss content:  0.1634 Loss fft:  5.0989\n",
            "Time:  1.0667 Epoch: 002 Iter: 5500/40967 LR: 0.0000666667 Loss content:  0.1639 Loss fft:  5.1210\n",
            "Time:  1.0673 Epoch: 002 Iter: 6000/40967 LR: 0.0000666667 Loss content:  0.1625 Loss fft:  5.0655\n",
            "Time:  1.0676 Epoch: 002 Iter: 6500/40967 LR: 0.0000666667 Loss content:  0.1591 Loss fft:  4.9792\n",
            "Time:  1.0720 Epoch: 002 Iter: 7000/40967 LR: 0.0000666667 Loss content:  0.1571 Loss fft:  4.8276\n",
            "Time:  1.0665 Epoch: 002 Iter: 7500/40967 LR: 0.0000666667 Loss content:  0.1601 Loss fft:  5.0064\n",
            "Time:  1.0676 Epoch: 002 Iter: 8000/40967 LR: 0.0000666667 Loss content:  0.1606 Loss fft:  5.0485\n",
            "Time:  1.0672 Epoch: 002 Iter: 8500/40967 LR: 0.0000666667 Loss content:  0.1560 Loss fft:  4.9445\n",
            "Time:  1.0673 Epoch: 002 Iter: 9000/40967 LR: 0.0000666667 Loss content:  0.1533 Loss fft:  4.7852\n",
            "Time:  1.0728 Epoch: 002 Iter: 9500/40967 LR: 0.0000666667 Loss content:  0.1529 Loss fft:  4.7656\n",
            "Time:  1.0664 Epoch: 002 Iter: 10000/40967 LR: 0.0000666667 Loss content:  0.1494 Loss fft:  4.6496\n",
            "Time:  1.0669 Epoch: 002 Iter: 10500/40967 LR: 0.0000666667 Loss content:  0.1535 Loss fft:  4.8030\n",
            "Time:  1.0663 Epoch: 002 Iter: 11000/40967 LR: 0.0000666667 Loss content:  0.1616 Loss fft:  5.1686\n",
            "Time:  1.0707 Epoch: 002 Iter: 11500/40967 LR: 0.0000666667 Loss content:  0.1578 Loss fft:  4.9682\n",
            "Time:  1.0681 Epoch: 002 Iter: 12000/40967 LR: 0.0000666667 Loss content:  0.1546 Loss fft:  4.8234\n",
            "Time:  1.0669 Epoch: 002 Iter: 12500/40967 LR: 0.0000666667 Loss content:  0.1538 Loss fft:  4.9116\n",
            "Time:  1.0663 Epoch: 002 Iter: 13000/40967 LR: 0.0000666667 Loss content:  0.1551 Loss fft:  4.9252\n",
            "Time:  1.0669 Epoch: 002 Iter: 13500/40967 LR: 0.0000666667 Loss content:  0.1541 Loss fft:  4.9329\n",
            "Time:  1.0719 Epoch: 002 Iter: 14000/40967 LR: 0.0000666667 Loss content:  0.1520 Loss fft:  4.8293\n",
            "Time:  1.0682 Epoch: 002 Iter: 14500/40967 LR: 0.0000666667 Loss content:  0.1625 Loss fft:  5.1427\n",
            "Time:  1.0667 Epoch: 002 Iter: 15000/40967 LR: 0.0000666667 Loss content:  0.1584 Loss fft:  5.0227\n",
            "Time:  1.0671 Epoch: 002 Iter: 15500/40967 LR: 0.0000666667 Loss content:  0.1534 Loss fft:  4.8541\n",
            "Time:  1.0706 Epoch: 002 Iter: 16000/40967 LR: 0.0000666667 Loss content:  0.1580 Loss fft:  5.0400\n",
            "Time:  1.0677 Epoch: 002 Iter: 16500/40967 LR: 0.0000666667 Loss content:  0.1481 Loss fft:  4.7384\n",
            "Time:  1.0666 Epoch: 002 Iter: 17000/40967 LR: 0.0000666667 Loss content:  0.1478 Loss fft:  4.6678\n",
            "Time:  1.0681 Epoch: 002 Iter: 17500/40967 LR: 0.0000666667 Loss content:  0.1553 Loss fft:  4.9684\n",
            "Time:  1.0686 Epoch: 002 Iter: 18000/40967 LR: 0.0000666667 Loss content:  0.1536 Loss fft:  4.8472\n",
            "Time:  1.0717 Epoch: 002 Iter: 18500/40967 LR: 0.0000666667 Loss content:  0.1535 Loss fft:  4.8771\n",
            "Time:  1.0678 Epoch: 002 Iter: 19000/40967 LR: 0.0000666667 Loss content:  0.1530 Loss fft:  4.8964\n",
            "Time:  1.0674 Epoch: 002 Iter: 19500/40967 LR: 0.0000666667 Loss content:  0.1527 Loss fft:  4.8924\n",
            "Time:  1.0682 Epoch: 002 Iter: 20000/40967 LR: 0.0000666667 Loss content:  0.1715 Loss fft:  5.5363\n",
            "Time:  1.0746 Epoch: 002 Iter: 20500/40967 LR: 0.0000666667 Loss content:  0.1478 Loss fft:  4.7373\n",
            "Time:  1.0675 Epoch: 002 Iter: 21000/40967 LR: 0.0000666667 Loss content:  0.1455 Loss fft:  4.6148\n",
            "Time:  1.0672 Epoch: 002 Iter: 21500/40967 LR: 0.0000666667 Loss content:  0.1661 Loss fft:  5.4401\n",
            "Time:  1.0688 Epoch: 002 Iter: 22000/40967 LR: 0.0000666667 Loss content:  0.1540 Loss fft:  4.9133\n",
            "Time:  1.0689 Epoch: 002 Iter: 22500/40967 LR: 0.0000666667 Loss content:  0.1443 Loss fft:  4.5609\n",
            "Time:  1.0729 Epoch: 002 Iter: 23000/40967 LR: 0.0000666667 Loss content:  0.1522 Loss fft:  4.8641\n",
            "Time:  1.0679 Epoch: 002 Iter: 23500/40967 LR: 0.0000666667 Loss content:  0.1565 Loss fft:  5.0276\n",
            "Time:  1.0667 Epoch: 002 Iter: 24000/40967 LR: 0.0000666667 Loss content:  0.1495 Loss fft:  4.8379\n",
            "Time:  1.0716 Epoch: 002 Iter: 24500/40967 LR: 0.0000666667 Loss content:  0.1558 Loss fft:  4.9752\n",
            "Time:  1.0733 Epoch: 002 Iter: 25000/40967 LR: 0.0000666667 Loss content:  0.1520 Loss fft:  4.8617\n",
            "Time:  1.0689 Epoch: 002 Iter: 25500/40967 LR: 0.0000666667 Loss content:  0.1604 Loss fft:  5.1788\n",
            "Time:  1.0692 Epoch: 002 Iter: 26000/40967 LR: 0.0000666667 Loss content:  0.1456 Loss fft:  4.7575\n",
            "Time:  1.0686 Epoch: 002 Iter: 26500/40967 LR: 0.0000666667 Loss content:  0.1496 Loss fft:  4.8546\n",
            "Time:  1.0680 Epoch: 002 Iter: 27000/40967 LR: 0.0000666667 Loss content:  0.1467 Loss fft:  4.7398\n",
            "Time:  1.0716 Epoch: 002 Iter: 27500/40967 LR: 0.0000666667 Loss content:  0.1541 Loss fft:  4.9568\n",
            "Time:  1.0687 Epoch: 002 Iter: 28000/40967 LR: 0.0000666667 Loss content:  0.1461 Loss fft:  4.6995\n",
            "Time:  1.0682 Epoch: 002 Iter: 28500/40967 LR: 0.0000666667 Loss content:  0.1543 Loss fft:  4.9624\n",
            "Time:  1.0687 Epoch: 002 Iter: 29000/40967 LR: 0.0000666667 Loss content:  0.1528 Loss fft:  4.9502\n",
            "Time:  1.0734 Epoch: 002 Iter: 29500/40967 LR: 0.0000666667 Loss content:  0.1517 Loss fft:  4.8674\n",
            "Time:  1.0690 Epoch: 002 Iter: 30000/40967 LR: 0.0000666667 Loss content:  0.1509 Loss fft:  4.8710\n",
            "Time:  1.0692 Epoch: 002 Iter: 30500/40967 LR: 0.0000666667 Loss content:  0.1500 Loss fft:  4.8086\n",
            "Time:  1.0677 Epoch: 002 Iter: 31000/40967 LR: 0.0000666667 Loss content:  0.1484 Loss fft:  4.8010\n",
            "Time:  1.0696 Epoch: 002 Iter: 31500/40967 LR: 0.0000666667 Loss content:  0.1449 Loss fft:  4.7369\n",
            "Time:  1.0738 Epoch: 002 Iter: 32000/40967 LR: 0.0000666667 Loss content:  0.1559 Loss fft:  5.0510\n",
            "Time:  1.0677 Epoch: 002 Iter: 32500/40967 LR: 0.0000666667 Loss content:  0.1437 Loss fft:  4.6403\n",
            "Time:  1.0679 Epoch: 002 Iter: 33000/40967 LR: 0.0000666667 Loss content:  0.1500 Loss fft:  4.9026\n",
            "Time:  1.0671 Epoch: 002 Iter: 33500/40967 LR: 0.0000666667 Loss content:  0.1417 Loss fft:  4.5534\n",
            "Time:  1.0728 Epoch: 002 Iter: 34000/40967 LR: 0.0000666667 Loss content:  0.1492 Loss fft:  4.8304\n",
            "Time:  1.0684 Epoch: 002 Iter: 34500/40967 LR: 0.0000666667 Loss content:  0.1524 Loss fft:  4.9810\n",
            "Time:  1.0683 Epoch: 002 Iter: 35000/40967 LR: 0.0000666667 Loss content:  0.1519 Loss fft:  4.9693\n",
            "Time:  1.0684 Epoch: 002 Iter: 35500/40967 LR: 0.0000666667 Loss content:  0.1503 Loss fft:  4.8840\n",
            "Time:  1.0687 Epoch: 002 Iter: 36000/40967 LR: 0.0000666667 Loss content:  0.1508 Loss fft:  4.8791\n",
            "Time:  1.0728 Epoch: 002 Iter: 36500/40967 LR: 0.0000666667 Loss content:  0.1485 Loss fft:  4.7887\n",
            "Time:  1.0683 Epoch: 002 Iter: 37000/40967 LR: 0.0000666667 Loss content:  0.1499 Loss fft:  4.9400\n",
            "Time:  1.0688 Epoch: 002 Iter: 37500/40967 LR: 0.0000666667 Loss content:  0.1443 Loss fft:  4.6993\n",
            "Time:  1.0685 Epoch: 002 Iter: 38000/40967 LR: 0.0000666667 Loss content:  0.1453 Loss fft:  4.6986\n",
            "Time:  1.0735 Epoch: 002 Iter: 38500/40967 LR: 0.0000666667 Loss content:  0.1519 Loss fft:  4.9860\n",
            "Time:  1.0672 Epoch: 002 Iter: 39000/40967 LR: 0.0000666667 Loss content:  0.1494 Loss fft:  4.8340\n",
            "Time:  1.0677 Epoch: 002 Iter: 39500/40967 LR: 0.0000666667 Loss content:  0.1448 Loss fft:  4.7390\n",
            "Time:  1.0681 Epoch: 002 Iter: 40000/40967 LR: 0.0000666667 Loss content:  0.1493 Loss fft:  4.9226\n",
            "Time:  1.0695 Epoch: 002 Iter: 40500/40967 LR: 0.0000666667 Loss content:  0.1460 Loss fft:  4.7766\n",
            "EPOCH: 02\n",
            "Elapsed time: 87.61 Epoch Pixel Loss:  0.1536 Epoch FFT Loss:  4.9051\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time:  1.0854 Epoch: 003 Iter:  500/40967 LR: 0.0001000000 Loss content:  0.1491 Loss fft:  4.8301\n",
            "Time:  1.0701 Epoch: 003 Iter: 1000/40967 LR: 0.0001000000 Loss content:  0.1534 Loss fft:  4.9391\n",
            "Time:  1.0732 Epoch: 003 Iter: 1500/40967 LR: 0.0001000000 Loss content:  0.1464 Loss fft:  4.7754\n",
            "Time:  1.0788 Epoch: 003 Iter: 2000/40967 LR: 0.0001000000 Loss content:  0.1484 Loss fft:  4.8250\n",
            "Time:  1.0759 Epoch: 003 Iter: 2500/40967 LR: 0.0001000000 Loss content:  0.1461 Loss fft:  4.7118\n",
            "Time:  1.0759 Epoch: 003 Iter: 2500/40967 LR: 0.0001000000 Loss content:  0.1461 Loss fft:  4.7118\n",
            "Time:  1.0731 Epoch: 003 Iter: 3000/40967 LR: 0.0001000000 Loss content:  0.1498 Loss fft:  4.8228\n",
            "Time:  1.0731 Epoch: 003 Iter: 3000/40967 LR: 0.0001000000 Loss content:  0.1498 Loss fft:  4.8228\n",
            "Time:  1.0701 Epoch: 003 Iter: 3500/40967 LR: 0.0001000000 Loss content:  0.1524 Loss fft:  4.9602\n",
            "Time:  1.0701 Epoch: 003 Iter: 3500/40967 LR: 0.0001000000 Loss content:  0.1524 Loss fft:  4.9602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68pANInIV7kJ",
        "outputId": "472f3d47-cdbc-483e-ec3a-fc05d617d92c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./reside-mix/train/gt/343*.jpg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJRgDMe5ABAq",
        "outputId": "452c3583-db08-444b-f78e-389a5cbbae26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access './reside-mix/train/gt/343*.jpg': No such file or directory\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}