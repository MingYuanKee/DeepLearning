{"cells":[{"cell_type":"markdown","metadata":{"id":"KL7RQ4RQH1q8"},"source":["Prep\n","=="]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18008,"status":"ok","timestamp":1700708871602,"user":{"displayName":"General","userId":"01731632481034566404"},"user_tz":-480},"id":"vLPM1G2oyK_V","outputId":"2e0bdad7-3776-494b-bf6a-bb6ca1275c14"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6616,"status":"ok","timestamp":1700708878213,"user":{"displayName":"General","userId":"01731632481034566404"},"user_tz":-480},"id":"R5aGTHRISo4o","outputId":"b3f91f94-f3ed-4976-a742-a8f18c7bca7a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.14.1)\n","Collecting einops\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.19.3)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n","Collecting pytorch_msssim\n","  Downloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n","Collecting warmup-scheduler\n","  Downloading warmup_scheduler-0.3.tar.gz (2.1 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py\u003e=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n","Requirement already satisfied: grpcio\u003e=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.59.2)\n","Requirement already satisfied: google-auth\u003c3,\u003e=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib\u003c1.1,\u003e=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.0.0)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.5.1)\n","Requirement already satisfied: numpy\u003e=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.23.5)\n","Requirement already satisfied: protobuf\u003e=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n","Requirement already satisfied: requests\u003c3,\u003e=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n","Requirement already satisfied: setuptools\u003e=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n","Requirement already satisfied: six\u003e1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server\u003c0.8.0,\u003e=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n","Requirement already satisfied: werkzeug\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.1)\n","Requirement already satisfied: scipy\u003e=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.11.3)\n","Requirement already satisfied: networkx\u003e=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.2.1)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,\u003e=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (9.4.0)\n","Requirement already satisfied: imageio\u003e=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.31.6)\n","Requirement already satisfied: tifffile\u003e=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2023.9.26)\n","Requirement already satisfied: PyWavelets\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.4.1)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (23.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pytorch_msssim) (2.1.0+cu118)\n","Requirement already satisfied: cachetools\u003c6.0,\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard) (5.3.2)\n","Requirement already satisfied: pyasn1-modules\u003e=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard) (0.3.0)\n","Requirement already satisfied: rsa\u003c5,\u003e=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard) (4.9)\n","Requirement already satisfied: requests-oauthlib\u003e=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib\u003c1.1,\u003e=0.5-\u003etensorboard) (1.3.1)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard) (3.4)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard) (2023.7.22)\n","Requirement already satisfied: MarkupSafe\u003e=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug\u003e=1.0.1-\u003etensorboard) (2.1.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch-\u003epytorch_msssim) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch-\u003epytorch_msssim) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch-\u003epytorch_msssim) (1.12)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-\u003epytorch_msssim) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-\u003epytorch_msssim) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch-\u003epytorch_msssim) (2.1.0)\n","Requirement already satisfied: pyasn1\u003c0.6.0,\u003e=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules\u003e=0.2.1-\u003egoogle-auth\u003c3,\u003e=1.6.3-\u003etensorboard) (0.5.0)\n","Requirement already satisfied: oauthlib\u003e=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib\u003e=0.7.0-\u003egoogle-auth-oauthlib\u003c1.1,\u003e=0.5-\u003etensorboard) (3.2.2)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch-\u003epytorch_msssim) (1.3.0)\n","Building wheels for collected packages: warmup-scheduler\n","  Building wheel for warmup-scheduler (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for warmup-scheduler: filename=warmup_scheduler-0.3-py3-none-any.whl size=2968 sha256=db442add55b9a99776527c3b596d8e3be1bd358527ae5e88c6e6b0422f11576f\n","  Stored in directory: /root/.cache/pip/wheels/59/01/9e/d1820991c32916e9808c940f572b462f3e46427f3e76c4d852\n","Successfully built warmup-scheduler\n","Installing collected packages: warmup-scheduler, einops, pytorch_msssim\n","Successfully installed einops-0.7.0 pytorch_msssim-1.0.0 warmup-scheduler-0.3\n"]}],"source":["!pip install tensorboard einops scikit-image opencv-python pytorch_msssim warmup-scheduler"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5211,"status":"ok","timestamp":1700708883417,"user":{"displayName":"General","userId":"01731632481034566404"},"user_tz":-480},"id":"eM3Sq-7tUl35","outputId":"55ae4116-65df-4f79-b26e-cffb60ce6170"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pytorch-msssim in /usr/local/lib/python3.10/dist-packages (1.0.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pytorch-msssim) (2.1.0+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch-\u003epytorch-msssim) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch-\u003epytorch-msssim) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch-\u003epytorch-msssim) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch-\u003epytorch-msssim) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-\u003epytorch-msssim) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-\u003epytorch-msssim) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch-\u003epytorch-msssim) (2.1.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch-\u003epytorch-msssim) (2.1.3)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch-\u003epytorch-msssim) (1.3.0)\n"]}],"source":["!pip install pytorch-msssim"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4409,"status":"ok","timestamp":1700708887819,"user":{"displayName":"General","userId":"01731632481034566404"},"user_tz":-480},"id":"j961Got9Uzd0","outputId":"31beb949-d93d-4cc7-e59f-bf4f3a108ff2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: warmup-scheduler in /usr/local/lib/python3.10/dist-packages (0.3)\n"]}],"source":["!pip install warmup-scheduler\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":544110,"status":"ok","timestamp":1700709431921,"user":{"displayName":"General","userId":"01731632481034566404"},"user_tz":-480},"id":"A0JSBeWuBY3R"},"outputs":[],"source":["!unzip  ./drive/MyDrive/temp/outdoor_train.zip -d ./reside-outdoorMY/ \u003e trainlog.txt\n","!unzip  ./drive/MyDrive/temp/outdoor_test.zip -d ./reside-outdoorMY/ \u003e testlog.txt\n","\n","#/content/drive/MyDrive/temp/outdoor_train.zip\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1700709431922,"user":{"displayName":"General","userId":"01731632481034566404"},"user_tz":-480},"id":"xhWsuX95jwvB"},"outputs":[],"source":["path = \"/content/reside-outdoorMY\"\n","#/content/reside-indoorMY/train/hazy\n","#path = \"/content/reside-indoorMY\"\n","#path = \"./reside-indoorMY\""]},{"cell_type":"markdown","metadata":{"id":"_h78ryOOBvc2"},"source":["Data Augment\n","=="]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":6204,"status":"ok","timestamp":1700709438117,"user":{"displayName":"General","userId":"01731632481034566404"},"user_tz":-480},"id":"3z9B1WKSCD5O"},"outputs":[],"source":["import random\n","import torchvision.transforms as transforms\n","import torchvision.transforms.functional as FUNCTIONAL\n","\n","\n","class PairRandomCrop(transforms.RandomCrop):\n","\n","    def __call__(self, image, label):\n","\n","        if self.padding is not None:\n","            image = FUNCTIONAL.pad(image, self.padding, self.fill, self.padding_mode)\n","            label = FUNCTIONAL.pad(label, self.padding, self.fill, self.padding_mode)\n","\n","        # pad the width if needed\n","        if self.pad_if_needed and image.size[0] \u003c self.size[1]:\n","            image = FUNCTIONAL.pad(image, (self.size[1] - image.size[0], 0), self.fill, self.padding_mode)\n","            label = FUNCTIONAL.pad(label, (self.size[1] - label.size[0], 0), self.fill, self.padding_mode)\n","        # pad the height if needed\n","        if self.pad_if_needed and image.size[1] \u003c self.size[0]:\n","            image = FUNCTIONAL.pad(image, (0, self.size[0] - image.size[1]), self.fill, self.padding_mode)\n","            label = FUNCTIONAL.pad(label, (0, self.size[0] - image.size[1]), self.fill, self.padding_mode)\n","\n","        i, j, h, w = self.get_params(image, self.size)\n","\n","        return FUNCTIONAL.crop(image, i, j, h, w), FUNCTIONAL.crop(label, i, j, h, w)\n","\n","\n","class PairCompose(transforms.Compose):\n","    def __call__(self, image, label):\n","        for t in self.transforms:\n","            image, label = t(image, label)\n","        return image, label\n","\n","\n","class PairRandomHorizontalFilp(transforms.RandomHorizontalFlip):\n","    def __call__(self, img, label):\n","        \"\"\"\n","        Args:\n","            img (PIL Image): Image to be flipped.\n","\n","        Returns:\n","            PIL Image: Randomly flipped image.\n","        \"\"\"\n","        if random.random() \u003c self.p:\n","            return FUNCTIONAL.hflip(img), FUNCTIONAL.hflip(label)\n","        return img, label\n","\n","\n","class PairToTensor(transforms.ToTensor):\n","    def __call__(self, pic, label):\n","        \"\"\"\n","        Args:\n","            pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n","\n","        Returns:\n","            Tensor: Converted image.\n","        \"\"\"\n","        return FUNCTIONAL.to_tensor(pic), FUNCTIONAL.to_tensor(label)"]},{"cell_type":"markdown","metadata":{"id":"Ik-THSPrBomo"},"source":["Data Loader\n","=="]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1700709438118,"user":{"displayName":"General","userId":"01731632481034566404"},"user_tz":-480},"id":"3rFIJk2JBsbA"},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","from PIL import Image as Image\n","from torchvision.transforms import functional as Functional\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.transforms.functional import to_tensor, resize\n","\n","def my_collate_fn(batch):\n","    images, labels = zip(*batch)  # Unzipping images and labels\n","    # Define the target size\n","    target_height = 256  # Example height\n","    target_width = 256   # Example width\n","\n","    # Resize and ensure all tensors have the same number of channels, in this case, 3\n","    resized_images = [resize(img, (target_height, target_width))[:3, :, :] if img.shape[0] \u003e 3 else resize(img, (target_height, target_width)) for img in images]\n","    resized_labels = [resize(lbl, (target_height, target_width))[:3, :, :] if lbl.shape[0] \u003e 3 else resize(lbl, (target_height, target_width)) for lbl in labels]\n","\n","    # Stack all images and labels\n","    images = torch.stack(resized_images)\n","    labels = torch.stack(resized_labels)\n","\n","    return images, labels\n","\n","def test_collate_fn(batch):\n","    images, labels, name = zip(*batch)\n","    # Define the target size\n","    target_height = 256  # Example height\n","    target_width = 256   # Example width\n","\n","    # Resize and ensure all tensors have the same number of channels, in this case, 3\n","    resized_images = [resize(img, (target_height, target_width))[:3, :, :] if img.shape[0] \u003e 3 else resize(img, (target_height, target_width)) for img in images]\n","    resized_labels = [resize(lbl, (target_height, target_width))[:3, :, :] if lbl.shape[0] \u003e 3 else resize(lbl, (target_height, target_width)) for lbl in labels]\n","\n","    # Stack all images and labels\n","    images = torch.stack(resized_images)\n","    labels = torch.stack(resized_labels)\n","\n","    return images, labels, name\n","\n","def my_collate_fn_backup2(batch):\n","    images, labels = zip(*batch)  # Unzipping images and labels\n","\n","    # Ensure all tensors have the same number of channels, in this case, 3\n","    images = [img[:3, :, :] if img.shape[0] \u003e 3 else img for img in images]\n","    labels = [lbl[:3, :, :] if lbl.shape[0] \u003e 3 else lbl for lbl in labels]\n","\n","    # Stack all images and labels\n","    images = torch.stack(images)\n","    labels = torch.stack(labels)\n","\n","    return images, labels\n","\n","def my_collate_fn_backup(batch):\n","    # Adjust the collate function to handle different number of channels\n","    images, labels = zip(*batch)  # Unzipping images and labels\n","\n","    # Ensure that all images and labels have 3 channels\n","    # This assumes images are PIL Images or similar and have a 'convert' method\n","    images = [to_tensor(img.convert(\"RGB\")) for img in images]\n","    labels = [to_tensor(lbl.convert(\"RGB\")) for lbl in labels]\n","\n","    # Stack all images and labels\n","    images = torch.stack(images)\n","    labels = torch.stack(labels)\n","\n","    return images, labels\n","\n","def train_dataloader(path, batch_size=8, num_workers=0, use_transform=True): # we change batch size = 1, but the original batch size = 64\n","    image_dir = os.path.join(path, 'train')\n","\n","    transform = None\n","    if use_transform:\n","        transform = PairCompose(\n","            [\n","                PairRandomCrop(256),\n","                PairRandomHorizontalFilp(),\n","                PairToTensor()\n","            ]\n","        )\n","    dataloader = DataLoader(\n","        DeblurDataset(image_dir, transform=transform),\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=num_workers,\n","        collate_fn=my_collate_fn,\n","        pin_memory=True\n","    )\n","    return dataloader\n","\n","\n","def test_dataloader(path, batch_size=1, num_workers=0):\n","    image_dir = os.path.join(path, 'test')\n","    dataloader = DataLoader(\n","        DeblurDataset(image_dir, is_test=True),\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=num_workers,\n","        collate_fn=test_collate_fn,\n","        pin_memory=True\n","    )\n","\n","    return dataloader\n","\n","\n","def valid_dataloader(path, batch_size=1, num_workers=0):\n","    dataloader = DataLoader(\n","        DeblurDataset(os.path.join(path, 'test', ), is_valid=True),\n","        batch_size=batch_size,\n","        shuffle=False,\n","        collate_fn=my_collate_fn,\n","        num_workers=num_workers\n","    )\n","\n","    return dataloader\n","\n","\n","class DeblurDataset_ex(Dataset):\n","    def __init__(self, image_dir, transform=None, is_test=False):\n","        self.image_dir = image_dir\n","        self.image_list = os.listdir(os.path.join(image_dir, 'hazy/'))\n","        self._check_image(self.image_list)\n","        self.image_list.sort()\n","        self.transform = transform\n","        self.is_test = is_test\n","\n","    def __len__(self):\n","        return len(self.image_list)\n","\n","    def __getitem__(self, idx):\n","        image = Image.open(os.path.join(self.image_dir, 'hazy', self.image_list[idx]))\n","        label = Image.open(os.path.join(self.image_dir, 'gt', self.image_list[idx].split('_')[0]+'.png'))\n","\n","        if self.transform:\n","            image, label = self.transform(image, label)\n","        else:\n","            image = Functional.to_tensor(image)\n","            label = Functional.to_tensor(label)\n","        if self.is_test:\n","            name = self.image_list[idx]\n","            return image, label, name\n","        return image, label\n","\n","    @staticmethod\n","    def _check_image(lst):\n","        for x in lst:\n","            splits = x.split('.')\n","            if splits[-1] not in ['png', 'jpg', 'jpeg']:\n","                raise ValueError\n","\n","import random\n","class DeblurDataset(Dataset):\n","    def __init__(self, image_dir, transform=None, is_test=False, is_valid=False, ps=None):\n","        self.image_dir = image_dir\n","        self.image_list = os.listdir(os.path.join(image_dir, 'hazy/'))\n","        self._check_image(self.image_list)\n","        self.image_list.sort()\n","        self.transform = transform\n","        self.is_test = is_test\n","        self.is_valid = is_valid\n","        self.ps = ps\n","\n","    def __len__(self):\n","        return len(self.image_list)\n","\n","    def __getitem__(self, idx):\n","        image = Image.open(os.path.join(self.image_dir, 'hazy', self.image_list[idx])).convert('RGB')\n","        if self.is_valid or self.is_test:\n","            label = Image.open(os.path.join(self.image_dir, 'gt', self.image_list[idx].split('_')[0]+'.png')).convert('RGB')\n","        else:\n","            try:\n","                label = Image.open(os.path.join(self.image_dir, 'gt', self.image_list[idx].split('_')[0]+'.jpg')).convert('RGB')\n","            except:\n","                label = Image.open(os.path.join(self.image_dir, 'gt', self.image_list[idx].split('_')[0]+'.png')).convert('RGB')\n","        ps = self.ps\n","\n","        if self.ps is not None:\n","            image = Functional.to_tensor(image)\n","            label = Functional.to_tensor(label)\n","\n","            hh, ww = label.shape[1], label.shape[2]\n","\n","            rr = random.randint(0, hh-ps)\n","            cc = random.randint(0, ww-ps)\n","\n","            image = image[:, rr:rr+ps, cc:cc+ps]\n","            label = label[:, rr:rr+ps, cc:cc+ps]\n","\n","            if random.random() \u003c 0.5:\n","                image = image.flip(2)\n","                label = label.flip(2)\n","        else:\n","            image = Functional.to_tensor(image)\n","            label = Functional.to_tensor(label)\n","\n","        if self.is_test:\n","            name = self.image_list[idx]\n","            return image, label, name\n","        return image, label\n","\n","    @staticmethod\n","    def _check_image(lst):\n","        for x in lst:\n","            splits = x.split('.')\n","            if splits[-1] not in ['png', 'jpg', 'jpeg']:\n","                raise ValueError"]},{"cell_type":"markdown","metadata":{"id":"FOmdMPEPBcqU"},"source":["Utils\n","=="]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1700709438118,"user":{"displayName":"General","userId":"01731632481034566404"},"user_tz":-480},"id":"c3KXVt8fBgN_"},"outputs":[],"source":["import time\n","import numpy as np\n","\n","\n","class Adder(object):\n","    def __init__(self):\n","        self.count = 0\n","        self.num = float(0)\n","\n","    def reset(self):\n","        self.count = 0\n","        self.num = float(0)\n","\n","    def __call__(self, num):\n","        self.count += 1\n","        self.num += num\n","\n","    def average(self):\n","        return self.num / self.count\n","\n","\n","class Timer(object):\n","    def __init__(self, option='s'):\n","        self.tm = 0\n","        self.option = option\n","        if option == 's':\n","            self.devider = 1\n","        elif option == 'm':\n","            self.devider = 60\n","        else:\n","            self.devider = 3600\n","\n","    def tic(self):\n","        self.tm = time.time()\n","\n","    def toc(self):\n","        return (time.time() - self.tm) / self.devider\n","\n","\n","def check_lr(optimizer):\n","    for i, param_group in enumerate(optimizer.param_groups):\n","        lr = param_group['lr']\n","    return lr"]},{"cell_type":"markdown","metadata":{"id":"xuIwak2DBSmK"},"source":["Eval\n","=="]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":955,"status":"ok","timestamp":1700709439066,"user":{"displayName":"General","userId":"01731632481034566404"},"user_tz":-480},"id":"WQmJ-vK5BT4n"},"outputs":[],"source":["from skimage.metrics import peak_signal_noise_ratio\n","from pytorch_msssim import ssim\n","import torch.nn.functional as fe\n","\n","from skimage import img_as_ubyte\n","import cv2\n","\n","def _eval(model, args):\n","    state_dict = torch.load(args.test_model)\n","    model.load_state_dict(state_dict['model'])\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    dataloader = test_dataloader(args.data_dir, batch_size=1, num_workers=0)\n","    torch.cuda.empty_cache()\n","    adder = Adder()\n","    model.eval()\n","    factor = 32\n","    with torch.no_grad():\n","        psnr_adder = Adder()\n","        ssim_adder = Adder()\n","\n","        for iter_idx, data in enumerate(dataloader):\n","            input_img, label_img, name = data\n","\n","            input_img = input_img.to(device)\n","\n","            h, w = input_img.shape[2], input_img.shape[3]\n","            H, W = ((h+factor)//factor)*factor, ((w+factor)//factor*factor)\n","            padh = H-h if h%factor!=0 else 0\n","            padw = W-w if w%factor!=0 else 0\n","            input_img = fe.pad(input_img, (0, padw, 0, padh), 'reflect')\n","\n","            tm = time.time()\n","\n","            pred = model(input_img)[2]\n","            pred = pred[:,:,:h,:w]\n","\n","            elapsed = time.time() - tm\n","            adder(elapsed)\n","\n","            pred_clip = torch.clamp(pred, 0, 1)\n","\n","            pred_numpy = pred_clip.squeeze(0).cpu().numpy()\n","            label_numpy = label_img.squeeze(0).cpu().numpy()\n","\n","\n","            label_img = (label_img).cuda()\n","            psnr_val = 10 * torch.log10(1 / fe.mse_loss(pred_clip, label_img))\n","            down_ratio = max(1, round(min(H, W) / 256))\n","            ssim_val = ssim(fe.adaptive_avg_pool2d(pred_clip, (int(H / down_ratio), int(W / down_ratio))),\n","                            fe.adaptive_avg_pool2d(label_img, (int(H / down_ratio), int(W / down_ratio))),\n","                            data_range=1, size_average=False)\n","            print('%d iter PSNR_dehazing: %.2f ssim: %f' % (iter_idx + 1, psnr_val, ssim_val))\n","            ssim_adder(ssim_val)\n","\n","            if args.save_image:\n","                save_name = os.path.join(args.result_dir, name[0])\n","                pred_clip += 0.5 / 255\n","                pred = FUNCTIONAL.to_pil_image(pred_clip.squeeze(0).cpu(), 'RGB')\n","                pred.save(save_name)\n","\n","            psnr_mimo = peak_signal_noise_ratio(pred_numpy, label_numpy, data_range=1)\n","            psnr_adder(psnr_val)\n","\n","            print('%d iter PSNR: %.2f time: %f' % (iter_idx + 1, psnr_mimo, elapsed))\n","\n","        print('==========================================================')\n","        print('The average PSNR is %.2f dB' % (psnr_adder.average()))\n","        print('The average SSIM is %.5f dB' % (ssim_adder.average()))\n","\n","        print(\"Average time: %f\" % adder.average())"]},{"cell_type":"markdown","metadata":{"id":"kiBJcTJbCOcj"},"source":["Layers\n","=="]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1700709439067,"user":{"displayName":"General","userId":"01731632481034566404"},"user_tz":-480},"id":"6qOrc8BLCPaH"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as fl\n","import math\n","\n","class BasicConv(nn.Module):\n","    def __init__(self, in_channel, out_channel, kernel_size, stride, bias=True, norm=False, relu=True, transpose=False):\n","        super(BasicConv, self).__init__()\n","        if bias and norm:\n","            bias = False\n","\n","        padding = kernel_size // 2\n","        layers = list()\n","        if transpose:\n","            padding = kernel_size // 2 -1\n","            layers.append(nn.ConvTranspose2d(in_channel, out_channel, kernel_size, padding=padding, stride=stride, bias=bias))\n","        else:\n","            layers.append(\n","                nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding, stride=stride, bias=bias))\n","        if norm:\n","            layers.append(nn.BatchNorm2d(out_channel))\n","        if relu:\n","            layers.append(nn.GELU())\n","        self.main = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.main(x)\n","\n","\n","class ResBlock(nn.Module):\n","    def __init__(self, in_channel, out_channel, filter=False):\n","        super(ResBlock, self).__init__()\n","        self.main = nn.Sequential(\n","            BasicConv(in_channel, out_channel, kernel_size=3, stride=1, relu=True),\n","            DeepPoolLayer(in_channel, out_channel) if filter else nn.Identity(),\n","            BasicConv(out_channel, out_channel, kernel_size=3, stride=1, relu=False)\n","        )\n","\n","    def forward(self, x):\n","        return self.main(x) + x\n","\n","\n","class DeepPoolLayer(nn.Module):\n","    def __init__(self, k, k_out):\n","        super(DeepPoolLayer, self).__init__()\n","        self.pools_sizes = [8,4,2]\n","        pools, convs, dynas = [],[],[]\n","        for i in self.pools_sizes:\n","            pools.append(nn.AvgPool2d(kernel_size=i, stride=i))\n","            convs.append(nn.Conv2d(k, k, 3, 1, 1, bias=False))\n","            dynas.append(dynamic_filter(inchannels=k, kernel_size=3))\n","        self.pools = nn.ModuleList(pools)\n","        self.convs = nn.ModuleList(convs)\n","        self.dynas = nn.ModuleList(dynas)\n","        self.relu = nn.GELU()\n","        self.conv_sum = nn.Conv2d(k, k_out, 3, 1, 1, bias=False)\n","\n","    def forward(self, x):\n","        x_size = x.size()\n","        resl = x\n","        for i in range(len(self.pools_sizes)):\n","            if i == 0:\n","                y = self.dynas[i](self.convs[i](self.pools[i](x)))\n","            else:\n","                y = self.dynas[i](self.convs[i](self.pools[i](x)+y_up))\n","            resl = torch.add(resl, fl.interpolate(y, x_size[2:], mode='bilinear', align_corners=True))\n","            if i != len(self.pools_sizes)-1:\n","                y_up = fl.interpolate(y, scale_factor=2, mode='bilinear', align_corners=True)\n","        resl = self.relu(resl)\n","        resl = self.conv_sum(resl)\n","\n","        return resl\n","\n","class dynamic_filter(nn.Module):\n","    def __init__(self, inchannels, kernel_size=3, stride=1, group=8):\n","        super(dynamic_filter, self).__init__()\n","\n","        self.stride = stride\n","        self.kernel_size = kernel_size\n","        self.group = group\n","\n","        self.conv = nn.Conv2d(inchannels, group*kernel_size**2, kernel_size=1, stride=1, bias=False)\n","        self.bn = nn.BatchNorm2d(group*kernel_size**2)\n","        self.act = nn.Tanh()\n","\n","        nn.init.kaiming_normal_(self.conv.weight, mode='fan_out', nonlinearity='relu')\n","        self.lamb_l = nn.Parameter(torch.zeros(inchannels), requires_grad=True)\n","        self.lamb_h = nn.Parameter(torch.zeros(inchannels), requires_grad=True)\n","        self.pad = nn.ReflectionPad2d(kernel_size//2)\n","\n","        self.ap = nn.AdaptiveAvgPool2d((1, 1))\n","        self.gap = nn.AdaptiveAvgPool2d(1)\n","\n","        self.inside_all = nn.Parameter(torch.zeros(inchannels,1,1), requires_grad=True)\n","\n","    def forward(self, x):\n","        identity_input = x\n","        # the Conv_{3x3} layer in eq.3 is included in DeepPoolLayer.convs\n","        low_filter = self.ap(x)\n","        low_filter = self.conv(low_filter)\n","        low_filter = self.bn(low_filter)\n","\n","        n, c, h, w = x.shape\n","        x = fl.unfold(self.pad(x), kernel_size=self.kernel_size).reshape(n, self.group, c//self.group, self.kernel_size**2, h*w)\n","\n","        n,c1,p,q = low_filter.shape\n","        low_filter = low_filter.reshape(n, c1//self.kernel_size**2, self.kernel_size**2, p*q).unsqueeze(2)\n","        low_filter = self.act(low_filter)\n","        low_part = torch.sum(x * low_filter, dim=3).reshape(n, c, h, w)\n","\n","        # the variables here are slightly different from the paper: (code) --\u003e (paper)\n","        # low_filter --\u003e A (eq.3)\n","        # In Eq.7, X*A'= X*(A_{l} + WA_{h})\n","        #              = X*A_{l} + WX*(A - A_{l})\n","        #              = X*A_{l} + WX*A - WX*A_{l}\n","        #              = WX*A - X*A_{l}(W-1)\n","        # we substitute gap for A_{l} for simplicity, which is a coarser low-frequency filter\n","        out_low = low_part * (self.inside_all + 1.) - self.inside_all * self.gap(identity_input)\n","\n","        out_low = out_low * self.lamb_l[None,:,None,None]\n","        out_high = (identity_input) * (self.lamb_h[None,:,None,None] + 1.)\n","\n","        return out_low + out_high"]},{"cell_type":"markdown","metadata":{"id":"muFr7px4CKua"},"source":["IRNeXt\n","=="]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1700709439067,"user":{"displayName":"General","userId":"01731632481034566404"},"user_tz":-480},"id":"XNdQN8JdCMEh"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as fwd\n","\n","class EBlock(nn.Module):\n","    def __init__(self, out_channel, num_res=8):\n","        super(EBlock, self).__init__()\n","\n","        layers = [ResBlock(out_channel, out_channel) for _ in range(num_res-1)]\n","        layers.append(ResBlock(out_channel, out_channel, filter=True))\n","        self.layers = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.layers(x)\n","\n","\n","class DBlock(nn.Module):\n","    def __init__(self, channel, num_res=8):\n","        super(DBlock, self).__init__()\n","\n","        layers = [ResBlock(channel, channel) for _ in range(num_res-1)]\n","        layers.append(ResBlock(channel, channel, filter=True))\n","        self.layers = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.layers(x)\n","\n","\n","class SCM(nn.Module):\n","    def __init__(self, out_plane):\n","        super(SCM, self).__init__()\n","\n","        self.main = nn.Sequential(\n","            BasicConv(3, out_plane//4, kernel_size=3, stride=1, relu=True),\n","            BasicConv(out_plane // 4, out_plane // 2, kernel_size=1, stride=1, relu=True),\n","            BasicConv(out_plane // 2, out_plane // 2, kernel_size=3, stride=1, relu=True),\n","            BasicConv(out_plane // 2, out_plane, kernel_size=1, stride=1, relu=False),\n","            nn.InstanceNorm2d(out_plane, affine=True)\n","        )\n","\n","    def forward(self, x):\n","        x = self.main(x)\n","        return x\n","\n","class FAM(nn.Module):\n","    def __init__(self, channel):\n","        super(FAM, self).__init__()\n","\n","        self.merge = BasicConv(channel*2, channel, kernel_size=3, stride=1, relu=False)\n","\n","    def forward(self, x1, x2):\n","        return self.merge(torch.cat([x1, x2], dim=1))\n","\n","class IRNeXt(nn.Module):\n","    def __init__(self, num_res=4):\n","        super(IRNeXt, self).__init__()\n","\n","        base_channel = 32\n","        self.Encoder = nn.ModuleList([\n","            EBlock(base_channel, num_res),\n","            EBlock(base_channel*2, num_res),\n","            EBlock(base_channel*4, num_res),\n","        ])\n","\n","        self.feat_extract = nn.ModuleList([\n","            BasicConv(3, base_channel, kernel_size=3, relu=True, stride=1),\n","            BasicConv(base_channel, base_channel*2, kernel_size=3, relu=True, stride=2),\n","            BasicConv(base_channel*2, base_channel*4, kernel_size=3, relu=True, stride=2),\n","            BasicConv(base_channel*4, base_channel*2, kernel_size=4, relu=True, stride=2, transpose=True),\n","            BasicConv(base_channel*2, base_channel, kernel_size=4, relu=True, stride=2, transpose=True),\n","            BasicConv(base_channel, 3, kernel_size=3, relu=False, stride=1)\n","        ])\n","\n","        self.Decoder = nn.ModuleList([\n","            DBlock(base_channel * 4, num_res),\n","            DBlock(base_channel * 2, num_res),\n","            DBlock(base_channel, num_res)\n","        ])\n","\n","        self.Convs = nn.ModuleList([\n","            BasicConv(base_channel * 4, base_channel * 2, kernel_size=1, relu=True, stride=1),\n","            BasicConv(base_channel * 2, base_channel, kernel_size=1, relu=True, stride=1),\n","        ])\n","\n","        self.ConvsOut = nn.ModuleList(\n","            [\n","                BasicConv(base_channel * 4, 3, kernel_size=3, relu=False, stride=1),\n","                BasicConv(base_channel * 2, 3, kernel_size=3, relu=False, stride=1),\n","            ]\n","        )\n","\n","        self.FAM1 = FAM(base_channel * 4)\n","        self.SCM1 = SCM(base_channel * 4)\n","        self.FAM2 = FAM(base_channel * 2)\n","        self.SCM2 = SCM(base_channel * 2)\n","\n","    def forward(self, x):\n","        x_2 = fwd.interpolate(x, scale_factor=0.5)\n","        x_4 = fwd.interpolate(x_2, scale_factor=0.5)\n","        z2 = self.SCM2(x_2)\n","        z4 = self.SCM1(x_4)\n","\n","        outputs = list()\n","        # 256\n","        x_ = self.feat_extract[0](x)\n","        res1 = self.Encoder[0](x_)\n","        # 128\n","        z = self.feat_extract[1](res1)\n","        z = self.FAM2(z, z2)\n","        res2 = self.Encoder[1](z)\n","        # 64\n","        z = self.feat_extract[2](res2)\n","        z = self.FAM1(z, z4)\n","        z = self.Encoder[2](z)\n","\n","        z = self.Decoder[0](z)\n","        z_ = self.ConvsOut[0](z)\n","        # 128\n","        z = self.feat_extract[3](z)\n","        outputs.append(z_+x_4)\n","\n","        z = torch.cat([z, res2], dim=1)\n","        z = self.Convs[0](z)\n","        z = self.Decoder[1](z)\n","        z_ = self.ConvsOut[1](z)\n","        # 256\n","        z = self.feat_extract[4](z)\n","        outputs.append(z_+x_2)\n","\n","        z = torch.cat([z, res1], dim=1)\n","        z = self.Convs[1](z)\n","        z = self.Decoder[2](z)\n","        z = self.feat_extract[5](z)\n","        outputs.append(z+x)\n","\n","        return outputs\n","\n","\n","def build_net():\n","    return IRNeXt()"]},{"cell_type":"markdown","metadata":{"id":"IZ_iPV8xBg0K"},"source":["Valid\n","=="]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1700709439067,"user":{"displayName":"General","userId":"01731632481034566404"},"user_tz":-480},"id":"kfAK2d13Bjq9"},"outputs":[],"source":["from skimage.metrics import peak_signal_noise_ratio\n","import torch.nn.functional as fv\n","\n","\n","def _valid(model, args, ep):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    its = valid_dataloader(args.data_dir, batch_size=1, num_workers=0)\n","    model.eval()\n","    psnr_adder = Adder()\n","\n","    with torch.no_grad():\n","        print('Start Evaluation')\n","        factor = 32\n","        for idx, data in enumerate(its):\n","            input_img, label_img = data\n","            input_img = input_img.to(device)\n","\n","            h, w = input_img.shape[2], input_img.shape[3]\n","            H, W = ((h+factor)//factor)*factor, ((w+factor)//factor*factor)\n","            padh = H-h if h%factor!=0 else 0\n","            padw = W-w if w%factor!=0 else 0\n","            input_img = fv.pad(input_img, (0, padw, 0, padh), 'reflect')\n","\n","            if not os.path.exists(os.path.join(args.result_dir, '%d' % (ep))):\n","                os.mkdir(os.path.join(args.result_dir, '%d' % (ep)))\n","\n","            pred = model(input_img)[2]\n","            pred = pred[:,:,:h,:w]\n","\n","            pred_clip = torch.clamp(pred, 0, 1)\n","            p_numpy = pred_clip.squeeze(0).cpu().numpy()\n","            label_numpy = label_img.squeeze(0).cpu().numpy()\n","\n","            psnr = peak_signal_noise_ratio(p_numpy, label_numpy, data_range=1)\n","\n","            psnr_adder(psnr)\n","            print('\\r%03d'%idx, end=' ')\n","\n","    print('\\n')\n","    model.train()\n","    return psnr_adder.average()"]},{"cell_type":"markdown","metadata":{"id":"1n87ZbXeBapS"},"source":["TRAIN\n","=="]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":4072,"status":"ok","timestamp":1700709443132,"user":{"displayName":"General","userId":"01731632481034566404"},"user_tz":-480},"id":"jt9V7QBGBcGV"},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter\n","import torch.nn.functional as ft\n","import torch.optim as optim\n","\n","\n","from warmup_scheduler import GradualWarmupScheduler\n","\n","def _train(model, args):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","    criterion = torch.nn.L1Loss().to(device)\n","    optimizer = optim.SGD(model.parameters(), lr=args.learning_rate, momentum=args.momentum_value, nesterov=True)\n","\n","    #optimizer = SGLD(params=model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n","    dataloader = train_dataloader(args.data_dir, args.batch_size, args.num_worker)\n","    max_iter = len(dataloader)\n","    warmup_epochs=3\n","    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.num_epoch-warmup_epochs, eta_min=1e-6)\n","    scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=warmup_epochs, after_scheduler=scheduler_cosine)\n","    scheduler.step()\n","    epoch = 1\n","    if args.resume:\n","        state = torch.load(args.resume)\n","        epoch = state['epoch']\n","        optimizer.load_state_dict(state['optimizer'])\n","        model.load_state_dict(state['model'])\n","        print('Resume from %d'%epoch)\n","        epoch += 1\n","        for i in range(epoch-1):\n","          scheduler.step()\n","\n","    writer = SummaryWriter()\n","    epoch_pixel_adder = Adder()\n","    epoch_fft_adder = Adder()\n","    iter_pixel_adder = Adder()\n","    iter_fft_adder = Adder()\n","    epoch_timer = Timer('m')\n","    iter_timer = Timer('m')\n","    best_psnr=-1\n","\n","    for epoch_idx in range(epoch, args.num_epoch + 1):\n","\n","        epoch_timer.tic()\n","        iter_timer.tic()\n","        for iter_idx, batch_data in enumerate(dataloader):\n","\n","            input_img, label_img = batch_data\n","            input_img = input_img.to(device)\n","            label_img = label_img.to(device)\n","\n","            optimizer.zero_grad()\n","            pred_img = model(input_img)\n","            label_img2 = ft.interpolate(label_img, scale_factor=0.5, mode='bilinear')\n","            label_img4 = ft.interpolate(label_img, scale_factor=0.25, mode='bilinear')\n","            l1 = criterion(pred_img[0], label_img4)\n","            l2 = criterion(pred_img[1], label_img2)\n","            l3 = criterion(pred_img[2], label_img)\n","            loss_content = l1+l2+l3\n","\n","            label_fft1 = torch.fft.fft2(label_img4, dim=(-2,-1))\n","            label_fft1 = torch.stack((label_fft1.real, label_fft1.imag), -1)\n","\n","            pred_fft1 = torch.fft.fft2(pred_img[0], dim=(-2,-1))\n","            pred_fft1 = torch.stack((pred_fft1.real, pred_fft1.imag), -1)\n","\n","            label_fft2 = torch.fft.fft2(label_img2, dim=(-2,-1))\n","            label_fft2 = torch.stack((label_fft2.real, label_fft2.imag), -1)\n","\n","            pred_fft2 = torch.fft.fft2(pred_img[1], dim=(-2,-1))\n","            pred_fft2 = torch.stack((pred_fft2.real, pred_fft2.imag), -1)\n","\n","            label_fft3 = torch.fft.fft2(label_img, dim=(-2,-1))\n","            label_fft3 = torch.stack((label_fft3.real, label_fft3.imag), -1)\n","\n","            pred_fft3 = torch.fft.fft2(pred_img[2], dim=(-2,-1))\n","            pred_fft3 = torch.stack((pred_fft3.real, pred_fft3.imag), -1)\n","\n","            f1 = criterion(pred_fft1, label_fft1)\n","            f2 = criterion(pred_fft2, label_fft2)\n","            f3 = criterion(pred_fft3, label_fft3)\n","            loss_fft = f1+f2+f3\n","\n","            loss = loss_content + 0.1 * loss_fft\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.001)\n","            optimizer.step()\n","\n","            iter_pixel_adder(loss_content.item())\n","            iter_fft_adder(loss_fft.item())\n","\n","            epoch_pixel_adder(loss_content.item())\n","            epoch_fft_adder(loss_fft.item())\n","\n","            if (iter_idx + 1) % args.print_freq == 0:\n","                print(\"Time: %7.4f Epoch: %03d Iter: %4d/%4d LR: %.10f Loss content: %7.4f Loss fft: %7.4f\" % (\n","                    iter_timer.toc(), epoch_idx, iter_idx + 1, max_iter, scheduler.get_lr()[0], iter_pixel_adder.average(),\n","                    iter_fft_adder.average()))\n","                writer.add_scalar('Pixel Loss', iter_pixel_adder.average(), iter_idx + (epoch_idx-1)* max_iter)\n","                writer.add_scalar('FFT Loss', iter_fft_adder.average(), iter_idx + (epoch_idx - 1) * max_iter)\n","\n","                iter_timer.tic()\n","                iter_pixel_adder.reset()\n","                iter_fft_adder.reset()\n","        overwrite_name = os.path.join(args.model_save_dir, 'model.pkl')\n","        torch.save({'model': model.state_dict(),\n","                    'optimizer': optimizer.state_dict(),\n","                    'epoch': epoch_idx}, overwrite_name)\n","\n","        if epoch_idx % args.save_freq == 0:\n","            save_name = os.path.join(args.model_save_dir, 'model_%d.pkl' % epoch_idx)\n","            torch.save({'model': model.state_dict()}, save_name)\n","        print(\"EPOCH: %02d\\nElapsed time: %4.2f Epoch Pixel Loss: %7.4f Epoch FFT Loss: %7.4f\" % (\n","            epoch_idx, epoch_timer.toc(), epoch_pixel_adder.average(), epoch_fft_adder.average()))\n","        epoch_fft_adder.reset()\n","        epoch_pixel_adder.reset()\n","        scheduler.step()\n","        if epoch_idx % args.valid_freq == 0:\n","            val = _valid(model, args, epoch_idx)\n","            print('%03d epoch \\n Average PSNR %.2f dB' % (epoch_idx, val))\n","            writer.add_scalar('PSNR', val, epoch_idx)\n","            if val \u003e= best_psnr:\n","                torch.save({'model': model.state_dict()}, os.path.join(args.model_save_dir, 'Best.pkl'))\n","    save_name = os.path.join(args.model_save_dir, 'Final.pkl')\n","    torch.save({'model': model.state_dict()}, save_name)"]},{"cell_type":"markdown","metadata":{"id":"hoDUXGBNBWr7"},"source":["Main (Train)\n","=="]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":856,"status":"ok","timestamp":1700736774934,"user":{"displayName":"General","userId":"01731632481034566404"},"user_tz":-480},"id":"kcF3qG8RSAcd"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u003c__main__.Args object at 0x7a1a56f90a60\u003e\n","IRNeXt(\n","  (Encoder): ModuleList(\n","    (0): EBlock(\n","      (layers): Sequential(\n","        (0): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (1): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (2): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (3): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): DeepPoolLayer(\n","              (pools): ModuleList(\n","                (0): AvgPool2d(kernel_size=8, stride=8, padding=0)\n","                (1): AvgPool2d(kernel_size=4, stride=4, padding=0)\n","                (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","              )\n","              (convs): ModuleList(\n","                (0-2): 3 x Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              )\n","              (dynas): ModuleList(\n","                (0-2): 3 x dynamic_filter(\n","                  (conv): Conv2d(32, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (act): Tanh()\n","                  (pad): ReflectionPad2d((1, 1, 1, 1))\n","                  (ap): AdaptiveAvgPool2d(output_size=(1, 1))\n","                  (gap): AdaptiveAvgPool2d(output_size=1)\n","                )\n","              )\n","              (relu): GELU(approximate='none')\n","              (conv_sum): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            )\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (1): EBlock(\n","      (layers): Sequential(\n","        (0): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (1): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (2): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (3): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): DeepPoolLayer(\n","              (pools): ModuleList(\n","                (0): AvgPool2d(kernel_size=8, stride=8, padding=0)\n","                (1): AvgPool2d(kernel_size=4, stride=4, padding=0)\n","                (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","              )\n","              (convs): ModuleList(\n","                (0-2): 3 x Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              )\n","              (dynas): ModuleList(\n","                (0-2): 3 x dynamic_filter(\n","                  (conv): Conv2d(64, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (act): Tanh()\n","                  (pad): ReflectionPad2d((1, 1, 1, 1))\n","                  (ap): AdaptiveAvgPool2d(output_size=(1, 1))\n","                  (gap): AdaptiveAvgPool2d(output_size=1)\n","                )\n","              )\n","              (relu): GELU(approximate='none')\n","              (conv_sum): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            )\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (2): EBlock(\n","      (layers): Sequential(\n","        (0): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (1): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (2): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (3): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): DeepPoolLayer(\n","              (pools): ModuleList(\n","                (0): AvgPool2d(kernel_size=8, stride=8, padding=0)\n","                (1): AvgPool2d(kernel_size=4, stride=4, padding=0)\n","                (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","              )\n","              (convs): ModuleList(\n","                (0-2): 3 x Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              )\n","              (dynas): ModuleList(\n","                (0-2): 3 x dynamic_filter(\n","                  (conv): Conv2d(128, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (act): Tanh()\n","                  (pad): ReflectionPad2d((1, 1, 1, 1))\n","                  (ap): AdaptiveAvgPool2d(output_size=(1, 1))\n","                  (gap): AdaptiveAvgPool2d(output_size=1)\n","                )\n","              )\n","              (relu): GELU(approximate='none')\n","              (conv_sum): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            )\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (feat_extract): ModuleList(\n","    (0): BasicConv(\n","      (main): Sequential(\n","        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GELU(approximate='none')\n","      )\n","    )\n","    (1): BasicConv(\n","      (main): Sequential(\n","        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (1): GELU(approximate='none')\n","      )\n","    )\n","    (2): BasicConv(\n","      (main): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (1): GELU(approximate='none')\n","      )\n","    )\n","    (3): BasicConv(\n","      (main): Sequential(\n","        (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","        (1): GELU(approximate='none')\n","      )\n","    )\n","    (4): BasicConv(\n","      (main): Sequential(\n","        (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","        (1): GELU(approximate='none')\n","      )\n","    )\n","    (5): BasicConv(\n","      (main): Sequential(\n","        (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","  )\n","  (Decoder): ModuleList(\n","    (0): DBlock(\n","      (layers): Sequential(\n","        (0): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (1): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (2): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (3): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): DeepPoolLayer(\n","              (pools): ModuleList(\n","                (0): AvgPool2d(kernel_size=8, stride=8, padding=0)\n","                (1): AvgPool2d(kernel_size=4, stride=4, padding=0)\n","                (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","              )\n","              (convs): ModuleList(\n","                (0-2): 3 x Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              )\n","              (dynas): ModuleList(\n","                (0-2): 3 x dynamic_filter(\n","                  (conv): Conv2d(128, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (act): Tanh()\n","                  (pad): ReflectionPad2d((1, 1, 1, 1))\n","                  (ap): AdaptiveAvgPool2d(output_size=(1, 1))\n","                  (gap): AdaptiveAvgPool2d(output_size=1)\n","                )\n","              )\n","              (relu): GELU(approximate='none')\n","              (conv_sum): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            )\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (1): DBlock(\n","      (layers): Sequential(\n","        (0): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (1): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (2): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (3): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): DeepPoolLayer(\n","              (pools): ModuleList(\n","                (0): AvgPool2d(kernel_size=8, stride=8, padding=0)\n","                (1): AvgPool2d(kernel_size=4, stride=4, padding=0)\n","                (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","              )\n","              (convs): ModuleList(\n","                (0-2): 3 x Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              )\n","              (dynas): ModuleList(\n","                (0-2): 3 x dynamic_filter(\n","                  (conv): Conv2d(64, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (act): Tanh()\n","                  (pad): ReflectionPad2d((1, 1, 1, 1))\n","                  (ap): AdaptiveAvgPool2d(output_size=(1, 1))\n","                  (gap): AdaptiveAvgPool2d(output_size=1)\n","                )\n","              )\n","              (relu): GELU(approximate='none')\n","              (conv_sum): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            )\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (2): DBlock(\n","      (layers): Sequential(\n","        (0): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (1): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (2): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (3): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): DeepPoolLayer(\n","              (pools): ModuleList(\n","                (0): AvgPool2d(kernel_size=8, stride=8, padding=0)\n","                (1): AvgPool2d(kernel_size=4, stride=4, padding=0)\n","                (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","              )\n","              (convs): ModuleList(\n","                (0-2): 3 x Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              )\n","              (dynas): ModuleList(\n","                (0-2): 3 x dynamic_filter(\n","                  (conv): Conv2d(32, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (act): Tanh()\n","                  (pad): ReflectionPad2d((1, 1, 1, 1))\n","                  (ap): AdaptiveAvgPool2d(output_size=(1, 1))\n","                  (gap): AdaptiveAvgPool2d(output_size=1)\n","                )\n","              )\n","              (relu): GELU(approximate='none')\n","              (conv_sum): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            )\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (Convs): ModuleList(\n","    (0): BasicConv(\n","      (main): Sequential(\n","        (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n","        (1): GELU(approximate='none')\n","      )\n","    )\n","    (1): BasicConv(\n","      (main): Sequential(\n","        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n","        (1): GELU(approximate='none')\n","      )\n","    )\n","  )\n","  (ConvsOut): ModuleList(\n","    (0): BasicConv(\n","      (main): Sequential(\n","        (0): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (1): BasicConv(\n","      (main): Sequential(\n","        (0): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","  )\n","  (FAM1): FAM(\n","    (merge): BasicConv(\n","      (main): Sequential(\n","        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","  )\n","  (SCM1): SCM(\n","    (main): Sequential(\n","      (0): BasicConv(\n","        (main): Sequential(\n","          (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): GELU(approximate='none')\n","        )\n","      )\n","      (1): BasicConv(\n","        (main): Sequential(\n","          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n","          (1): GELU(approximate='none')\n","        )\n","      )\n","      (2): BasicConv(\n","        (main): Sequential(\n","          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): GELU(approximate='none')\n","        )\n","      )\n","      (3): BasicConv(\n","        (main): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n","    )\n","  )\n","  (FAM2): FAM(\n","    (merge): BasicConv(\n","      (main): Sequential(\n","        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","  )\n","  (SCM2): SCM(\n","    (main): Sequential(\n","      (0): BasicConv(\n","        (main): Sequential(\n","          (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): GELU(approximate='none')\n","        )\n","      )\n","      (1): BasicConv(\n","        (main): Sequential(\n","          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n","          (1): GELU(approximate='none')\n","        )\n","      )\n","      (2): BasicConv(\n","        (main): Sequential(\n","          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): GELU(approximate='none')\n","        )\n","      )\n","      (3): BasicConv(\n","        (main): Sequential(\n","          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n","    )\n","  )\n",")\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Time:  0.5195 Epoch: 001 Iter:  100/39218 LR: 0.0000333333 Loss content:  0.4128 Loss fft: 10.4165\n","Time:  0.3504 Epoch: 001 Iter:  200/39218 LR: 0.0000333333 Loss content:  0.4139 Loss fft: 10.3315\n","Time:  0.3506 Epoch: 001 Iter:  300/39218 LR: 0.0000333333 Loss content:  0.4106 Loss fft: 10.4142\n","Time:  0.3509 Epoch: 001 Iter:  400/39218 LR: 0.0000333333 Loss content:  0.4085 Loss fft: 10.4292\n","Time:  0.3510 Epoch: 001 Iter:  500/39218 LR: 0.0000333333 Loss content:  0.4141 Loss fft: 10.3128\n","Time:  0.3510 Epoch: 001 Iter:  600/39218 LR: 0.0000333333 Loss content:  0.4080 Loss fft: 10.3621\n","Time:  0.3512 Epoch: 001 Iter:  700/39218 LR: 0.0000333333 Loss content:  0.4082 Loss fft: 10.2416\n","Time:  0.3510 Epoch: 001 Iter:  800/39218 LR: 0.0000333333 Loss content:  0.4130 Loss fft: 10.4350\n","Time:  0.3511 Epoch: 001 Iter:  900/39218 LR: 0.0000333333 Loss content:  0.4105 Loss fft: 10.1429\n","Time:  0.3509 Epoch: 001 Iter: 1000/39218 LR: 0.0000333333 Loss content:  0.4149 Loss fft: 10.4271\n","Time:  0.3567 Epoch: 001 Iter: 1100/39218 LR: 0.0000333333 Loss content:  0.4049 Loss fft: 10.3052\n","Time:  0.3512 Epoch: 001 Iter: 1200/39218 LR: 0.0000333333 Loss content:  0.3990 Loss fft: 10.1508\n","Time:  0.3513 Epoch: 001 Iter: 1300/39218 LR: 0.0000333333 Loss content:  0.4096 Loss fft: 10.2480\n","Time:  0.3513 Epoch: 001 Iter: 1400/39218 LR: 0.0000333333 Loss content:  0.4122 Loss fft: 10.3824\n","Time:  0.3512 Epoch: 001 Iter: 1500/39218 LR: 0.0000333333 Loss content:  0.4113 Loss fft: 10.3320\n","Time:  0.3513 Epoch: 001 Iter: 1600/39218 LR: 0.0000333333 Loss content:  0.4052 Loss fft: 10.2569\n","Time:  0.3513 Epoch: 001 Iter: 1700/39218 LR: 0.0000333333 Loss content:  0.4016 Loss fft: 10.1277\n","Time:  0.3511 Epoch: 001 Iter: 1800/39218 LR: 0.0000333333 Loss content:  0.4058 Loss fft: 10.3159\n","Time:  0.3512 Epoch: 001 Iter: 1900/39218 LR: 0.0000333333 Loss content:  0.4159 Loss fft: 10.4663\n","Time:  0.3513 Epoch: 001 Iter: 2000/39218 LR: 0.0000333333 Loss content:  0.4072 Loss fft: 10.2974\n","Time:  0.3512 Epoch: 001 Iter: 2100/39218 LR: 0.0000333333 Loss content:  0.4095 Loss fft: 10.3181\n","Time:  0.3512 Epoch: 001 Iter: 2200/39218 LR: 0.0000333333 Loss content:  0.4083 Loss fft: 10.5245\n","Time:  0.3510 Epoch: 001 Iter: 2300/39218 LR: 0.0000333333 Loss content:  0.4064 Loss fft: 10.3515\n","Time:  0.3511 Epoch: 001 Iter: 2400/39218 LR: 0.0000333333 Loss content:  0.4091 Loss fft: 10.2997\n","Time:  0.3515 Epoch: 001 Iter: 2500/39218 LR: 0.0000333333 Loss content:  0.4057 Loss fft: 10.1009\n","Time:  0.3512 Epoch: 001 Iter: 2600/39218 LR: 0.0000333333 Loss content:  0.4071 Loss fft: 10.3241\n","Time:  0.3511 Epoch: 001 Iter: 2700/39218 LR: 0.0000333333 Loss content:  0.4047 Loss fft: 10.3249\n","Time:  0.3516 Epoch: 001 Iter: 2800/39218 LR: 0.0000333333 Loss content:  0.4101 Loss fft: 10.3494\n","Time:  0.3514 Epoch: 001 Iter: 2900/39218 LR: 0.0000333333 Loss content:  0.4071 Loss fft: 10.3728\n","Time:  0.3513 Epoch: 001 Iter: 3000/39218 LR: 0.0000333333 Loss content:  0.4143 Loss fft: 10.3694\n","Time:  0.3514 Epoch: 001 Iter: 3100/39218 LR: 0.0000333333 Loss content:  0.4072 Loss fft: 10.3753\n","Time:  0.3514 Epoch: 001 Iter: 3200/39218 LR: 0.0000333333 Loss content:  0.4210 Loss fft: 10.3565\n","Time:  0.3547 Epoch: 001 Iter: 3300/39218 LR: 0.0000333333 Loss content:  0.4132 Loss fft: 10.4642\n","Time:  0.3515 Epoch: 001 Iter: 3400/39218 LR: 0.0000333333 Loss content:  0.4196 Loss fft: 10.4560\n","Time:  0.3515 Epoch: 001 Iter: 3500/39218 LR: 0.0000333333 Loss content:  0.4124 Loss fft: 10.3937\n","Time:  0.3526 Epoch: 001 Iter: 3600/39218 LR: 0.0000333333 Loss content:  0.4038 Loss fft: 10.0953\n","Time:  0.3515 Epoch: 001 Iter: 3700/39218 LR: 0.0000333333 Loss content:  0.4143 Loss fft: 10.3868\n","Time:  0.3515 Epoch: 001 Iter: 3800/39218 LR: 0.0000333333 Loss content:  0.4149 Loss fft: 10.2641\n","Time:  0.3514 Epoch: 001 Iter: 3900/39218 LR: 0.0000333333 Loss content:  0.4139 Loss fft: 10.4484\n","Time:  0.3513 Epoch: 001 Iter: 4000/39218 LR: 0.0000333333 Loss content:  0.4151 Loss fft: 10.2689\n","Time:  0.3515 Epoch: 001 Iter: 4100/39218 LR: 0.0000333333 Loss content:  0.4097 Loss fft: 10.4315\n","Time:  0.3514 Epoch: 001 Iter: 4200/39218 LR: 0.0000333333 Loss content:  0.4089 Loss fft: 10.2183\n","Time:  0.3514 Epoch: 001 Iter: 4300/39218 LR: 0.0000333333 Loss content:  0.4120 Loss fft: 10.2924\n","Time:  0.3517 Epoch: 001 Iter: 4400/39218 LR: 0.0000333333 Loss content:  0.3984 Loss fft: 10.1233\n","Time:  0.3516 Epoch: 001 Iter: 4500/39218 LR: 0.0000333333 Loss content:  0.4095 Loss fft: 10.2970\n","Time:  0.3518 Epoch: 001 Iter: 4600/39218 LR: 0.0000333333 Loss content:  0.4106 Loss fft: 10.3302\n","Time:  0.3516 Epoch: 001 Iter: 4700/39218 LR: 0.0000333333 Loss content:  0.4179 Loss fft: 10.1988\n","Time:  0.3518 Epoch: 001 Iter: 4800/39218 LR: 0.0000333333 Loss content:  0.4172 Loss fft: 10.4042\n","Time:  0.3518 Epoch: 001 Iter: 4900/39218 LR: 0.0000333333 Loss content:  0.4098 Loss fft: 10.4614\n","Time:  0.3516 Epoch: 001 Iter: 5000/39218 LR: 0.0000333333 Loss content:  0.4053 Loss fft: 10.2446\n","Time:  0.3516 Epoch: 001 Iter: 5100/39218 LR: 0.0000333333 Loss content:  0.4063 Loss fft: 10.2484\n","Time:  0.3518 Epoch: 001 Iter: 5200/39218 LR: 0.0000333333 Loss content:  0.4087 Loss fft: 10.3024\n","Time:  0.3518 Epoch: 001 Iter: 5300/39218 LR: 0.0000333333 Loss content:  0.4080 Loss fft: 10.3152\n","Time:  0.3517 Epoch: 001 Iter: 5400/39218 LR: 0.0000333333 Loss content:  0.4087 Loss fft: 10.4316\n","Time:  0.3551 Epoch: 001 Iter: 5500/39218 LR: 0.0000333333 Loss content:  0.4104 Loss fft: 10.3243\n","Time:  0.3518 Epoch: 001 Iter: 5600/39218 LR: 0.0000333333 Loss content:  0.4129 Loss fft: 10.4230\n","Time:  0.3516 Epoch: 001 Iter: 5700/39218 LR: 0.0000333333 Loss content:  0.4053 Loss fft: 10.1655\n","Time:  0.3518 Epoch: 001 Iter: 5800/39218 LR: 0.0000333333 Loss content:  0.4206 Loss fft: 10.5388\n","Time:  0.3516 Epoch: 001 Iter: 5900/39218 LR: 0.0000333333 Loss content:  0.4053 Loss fft: 10.2558\n","Time:  0.3515 Epoch: 001 Iter: 6000/39218 LR: 0.0000333333 Loss content:  0.4129 Loss fft: 10.2966\n","Time:  0.3516 Epoch: 001 Iter: 6100/39218 LR: 0.0000333333 Loss content:  0.4151 Loss fft: 10.3771\n","Time:  0.3515 Epoch: 001 Iter: 6200/39218 LR: 0.0000333333 Loss content:  0.4067 Loss fft: 10.3458\n","Time:  0.3517 Epoch: 001 Iter: 6300/39218 LR: 0.0000333333 Loss content:  0.4148 Loss fft: 10.5056\n","Time:  0.3516 Epoch: 001 Iter: 6400/39218 LR: 0.0000333333 Loss content:  0.4144 Loss fft: 10.2912\n","Time:  0.3515 Epoch: 001 Iter: 6500/39218 LR: 0.0000333333 Loss content:  0.4058 Loss fft: 10.3589\n","Time:  0.3518 Epoch: 001 Iter: 6600/39218 LR: 0.0000333333 Loss content:  0.4109 Loss fft: 10.3722\n","Time:  0.3515 Epoch: 001 Iter: 6700/39218 LR: 0.0000333333 Loss content:  0.4132 Loss fft: 10.4174\n","Time:  0.3519 Epoch: 001 Iter: 6800/39218 LR: 0.0000333333 Loss content:  0.4084 Loss fft: 10.2033\n","Time:  0.3518 Epoch: 001 Iter: 6900/39218 LR: 0.0000333333 Loss content:  0.4089 Loss fft: 10.1648\n","Time:  0.3517 Epoch: 001 Iter: 7000/39218 LR: 0.0000333333 Loss content:  0.4140 Loss fft: 10.3502\n","Time:  0.3517 Epoch: 001 Iter: 7100/39218 LR: 0.0000333333 Loss content:  0.4151 Loss fft: 10.3215\n","Time:  0.3516 Epoch: 001 Iter: 7200/39218 LR: 0.0000333333 Loss content:  0.4084 Loss fft: 10.4260\n","Time:  0.3516 Epoch: 001 Iter: 7300/39218 LR: 0.0000333333 Loss content:  0.4080 Loss fft: 10.2690\n","Time:  0.3516 Epoch: 001 Iter: 7400/39218 LR: 0.0000333333 Loss content:  0.4142 Loss fft: 10.5964\n","Time:  0.3516 Epoch: 001 Iter: 7500/39218 LR: 0.0000333333 Loss content:  0.4071 Loss fft: 10.4463\n","Time:  0.3518 Epoch: 001 Iter: 7600/39218 LR: 0.0000333333 Loss content:  0.4163 Loss fft: 10.4585\n","Time:  0.3516 Epoch: 001 Iter: 7700/39218 LR: 0.0000333333 Loss content:  0.4066 Loss fft: 10.2196\n","Time:  0.3550 Epoch: 001 Iter: 7800/39218 LR: 0.0000333333 Loss content:  0.4114 Loss fft: 10.2296\n","Time:  0.3517 Epoch: 001 Iter: 7900/39218 LR: 0.0000333333 Loss content:  0.4089 Loss fft: 10.2941\n","Time:  0.3515 Epoch: 001 Iter: 8000/39218 LR: 0.0000333333 Loss content:  0.4159 Loss fft: 10.2139\n","Time:  0.3515 Epoch: 001 Iter: 8100/39218 LR: 0.0000333333 Loss content:  0.4117 Loss fft: 10.4272\n","Time:  0.3517 Epoch: 001 Iter: 8200/39218 LR: 0.0000333333 Loss content:  0.4107 Loss fft: 10.1574\n","Time:  0.3516 Epoch: 001 Iter: 8300/39218 LR: 0.0000333333 Loss content:  0.4097 Loss fft: 10.3854\n","Time:  0.3518 Epoch: 001 Iter: 8400/39218 LR: 0.0000333333 Loss content:  0.4065 Loss fft: 10.2455\n","Time:  0.3518 Epoch: 001 Iter: 8500/39218 LR: 0.0000333333 Loss content:  0.4041 Loss fft: 10.2063\n","Time:  0.3519 Epoch: 001 Iter: 8600/39218 LR: 0.0000333333 Loss content:  0.4161 Loss fft: 10.3839\n","Time:  0.3519 Epoch: 001 Iter: 8700/39218 LR: 0.0000333333 Loss content:  0.4136 Loss fft: 10.4683\n","Time:  0.3521 Epoch: 001 Iter: 8800/39218 LR: 0.0000333333 Loss content:  0.4137 Loss fft: 10.4555\n","Time:  0.3518 Epoch: 001 Iter: 8900/39218 LR: 0.0000333333 Loss content:  0.4145 Loss fft: 10.4306\n","Time:  0.3520 Epoch: 001 Iter: 9000/39218 LR: 0.0000333333 Loss content:  0.4035 Loss fft: 10.2820\n","Time:  0.3519 Epoch: 001 Iter: 9100/39218 LR: 0.0000333333 Loss content:  0.4137 Loss fft: 10.3445\n","Time:  0.3517 Epoch: 001 Iter: 9200/39218 LR: 0.0000333333 Loss content:  0.4111 Loss fft: 10.2858\n","Time:  0.3518 Epoch: 001 Iter: 9300/39218 LR: 0.0000333333 Loss content:  0.4103 Loss fft: 10.2365\n","Time:  0.3521 Epoch: 001 Iter: 9400/39218 LR: 0.0000333333 Loss content:  0.4114 Loss fft: 10.3715\n","Time:  0.3519 Epoch: 001 Iter: 9500/39218 LR: 0.0000333333 Loss content:  0.4169 Loss fft: 10.4716\n","Time:  0.3518 Epoch: 001 Iter: 9600/39218 LR: 0.0000333333 Loss content:  0.4101 Loss fft: 10.4620\n","Time:  0.3518 Epoch: 001 Iter: 9700/39218 LR: 0.0000333333 Loss content:  0.4137 Loss fft: 10.4325\n","Time:  0.3519 Epoch: 001 Iter: 9800/39218 LR: 0.0000333333 Loss content:  0.4113 Loss fft: 10.3076\n","Time:  0.3521 Epoch: 001 Iter: 9900/39218 LR: 0.0000333333 Loss content:  0.4125 Loss fft: 10.5240\n","Time:  0.3554 Epoch: 001 Iter: 10000/39218 LR: 0.0000333333 Loss content:  0.4074 Loss fft: 10.4145\n","Time:  0.3519 Epoch: 001 Iter: 10100/39218 LR: 0.0000333333 Loss content:  0.4156 Loss fft: 10.1906\n","Time:  0.3520 Epoch: 001 Iter: 10200/39218 LR: 0.0000333333 Loss content:  0.4096 Loss fft: 10.5741\n","Time:  0.3521 Epoch: 001 Iter: 10300/39218 LR: 0.0000333333 Loss content:  0.4076 Loss fft: 10.4423\n","Time:  0.3519 Epoch: 001 Iter: 10400/39218 LR: 0.0000333333 Loss content:  0.4089 Loss fft: 10.3791\n","Time:  0.3520 Epoch: 001 Iter: 10500/39218 LR: 0.0000333333 Loss content:  0.4093 Loss fft: 10.2476\n","Time:  0.3519 Epoch: 001 Iter: 10600/39218 LR: 0.0000333333 Loss content:  0.4054 Loss fft: 10.2739\n","Time:  0.3522 Epoch: 001 Iter: 10700/39218 LR: 0.0000333333 Loss content:  0.4187 Loss fft: 10.4334\n","Time:  0.3520 Epoch: 001 Iter: 10800/39218 LR: 0.0000333333 Loss content:  0.4106 Loss fft: 10.3045\n","Time:  0.3519 Epoch: 001 Iter: 10900/39218 LR: 0.0000333333 Loss content:  0.3997 Loss fft: 10.1606\n","Time:  0.3518 Epoch: 001 Iter: 11000/39218 LR: 0.0000333333 Loss content:  0.4057 Loss fft: 10.3104\n","Time:  0.3520 Epoch: 001 Iter: 11100/39218 LR: 0.0000333333 Loss content:  0.4057 Loss fft: 10.4499\n","Time:  0.3519 Epoch: 001 Iter: 11200/39218 LR: 0.0000333333 Loss content:  0.4155 Loss fft: 10.3685\n","Time:  0.3519 Epoch: 001 Iter: 11300/39218 LR: 0.0000333333 Loss content:  0.4102 Loss fft: 10.3035\n","Time:  0.3520 Epoch: 001 Iter: 11400/39218 LR: 0.0000333333 Loss content:  0.4044 Loss fft: 10.2643\n","Time:  0.3518 Epoch: 001 Iter: 11500/39218 LR: 0.0000333333 Loss content:  0.4051 Loss fft: 10.2800\n","Time:  0.3519 Epoch: 001 Iter: 11600/39218 LR: 0.0000333333 Loss content:  0.4065 Loss fft: 10.3785\n","Time:  0.3519 Epoch: 001 Iter: 11700/39218 LR: 0.0000333333 Loss content:  0.4040 Loss fft: 10.2626\n","Time:  0.3517 Epoch: 001 Iter: 11800/39218 LR: 0.0000333333 Loss content:  0.4005 Loss fft: 10.3157\n","Time:  0.3519 Epoch: 001 Iter: 11900/39218 LR: 0.0000333333 Loss content:  0.4029 Loss fft: 10.3234\n","Time:  0.3519 Epoch: 001 Iter: 12000/39218 LR: 0.0000333333 Loss content:  0.4132 Loss fft: 10.4001\n","Time:  0.3518 Epoch: 001 Iter: 12100/39218 LR: 0.0000333333 Loss content:  0.4177 Loss fft: 10.4938\n","Time:  0.3553 Epoch: 001 Iter: 12200/39218 LR: 0.0000333333 Loss content:  0.4015 Loss fft: 10.1678\n","Time:  0.3520 Epoch: 001 Iter: 12300/39218 LR: 0.0000333333 Loss content:  0.4158 Loss fft: 10.4910\n","Time:  0.3520 Epoch: 001 Iter: 12400/39218 LR: 0.0000333333 Loss content:  0.4097 Loss fft: 10.2345\n","Time:  0.3520 Epoch: 001 Iter: 12500/39218 LR: 0.0000333333 Loss content:  0.4116 Loss fft: 10.4221\n","Time:  0.3520 Epoch: 001 Iter: 12600/39218 LR: 0.0000333333 Loss content:  0.4009 Loss fft: 10.1353\n","Time:  0.3522 Epoch: 001 Iter: 12700/39218 LR: 0.0000333333 Loss content:  0.4106 Loss fft: 10.3894\n","Time:  0.3519 Epoch: 001 Iter: 12800/39218 LR: 0.0000333333 Loss content:  0.4029 Loss fft: 10.1230\n","Time:  0.3518 Epoch: 001 Iter: 12900/39218 LR: 0.0000333333 Loss content:  0.4070 Loss fft: 10.3544\n","Time:  0.3519 Epoch: 001 Iter: 13000/39218 LR: 0.0000333333 Loss content:  0.4085 Loss fft: 10.4206\n","Time:  0.3519 Epoch: 001 Iter: 13100/39218 LR: 0.0000333333 Loss content:  0.4120 Loss fft: 10.3719\n","Time:  0.3521 Epoch: 001 Iter: 13200/39218 LR: 0.0000333333 Loss content:  0.4112 Loss fft: 10.3649\n","Time:  0.3520 Epoch: 001 Iter: 13300/39218 LR: 0.0000333333 Loss content:  0.4060 Loss fft: 10.3171\n","Time:  0.3521 Epoch: 001 Iter: 13400/39218 LR: 0.0000333333 Loss content:  0.4117 Loss fft: 10.4384\n","Time:  0.3518 Epoch: 001 Iter: 13500/39218 LR: 0.0000333333 Loss content:  0.4118 Loss fft: 10.2111\n","Time:  0.3517 Epoch: 001 Iter: 13600/39218 LR: 0.0000333333 Loss content:  0.4068 Loss fft: 10.3291\n","Time:  0.3519 Epoch: 001 Iter: 13700/39218 LR: 0.0000333333 Loss content:  0.4104 Loss fft: 10.2106\n","Time:  0.3518 Epoch: 001 Iter: 13800/39218 LR: 0.0000333333 Loss content:  0.4091 Loss fft: 10.4545\n","Time:  0.3521 Epoch: 001 Iter: 13900/39218 LR: 0.0000333333 Loss content:  0.4068 Loss fft: 10.3300\n","Time:  0.3522 Epoch: 001 Iter: 14000/39218 LR: 0.0000333333 Loss content:  0.4084 Loss fft: 10.2598\n","Time:  0.3521 Epoch: 001 Iter: 14100/39218 LR: 0.0000333333 Loss content:  0.4036 Loss fft: 10.4340\n","Time:  0.3524 Epoch: 001 Iter: 14200/39218 LR: 0.0000333333 Loss content:  0.4163 Loss fft: 10.3902\n","Time:  0.3520 Epoch: 001 Iter: 14300/39218 LR: 0.0000333333 Loss content:  0.4068 Loss fft: 10.2283\n","Time:  0.3520 Epoch: 001 Iter: 14400/39218 LR: 0.0000333333 Loss content:  0.4138 Loss fft: 10.3965\n","Time:  0.3556 Epoch: 001 Iter: 14500/39218 LR: 0.0000333333 Loss content:  0.4140 Loss fft: 10.1090\n","Time:  0.3520 Epoch: 001 Iter: 14600/39218 LR: 0.0000333333 Loss content:  0.4202 Loss fft: 10.4669\n","Time:  0.3517 Epoch: 001 Iter: 14700/39218 LR: 0.0000333333 Loss content:  0.4025 Loss fft: 10.2168\n","Time:  0.3521 Epoch: 001 Iter: 14800/39218 LR: 0.0000333333 Loss content:  0.4033 Loss fft: 10.2236\n","Time:  0.3519 Epoch: 001 Iter: 14900/39218 LR: 0.0000333333 Loss content:  0.4137 Loss fft: 10.4620\n","Time:  0.3521 Epoch: 001 Iter: 15000/39218 LR: 0.0000333333 Loss content:  0.4044 Loss fft: 10.2262\n","Time:  0.3519 Epoch: 001 Iter: 15100/39218 LR: 0.0000333333 Loss content:  0.4079 Loss fft: 10.4117\n","Time:  0.3519 Epoch: 001 Iter: 15200/39218 LR: 0.0000333333 Loss content:  0.4154 Loss fft: 10.2923\n","Time:  0.3520 Epoch: 001 Iter: 15300/39218 LR: 0.0000333333 Loss content:  0.4080 Loss fft: 10.2466\n","Time:  0.3519 Epoch: 001 Iter: 15400/39218 LR: 0.0000333333 Loss content:  0.4112 Loss fft: 10.5033\n","Time:  0.3519 Epoch: 001 Iter: 15500/39218 LR: 0.0000333333 Loss content:  0.4052 Loss fft: 10.2764\n","Time:  0.3517 Epoch: 001 Iter: 15600/39218 LR: 0.0000333333 Loss content:  0.4080 Loss fft: 10.2978\n","Time:  0.3518 Epoch: 001 Iter: 15700/39218 LR: 0.0000333333 Loss content:  0.4012 Loss fft: 10.1743\n","Time:  0.3517 Epoch: 001 Iter: 15800/39218 LR: 0.0000333333 Loss content:  0.4113 Loss fft: 10.2893\n","Time:  0.3518 Epoch: 001 Iter: 15900/39218 LR: 0.0000333333 Loss content:  0.4066 Loss fft: 10.2766\n","Time:  0.3519 Epoch: 001 Iter: 16000/39218 LR: 0.0000333333 Loss content:  0.4129 Loss fft: 10.3282\n","Time:  0.3518 Epoch: 001 Iter: 16100/39218 LR: 0.0000333333 Loss content:  0.4065 Loss fft: 10.3627\n","Time:  0.3518 Epoch: 001 Iter: 16200/39218 LR: 0.0000333333 Loss content:  0.4138 Loss fft: 10.5079\n","Time:  0.3518 Epoch: 001 Iter: 16300/39218 LR: 0.0000333333 Loss content:  0.4093 Loss fft: 10.3461\n","Time:  0.3518 Epoch: 001 Iter: 16400/39218 LR: 0.0000333333 Loss content:  0.4177 Loss fft: 10.3675\n","Time:  0.3517 Epoch: 001 Iter: 16500/39218 LR: 0.0000333333 Loss content:  0.4038 Loss fft: 10.3252\n","Time:  0.3514 Epoch: 001 Iter: 16600/39218 LR: 0.0000333333 Loss content:  0.4071 Loss fft: 10.3377\n","Time:  0.3551 Epoch: 001 Iter: 16700/39218 LR: 0.0000333333 Loss content:  0.4085 Loss fft: 10.2917\n","Time:  0.3518 Epoch: 001 Iter: 16800/39218 LR: 0.0000333333 Loss content:  0.4194 Loss fft: 10.4687\n","Time:  0.3520 Epoch: 001 Iter: 16900/39218 LR: 0.0000333333 Loss content:  0.4177 Loss fft: 10.1556\n","Time:  0.3516 Epoch: 001 Iter: 17000/39218 LR: 0.0000333333 Loss content:  0.4135 Loss fft: 10.5165\n","Time:  0.3518 Epoch: 001 Iter: 17100/39218 LR: 0.0000333333 Loss content:  0.4093 Loss fft: 10.3134\n","Time:  0.3518 Epoch: 001 Iter: 17200/39218 LR: 0.0000333333 Loss content:  0.4138 Loss fft: 10.3607\n","Time:  0.3518 Epoch: 001 Iter: 17300/39218 LR: 0.0000333333 Loss content:  0.4122 Loss fft: 10.3966\n","Time:  0.3517 Epoch: 001 Iter: 17400/39218 LR: 0.0000333333 Loss content:  0.4114 Loss fft: 10.3474\n","Time:  0.3517 Epoch: 001 Iter: 17500/39218 LR: 0.0000333333 Loss content:  0.4084 Loss fft: 10.4340\n","Time:  0.3516 Epoch: 001 Iter: 17600/39218 LR: 0.0000333333 Loss content:  0.4086 Loss fft: 10.4922\n","Time:  0.3517 Epoch: 001 Iter: 17700/39218 LR: 0.0000333333 Loss content:  0.4066 Loss fft: 10.2904\n","Time:  0.3519 Epoch: 001 Iter: 17800/39218 LR: 0.0000333333 Loss content:  0.4036 Loss fft: 10.2040\n","Time:  0.3518 Epoch: 001 Iter: 17900/39218 LR: 0.0000333333 Loss content:  0.4049 Loss fft: 10.2706\n","Time:  0.3517 Epoch: 001 Iter: 18000/39218 LR: 0.0000333333 Loss content:  0.4064 Loss fft: 10.3612\n","Time:  0.3516 Epoch: 001 Iter: 18100/39218 LR: 0.0000333333 Loss content:  0.4138 Loss fft: 10.4351\n","Time:  0.3516 Epoch: 001 Iter: 18200/39218 LR: 0.0000333333 Loss content:  0.4082 Loss fft: 10.2967\n","Time:  0.3518 Epoch: 001 Iter: 18300/39218 LR: 0.0000333333 Loss content:  0.4092 Loss fft: 10.3662\n","Time:  0.3518 Epoch: 001 Iter: 18400/39218 LR: 0.0000333333 Loss content:  0.3925 Loss fft: 10.3027\n","Time:  0.3519 Epoch: 001 Iter: 18500/39218 LR: 0.0000333333 Loss content:  0.4150 Loss fft: 10.4496\n","Time:  0.3518 Epoch: 001 Iter: 18600/39218 LR: 0.0000333333 Loss content:  0.4027 Loss fft: 10.3055\n","Time:  0.3517 Epoch: 001 Iter: 18700/39218 LR: 0.0000333333 Loss content:  0.4116 Loss fft: 10.4372\n","Time:  0.3518 Epoch: 001 Iter: 18800/39218 LR: 0.0000333333 Loss content:  0.4068 Loss fft: 10.1599\n","Time:  0.3518 Epoch: 001 Iter: 18900/39218 LR: 0.0000333333 Loss content:  0.4178 Loss fft: 10.2166\n","Time:  0.3551 Epoch: 001 Iter: 19000/39218 LR: 0.0000333333 Loss content:  0.4107 Loss fft: 10.5022\n","Time:  0.3518 Epoch: 001 Iter: 19100/39218 LR: 0.0000333333 Loss content:  0.4115 Loss fft: 10.3391\n","Time:  0.3516 Epoch: 001 Iter: 19200/39218 LR: 0.0000333333 Loss content:  0.3985 Loss fft: 10.1122\n","Time:  0.3517 Epoch: 001 Iter: 19300/39218 LR: 0.0000333333 Loss content:  0.4098 Loss fft: 10.2601\n","Time:  0.3518 Epoch: 001 Iter: 19400/39218 LR: 0.0000333333 Loss content:  0.4085 Loss fft: 10.4430\n","Time:  0.3520 Epoch: 001 Iter: 19500/39218 LR: 0.0000333333 Loss content:  0.4028 Loss fft: 10.0945\n","Time:  0.3517 Epoch: 001 Iter: 19600/39218 LR: 0.0000333333 Loss content:  0.4110 Loss fft: 10.5492\n","Time:  0.3517 Epoch: 001 Iter: 19700/39218 LR: 0.0000333333 Loss content:  0.4023 Loss fft: 10.3737\n","Time:  0.3518 Epoch: 001 Iter: 19800/39218 LR: 0.0000333333 Loss content:  0.4044 Loss fft: 10.1495\n","Time:  0.3518 Epoch: 001 Iter: 19900/39218 LR: 0.0000333333 Loss content:  0.4072 Loss fft: 10.0424\n","Time:  0.3516 Epoch: 001 Iter: 20000/39218 LR: 0.0000333333 Loss content:  0.4028 Loss fft: 10.3638\n","Time:  0.3516 Epoch: 001 Iter: 20100/39218 LR: 0.0000333333 Loss content:  0.4181 Loss fft: 10.4152\n","Time:  0.3519 Epoch: 001 Iter: 20200/39218 LR: 0.0000333333 Loss content:  0.4143 Loss fft: 10.4591\n","Time:  0.3518 Epoch: 001 Iter: 20300/39218 LR: 0.0000333333 Loss content:  0.4088 Loss fft: 10.3210\n","Time:  0.3517 Epoch: 001 Iter: 20400/39218 LR: 0.0000333333 Loss content:  0.3988 Loss fft: 10.2551\n","Time:  0.3516 Epoch: 001 Iter: 20500/39218 LR: 0.0000333333 Loss content:  0.4188 Loss fft: 10.4794\n","Time:  0.3517 Epoch: 001 Iter: 20600/39218 LR: 0.0000333333 Loss content:  0.4105 Loss fft: 10.3550\n","Time:  0.3518 Epoch: 001 Iter: 20700/39218 LR: 0.0000333333 Loss content:  0.4103 Loss fft: 10.3485\n","Time:  0.3517 Epoch: 001 Iter: 20800/39218 LR: 0.0000333333 Loss content:  0.4025 Loss fft: 10.2476\n","Time:  0.3517 Epoch: 001 Iter: 20900/39218 LR: 0.0000333333 Loss content:  0.4035 Loss fft: 10.4444\n","Time:  0.3517 Epoch: 001 Iter: 21000/39218 LR: 0.0000333333 Loss content:  0.4059 Loss fft: 10.2226\n","Time:  0.3517 Epoch: 001 Iter: 21100/39218 LR: 0.0000333333 Loss content:  0.4008 Loss fft: 10.4347\n","Time:  0.3552 Epoch: 001 Iter: 21200/39218 LR: 0.0000333333 Loss content:  0.3970 Loss fft: 10.2629\n","Time:  0.3517 Epoch: 001 Iter: 21300/39218 LR: 0.0000333333 Loss content:  0.4110 Loss fft: 10.2609\n","Time:  0.3515 Epoch: 001 Iter: 21400/39218 LR: 0.0000333333 Loss content:  0.4113 Loss fft: 10.3533\n","Time:  0.3515 Epoch: 001 Iter: 21500/39218 LR: 0.0000333333 Loss content:  0.4068 Loss fft: 10.1738\n","Time:  0.3515 Epoch: 001 Iter: 21600/39218 LR: 0.0000333333 Loss content:  0.4018 Loss fft: 10.2171\n","Time:  0.3515 Epoch: 001 Iter: 21700/39218 LR: 0.0000333333 Loss content:  0.4140 Loss fft: 10.5123\n","Time:  0.3517 Epoch: 001 Iter: 21800/39218 LR: 0.0000333333 Loss content:  0.4057 Loss fft: 10.1420\n","Time:  0.3516 Epoch: 001 Iter: 21900/39218 LR: 0.0000333333 Loss content:  0.4078 Loss fft: 10.3934\n","Time:  0.3516 Epoch: 001 Iter: 22000/39218 LR: 0.0000333333 Loss content:  0.4034 Loss fft: 10.4031\n","Time:  0.3519 Epoch: 001 Iter: 22100/39218 LR: 0.0000333333 Loss content:  0.4118 Loss fft: 10.3246\n","Time:  0.3514 Epoch: 001 Iter: 22200/39218 LR: 0.0000333333 Loss content:  0.4072 Loss fft: 10.3756\n","Time:  0.3517 Epoch: 001 Iter: 22300/39218 LR: 0.0000333333 Loss content:  0.4113 Loss fft: 10.3759\n","Time:  0.3518 Epoch: 001 Iter: 22400/39218 LR: 0.0000333333 Loss content:  0.4099 Loss fft: 10.4835\n","Time:  0.3516 Epoch: 001 Iter: 22500/39218 LR: 0.0000333333 Loss content:  0.3969 Loss fft: 10.1464\n","Time:  0.3516 Epoch: 001 Iter: 22600/39218 LR: 0.0000333333 Loss content:  0.4090 Loss fft: 10.3898\n","Time:  0.3516 Epoch: 001 Iter: 22700/39218 LR: 0.0000333333 Loss content:  0.4177 Loss fft: 10.3243\n","Time:  0.3517 Epoch: 001 Iter: 22800/39218 LR: 0.0000333333 Loss content:  0.4001 Loss fft: 10.1745\n","Time:  0.3516 Epoch: 001 Iter: 22900/39218 LR: 0.0000333333 Loss content:  0.4109 Loss fft: 10.5109\n","Time:  0.3515 Epoch: 001 Iter: 23000/39218 LR: 0.0000333333 Loss content:  0.4089 Loss fft: 10.3059\n","Time:  0.3517 Epoch: 001 Iter: 23100/39218 LR: 0.0000333333 Loss content:  0.4161 Loss fft: 10.4520\n","Time:  0.3516 Epoch: 001 Iter: 23200/39218 LR: 0.0000333333 Loss content:  0.4126 Loss fft: 10.4134\n","Time:  0.3516 Epoch: 001 Iter: 23300/39218 LR: 0.0000333333 Loss content:  0.4111 Loss fft: 10.3721\n","Time:  0.3517 Epoch: 001 Iter: 23400/39218 LR: 0.0000333333 Loss content:  0.4121 Loss fft: 10.4317\n","Time:  0.3552 Epoch: 001 Iter: 23500/39218 LR: 0.0000333333 Loss content:  0.4061 Loss fft: 10.3756\n","Time:  0.3518 Epoch: 001 Iter: 23600/39218 LR: 0.0000333333 Loss content:  0.4113 Loss fft: 10.1773\n","Time:  0.3518 Epoch: 001 Iter: 23700/39218 LR: 0.0000333333 Loss content:  0.4076 Loss fft: 10.5772\n","Time:  0.3515 Epoch: 001 Iter: 23800/39218 LR: 0.0000333333 Loss content:  0.4011 Loss fft: 10.1544\n","Time:  0.3519 Epoch: 001 Iter: 23900/39218 LR: 0.0000333333 Loss content:  0.4097 Loss fft: 10.4293\n","Time:  0.3518 Epoch: 001 Iter: 24000/39218 LR: 0.0000333333 Loss content:  0.4015 Loss fft: 10.2590\n","Time:  0.3519 Epoch: 001 Iter: 24100/39218 LR: 0.0000333333 Loss content:  0.4048 Loss fft: 10.3902\n","Time:  0.3519 Epoch: 001 Iter: 24200/39218 LR: 0.0000333333 Loss content:  0.3998 Loss fft: 10.3319\n","Time:  0.3517 Epoch: 001 Iter: 24300/39218 LR: 0.0000333333 Loss content:  0.4049 Loss fft: 10.2412\n","Time:  0.3517 Epoch: 001 Iter: 24400/39218 LR: 0.0000333333 Loss content:  0.4071 Loss fft: 10.4264\n","Time:  0.3516 Epoch: 001 Iter: 24500/39218 LR: 0.0000333333 Loss content:  0.4043 Loss fft: 10.2610\n","Time:  0.3518 Epoch: 001 Iter: 24600/39218 LR: 0.0000333333 Loss content:  0.4036 Loss fft: 10.2910\n","Time:  0.3517 Epoch: 001 Iter: 24700/39218 LR: 0.0000333333 Loss content:  0.4092 Loss fft: 10.4729\n","Time:  0.3519 Epoch: 001 Iter: 24800/39218 LR: 0.0000333333 Loss content:  0.4114 Loss fft: 10.3483\n","Time:  0.3517 Epoch: 001 Iter: 24900/39218 LR: 0.0000333333 Loss content:  0.4018 Loss fft: 10.1984\n","Time:  0.3516 Epoch: 001 Iter: 25000/39218 LR: 0.0000333333 Loss content:  0.4133 Loss fft: 10.2960\n","Time:  0.3517 Epoch: 001 Iter: 25100/39218 LR: 0.0000333333 Loss content:  0.3991 Loss fft: 10.2713\n","Time:  0.3516 Epoch: 001 Iter: 25200/39218 LR: 0.0000333333 Loss content:  0.4163 Loss fft: 10.4132\n","Time:  0.3517 Epoch: 001 Iter: 25300/39218 LR: 0.0000333333 Loss content:  0.4144 Loss fft: 10.4184\n","Time:  0.3517 Epoch: 001 Iter: 25400/39218 LR: 0.0000333333 Loss content:  0.4067 Loss fft: 10.3823\n","Time:  0.3519 Epoch: 001 Iter: 25500/39218 LR: 0.0000333333 Loss content:  0.4054 Loss fft: 10.4104\n","Time:  0.3519 Epoch: 001 Iter: 25600/39218 LR: 0.0000333333 Loss content:  0.4042 Loss fft: 10.3476\n","Time:  0.3519 Epoch: 001 Iter: 25700/39218 LR: 0.0000333333 Loss content:  0.4178 Loss fft: 10.3181\n","Time:  0.3549 Epoch: 001 Iter: 25800/39218 LR: 0.0000333333 Loss content:  0.4000 Loss fft:  9.9946\n","Time:  0.3515 Epoch: 001 Iter: 25900/39218 LR: 0.0000333333 Loss content:  0.4022 Loss fft: 10.2338\n","Time:  0.3518 Epoch: 001 Iter: 26000/39218 LR: 0.0000333333 Loss content:  0.4009 Loss fft: 10.2523\n","Time:  0.3516 Epoch: 001 Iter: 26100/39218 LR: 0.0000333333 Loss content:  0.4112 Loss fft: 10.3339\n","Time:  0.3514 Epoch: 001 Iter: 26200/39218 LR: 0.0000333333 Loss content:  0.4104 Loss fft: 10.2738\n","Time:  0.3516 Epoch: 001 Iter: 26300/39218 LR: 0.0000333333 Loss content:  0.4041 Loss fft: 10.2307\n","Time:  0.3517 Epoch: 001 Iter: 26400/39218 LR: 0.0000333333 Loss content:  0.4034 Loss fft: 10.3068\n","Time:  0.3516 Epoch: 001 Iter: 26500/39218 LR: 0.0000333333 Loss content:  0.4093 Loss fft: 10.1531\n","Time:  0.3517 Epoch: 001 Iter: 26600/39218 LR: 0.0000333333 Loss content:  0.4055 Loss fft: 10.3108\n","Time:  0.3519 Epoch: 001 Iter: 26700/39218 LR: 0.0000333333 Loss content:  0.4004 Loss fft: 10.2023\n","Time:  0.3516 Epoch: 001 Iter: 26800/39218 LR: 0.0000333333 Loss content:  0.4068 Loss fft: 10.3486\n","Time:  0.3518 Epoch: 001 Iter: 26900/39218 LR: 0.0000333333 Loss content:  0.4088 Loss fft: 10.3902\n","Time:  0.3517 Epoch: 001 Iter: 27000/39218 LR: 0.0000333333 Loss content:  0.4044 Loss fft: 10.1222\n","Time:  0.3518 Epoch: 001 Iter: 27100/39218 LR: 0.0000333333 Loss content:  0.4084 Loss fft: 10.3321\n","Time:  0.3519 Epoch: 001 Iter: 27200/39218 LR: 0.0000333333 Loss content:  0.4043 Loss fft: 10.3077\n","Time:  0.3519 Epoch: 001 Iter: 27300/39218 LR: 0.0000333333 Loss content:  0.4080 Loss fft: 10.2335\n","Time:  0.3518 Epoch: 001 Iter: 27400/39218 LR: 0.0000333333 Loss content:  0.4068 Loss fft: 10.3905\n","Time:  0.3521 Epoch: 001 Iter: 27500/39218 LR: 0.0000333333 Loss content:  0.4071 Loss fft: 10.3051\n","Time:  0.3517 Epoch: 001 Iter: 27600/39218 LR: 0.0000333333 Loss content:  0.4109 Loss fft: 10.3054\n","Time:  0.3516 Epoch: 001 Iter: 27700/39218 LR: 0.0000333333 Loss content:  0.4054 Loss fft: 10.1303\n","Time:  0.3517 Epoch: 001 Iter: 27800/39218 LR: 0.0000333333 Loss content:  0.4110 Loss fft: 10.2455\n","Time:  0.3518 Epoch: 001 Iter: 27900/39218 LR: 0.0000333333 Loss content:  0.4066 Loss fft: 10.1850\n","Time:  0.3520 Epoch: 001 Iter: 28000/39218 LR: 0.0000333333 Loss content:  0.4080 Loss fft: 10.3140\n","Time:  0.3553 Epoch: 001 Iter: 28100/39218 LR: 0.0000333333 Loss content:  0.4107 Loss fft: 10.3604\n","Time:  0.3519 Epoch: 001 Iter: 28200/39218 LR: 0.0000333333 Loss content:  0.4038 Loss fft: 10.4344\n","Time:  0.3518 Epoch: 001 Iter: 28300/39218 LR: 0.0000333333 Loss content:  0.4099 Loss fft: 10.1990\n","Time:  0.3520 Epoch: 001 Iter: 28400/39218 LR: 0.0000333333 Loss content:  0.4024 Loss fft: 10.4286\n","Time:  0.3517 Epoch: 001 Iter: 28500/39218 LR: 0.0000333333 Loss content:  0.4067 Loss fft: 10.3553\n","Time:  0.3516 Epoch: 001 Iter: 28600/39218 LR: 0.0000333333 Loss content:  0.4072 Loss fft: 10.2006\n","Time:  0.3519 Epoch: 001 Iter: 28700/39218 LR: 0.0000333333 Loss content:  0.3970 Loss fft: 10.2121\n","Time:  0.3517 Epoch: 001 Iter: 28800/39218 LR: 0.0000333333 Loss content:  0.4130 Loss fft: 10.5976\n","Time:  0.3518 Epoch: 001 Iter: 28900/39218 LR: 0.0000333333 Loss content:  0.4023 Loss fft: 10.2366\n","Time:  0.3518 Epoch: 001 Iter: 29000/39218 LR: 0.0000333333 Loss content:  0.4122 Loss fft: 10.4723\n","Time:  0.3516 Epoch: 001 Iter: 29100/39218 LR: 0.0000333333 Loss content:  0.4048 Loss fft: 10.3087\n","Time:  0.3519 Epoch: 001 Iter: 29200/39218 LR: 0.0000333333 Loss content:  0.4054 Loss fft: 10.2135\n","Time:  0.3519 Epoch: 001 Iter: 29300/39218 LR: 0.0000333333 Loss content:  0.4001 Loss fft: 10.1089\n","Time:  0.3518 Epoch: 001 Iter: 29400/39218 LR: 0.0000333333 Loss content:  0.4110 Loss fft: 10.1589\n","Time:  0.3517 Epoch: 001 Iter: 29500/39218 LR: 0.0000333333 Loss content:  0.4112 Loss fft: 10.3428\n","Time:  0.3518 Epoch: 001 Iter: 29600/39218 LR: 0.0000333333 Loss content:  0.4006 Loss fft: 10.2483\n","Time:  0.3519 Epoch: 001 Iter: 29700/39218 LR: 0.0000333333 Loss content:  0.4038 Loss fft: 10.4954\n","Time:  0.3517 Epoch: 001 Iter: 29800/39218 LR: 0.0000333333 Loss content:  0.4096 Loss fft: 10.3465\n","Time:  0.3516 Epoch: 001 Iter: 29900/39218 LR: 0.0000333333 Loss content:  0.4226 Loss fft: 10.5726\n","Time:  0.3518 Epoch: 001 Iter: 30000/39218 LR: 0.0000333333 Loss content:  0.4133 Loss fft: 10.1899\n","Time:  0.3520 Epoch: 001 Iter: 30100/39218 LR: 0.0000333333 Loss content:  0.3980 Loss fft: 10.2952\n","Time:  0.3519 Epoch: 001 Iter: 30200/39218 LR: 0.0000333333 Loss content:  0.4037 Loss fft: 10.2647\n","Time:  0.3555 Epoch: 001 Iter: 30300/39218 LR: 0.0000333333 Loss content:  0.4003 Loss fft: 10.2182\n","Time:  0.3517 Epoch: 001 Iter: 30400/39218 LR: 0.0000333333 Loss content:  0.3983 Loss fft: 10.2351\n","Time:  0.3514 Epoch: 001 Iter: 30500/39218 LR: 0.0000333333 Loss content:  0.4083 Loss fft: 10.2976\n","Time:  0.3515 Epoch: 001 Iter: 30600/39218 LR: 0.0000333333 Loss content:  0.4043 Loss fft: 10.2383\n","Time:  0.3517 Epoch: 001 Iter: 30700/39218 LR: 0.0000333333 Loss content:  0.3956 Loss fft: 10.1600\n","Time:  0.3517 Epoch: 001 Iter: 30800/39218 LR: 0.0000333333 Loss content:  0.4024 Loss fft: 10.2254\n","Time:  0.3520 Epoch: 001 Iter: 30900/39218 LR: 0.0000333333 Loss content:  0.4037 Loss fft: 10.1507\n","Time:  0.3517 Epoch: 001 Iter: 31000/39218 LR: 0.0000333333 Loss content:  0.4080 Loss fft: 10.3654\n","Time:  0.3519 Epoch: 001 Iter: 31100/39218 LR: 0.0000333333 Loss content:  0.4061 Loss fft: 10.3740\n","Time:  0.3519 Epoch: 001 Iter: 31200/39218 LR: 0.0000333333 Loss content:  0.3932 Loss fft: 10.1255\n","Time:  0.3516 Epoch: 001 Iter: 31300/39218 LR: 0.0000333333 Loss content:  0.4134 Loss fft: 10.4968\n","Time:  0.3520 Epoch: 001 Iter: 31400/39218 LR: 0.0000333333 Loss content:  0.3999 Loss fft: 10.3036\n","Time:  0.3515 Epoch: 001 Iter: 31500/39218 LR: 0.0000333333 Loss content:  0.4049 Loss fft: 10.3292\n","Time:  0.3517 Epoch: 001 Iter: 31600/39218 LR: 0.0000333333 Loss content:  0.3974 Loss fft: 10.2256\n","Time:  0.3517 Epoch: 001 Iter: 31700/39218 LR: 0.0000333333 Loss content:  0.3990 Loss fft: 10.0068\n","Time:  0.3516 Epoch: 001 Iter: 31800/39218 LR: 0.0000333333 Loss content:  0.4019 Loss fft: 10.2424\n","Time:  0.3516 Epoch: 001 Iter: 31900/39218 LR: 0.0000333333 Loss content:  0.3989 Loss fft: 10.4561\n","Time:  0.3517 Epoch: 001 Iter: 32000/39218 LR: 0.0000333333 Loss content:  0.4046 Loss fft: 10.2402\n","Time:  0.3515 Epoch: 001 Iter: 32100/39218 LR: 0.0000333333 Loss content:  0.3962 Loss fft: 10.3488\n","Time:  0.3519 Epoch: 001 Iter: 32200/39218 LR: 0.0000333333 Loss content:  0.4048 Loss fft: 10.3111\n","Time:  0.3517 Epoch: 001 Iter: 32300/39218 LR: 0.0000333333 Loss content:  0.4148 Loss fft: 10.1000\n","Time:  0.3515 Epoch: 001 Iter: 32400/39218 LR: 0.0000333333 Loss content:  0.4094 Loss fft: 10.2841\n","Time:  0.3516 Epoch: 001 Iter: 32500/39218 LR: 0.0000333333 Loss content:  0.3977 Loss fft: 10.2322\n","Time:  0.3549 Epoch: 001 Iter: 32600/39218 LR: 0.0000333333 Loss content:  0.4033 Loss fft: 10.3949\n","Time:  0.3516 Epoch: 001 Iter: 32700/39218 LR: 0.0000333333 Loss content:  0.3987 Loss fft: 10.2668\n","Time:  0.3518 Epoch: 001 Iter: 32800/39218 LR: 0.0000333333 Loss content:  0.3979 Loss fft: 10.1826\n","Time:  0.3516 Epoch: 001 Iter: 32900/39218 LR: 0.0000333333 Loss content:  0.4102 Loss fft: 10.4121\n","Time:  0.3521 Epoch: 001 Iter: 33000/39218 LR: 0.0000333333 Loss content:  0.4008 Loss fft: 10.2930\n","Time:  0.3518 Epoch: 001 Iter: 33100/39218 LR: 0.0000333333 Loss content:  0.4179 Loss fft: 10.3343\n","Time:  0.3518 Epoch: 001 Iter: 33200/39218 LR: 0.0000333333 Loss content:  0.4084 Loss fft: 10.2127\n","Time:  0.3517 Epoch: 001 Iter: 33300/39218 LR: 0.0000333333 Loss content:  0.4229 Loss fft: 10.5844\n","Time:  0.3519 Epoch: 001 Iter: 33400/39218 LR: 0.0000333333 Loss content:  0.4042 Loss fft: 10.2185\n","Time:  0.3519 Epoch: 001 Iter: 33500/39218 LR: 0.0000333333 Loss content:  0.4033 Loss fft: 10.2325\n","Time:  0.3516 Epoch: 001 Iter: 33600/39218 LR: 0.0000333333 Loss content:  0.4045 Loss fft: 10.3302\n","Time:  0.3517 Epoch: 001 Iter: 33700/39218 LR: 0.0000333333 Loss content:  0.4075 Loss fft: 10.2559\n","Time:  0.3521 Epoch: 001 Iter: 33800/39218 LR: 0.0000333333 Loss content:  0.4080 Loss fft: 10.4167\n","Time:  0.3515 Epoch: 001 Iter: 33900/39218 LR: 0.0000333333 Loss content:  0.4055 Loss fft: 10.3593\n","Time:  0.3518 Epoch: 001 Iter: 34000/39218 LR: 0.0000333333 Loss content:  0.4119 Loss fft: 10.3392\n","Time:  0.3518 Epoch: 001 Iter: 34100/39218 LR: 0.0000333333 Loss content:  0.4069 Loss fft: 10.3494\n","Time:  0.3518 Epoch: 001 Iter: 34200/39218 LR: 0.0000333333 Loss content:  0.3974 Loss fft: 10.2719\n","Time:  0.3517 Epoch: 001 Iter: 34300/39218 LR: 0.0000333333 Loss content:  0.3962 Loss fft: 10.1928\n","Time:  0.3518 Epoch: 001 Iter: 34400/39218 LR: 0.0000333333 Loss content:  0.4105 Loss fft: 10.3154\n","Time:  0.3518 Epoch: 001 Iter: 34500/39218 LR: 0.0000333333 Loss content:  0.4033 Loss fft: 10.2139\n","Time:  0.3517 Epoch: 001 Iter: 34600/39218 LR: 0.0000333333 Loss content:  0.4055 Loss fft: 10.2482\n","Time:  0.3517 Epoch: 001 Iter: 34700/39218 LR: 0.0000333333 Loss content:  0.4015 Loss fft: 10.2811\n","Time:  0.3517 Epoch: 001 Iter: 34800/39218 LR: 0.0000333333 Loss content:  0.4074 Loss fft: 10.3139\n","Time:  0.3551 Epoch: 001 Iter: 34900/39218 LR: 0.0000333333 Loss content:  0.4092 Loss fft: 10.5003\n","Time:  0.3515 Epoch: 001 Iter: 35000/39218 LR: 0.0000333333 Loss content:  0.4009 Loss fft: 10.3513\n","Time:  0.3517 Epoch: 001 Iter: 35100/39218 LR: 0.0000333333 Loss content:  0.4093 Loss fft: 10.3694\n","Time:  0.3517 Epoch: 001 Iter: 35200/39218 LR: 0.0000333333 Loss content:  0.4053 Loss fft: 10.2175\n","Time:  0.3518 Epoch: 001 Iter: 35300/39218 LR: 0.0000333333 Loss content:  0.4087 Loss fft: 10.4084\n","Time:  0.3518 Epoch: 001 Iter: 35400/39218 LR: 0.0000333333 Loss content:  0.4116 Loss fft: 10.4838\n","Time:  0.3516 Epoch: 001 Iter: 35500/39218 LR: 0.0000333333 Loss content:  0.4059 Loss fft: 10.4412\n","Time:  0.3518 Epoch: 001 Iter: 35600/39218 LR: 0.0000333333 Loss content:  0.4056 Loss fft: 10.2850\n","Time:  0.3518 Epoch: 001 Iter: 35700/39218 LR: 0.0000333333 Loss content:  0.4134 Loss fft: 10.4254\n","Time:  0.3519 Epoch: 001 Iter: 35800/39218 LR: 0.0000333333 Loss content:  0.4089 Loss fft: 10.3100\n","Time:  0.3517 Epoch: 001 Iter: 35900/39218 LR: 0.0000333333 Loss content:  0.4012 Loss fft: 10.2785\n","Time:  0.3519 Epoch: 001 Iter: 36000/39218 LR: 0.0000333333 Loss content:  0.4091 Loss fft: 10.2038\n","Time:  0.3519 Epoch: 001 Iter: 36100/39218 LR: 0.0000333333 Loss content:  0.4078 Loss fft: 10.4887\n","Time:  0.3516 Epoch: 001 Iter: 36200/39218 LR: 0.0000333333 Loss content:  0.4118 Loss fft: 10.2930\n","Time:  0.3517 Epoch: 001 Iter: 36300/39218 LR: 0.0000333333 Loss content:  0.3970 Loss fft: 10.1855\n","Time:  0.3517 Epoch: 001 Iter: 36400/39218 LR: 0.0000333333 Loss content:  0.4037 Loss fft: 10.4908\n","Time:  0.3517 Epoch: 001 Iter: 36500/39218 LR: 0.0000333333 Loss content:  0.4010 Loss fft: 10.2823\n","Time:  0.3518 Epoch: 001 Iter: 36600/39218 LR: 0.0000333333 Loss content:  0.4125 Loss fft: 10.2681\n","Time:  0.3517 Epoch: 001 Iter: 36700/39218 LR: 0.0000333333 Loss content:  0.4153 Loss fft: 10.4022\n","Time:  0.3518 Epoch: 001 Iter: 36800/39218 LR: 0.0000333333 Loss content:  0.4093 Loss fft: 10.3768\n","Time:  0.3519 Epoch: 001 Iter: 36900/39218 LR: 0.0000333333 Loss content:  0.4085 Loss fft: 10.2966\n","Time:  0.3517 Epoch: 001 Iter: 37000/39218 LR: 0.0000333333 Loss content:  0.4063 Loss fft: 10.4199\n","Time:  0.3518 Epoch: 001 Iter: 37100/39218 LR: 0.0000333333 Loss content:  0.4069 Loss fft: 10.4478\n","Time:  0.3555 Epoch: 001 Iter: 37200/39218 LR: 0.0000333333 Loss content:  0.4017 Loss fft: 10.0673\n","Time:  0.3518 Epoch: 001 Iter: 37300/39218 LR: 0.0000333333 Loss content:  0.4101 Loss fft: 10.2906\n","Time:  0.3517 Epoch: 001 Iter: 37400/39218 LR: 0.0000333333 Loss content:  0.3967 Loss fft: 10.1924\n","Time:  0.3518 Epoch: 001 Iter: 37500/39218 LR: 0.0000333333 Loss content:  0.4026 Loss fft: 10.2713\n","Time:  0.3517 Epoch: 001 Iter: 37600/39218 LR: 0.0000333333 Loss content:  0.4064 Loss fft: 10.4059\n","Time:  0.3517 Epoch: 001 Iter: 37700/39218 LR: 0.0000333333 Loss content:  0.3952 Loss fft: 10.1928\n","Time:  0.3518 Epoch: 001 Iter: 37800/39218 LR: 0.0000333333 Loss content:  0.4146 Loss fft: 10.2564\n","Time:  0.3520 Epoch: 001 Iter: 37900/39218 LR: 0.0000333333 Loss content:  0.4048 Loss fft: 10.3211\n","Time:  0.3517 Epoch: 001 Iter: 38000/39218 LR: 0.0000333333 Loss content:  0.4068 Loss fft: 10.2823\n","Time:  0.3516 Epoch: 001 Iter: 38100/39218 LR: 0.0000333333 Loss content:  0.4044 Loss fft: 10.3837\n","Time:  0.3517 Epoch: 001 Iter: 38200/39218 LR: 0.0000333333 Loss content:  0.4073 Loss fft: 10.3912\n","Time:  0.3519 Epoch: 001 Iter: 38300/39218 LR: 0.0000333333 Loss content:  0.4029 Loss fft: 10.1893\n","Time:  0.3517 Epoch: 001 Iter: 38400/39218 LR: 0.0000333333 Loss content:  0.4006 Loss fft: 10.3643\n","Time:  0.3518 Epoch: 001 Iter: 38500/39218 LR: 0.0000333333 Loss content:  0.4035 Loss fft: 10.2844\n","Time:  0.3518 Epoch: 001 Iter: 38600/39218 LR: 0.0000333333 Loss content:  0.4048 Loss fft: 10.3148\n","Time:  0.3519 Epoch: 001 Iter: 38700/39218 LR: 0.0000333333 Loss content:  0.3918 Loss fft: 10.1835\n","Time:  0.3518 Epoch: 001 Iter: 38800/39218 LR: 0.0000333333 Loss content:  0.4181 Loss fft: 10.5899\n","Time:  0.3517 Epoch: 001 Iter: 38900/39218 LR: 0.0000333333 Loss content:  0.3888 Loss fft:  9.9077\n","Time:  0.3519 Epoch: 001 Iter: 39000/39218 LR: 0.0000333333 Loss content:  0.3995 Loss fft: 10.2930\n","Time:  0.3519 Epoch: 001 Iter: 39100/39218 LR: 0.0000333333 Loss content:  0.4159 Loss fft: 10.4379\n","Time:  0.3518 Epoch: 001 Iter: 39200/39218 LR: 0.0000333333 Loss content:  0.4023 Loss fft: 10.2716\n","EPOCH: 01\n","Elapsed time: 138.17 Epoch Pixel Loss:  0.4079 Epoch FFT Loss: 10.3216\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Time:  0.3666 Epoch: 002 Iter:  100/39218 LR: 0.0000666667 Loss content:  0.4082 Loss fft: 10.3699\n","Time:  0.3570 Epoch: 002 Iter:  200/39218 LR: 0.0000666667 Loss content:  0.4071 Loss fft: 10.2861\n","Time:  0.3518 Epoch: 002 Iter:  300/39218 LR: 0.0000666667 Loss content:  0.3989 Loss fft: 10.3173\n","Time:  0.3519 Epoch: 002 Iter:  400/39218 LR: 0.0000666667 Loss content:  0.4046 Loss fft: 10.2928\n","Time:  0.3518 Epoch: 002 Iter:  500/39218 LR: 0.0000666667 Loss content:  0.4128 Loss fft: 10.3783\n","Time:  0.3518 Epoch: 002 Iter:  600/39218 LR: 0.0000666667 Loss content:  0.4052 Loss fft: 10.3486\n","Time:  0.3517 Epoch: 002 Iter:  700/39218 LR: 0.0000666667 Loss content:  0.4126 Loss fft: 10.2989\n","Time:  0.3517 Epoch: 002 Iter:  800/39218 LR: 0.0000666667 Loss content:  0.3978 Loss fft: 10.1267\n","Time:  0.3518 Epoch: 002 Iter:  900/39218 LR: 0.0000666667 Loss content:  0.3942 Loss fft: 10.1987\n","Time:  0.3518 Epoch: 002 Iter: 1000/39218 LR: 0.0000666667 Loss content:  0.4060 Loss fft: 10.5140\n","Time:  0.3520 Epoch: 002 Iter: 1100/39218 LR: 0.0000666667 Loss content:  0.4133 Loss fft: 10.3850\n","Time:  0.3517 Epoch: 002 Iter: 1200/39218 LR: 0.0000666667 Loss content:  0.4115 Loss fft: 10.3941\n","Time:  0.3518 Epoch: 002 Iter: 1300/39218 LR: 0.0000666667 Loss content:  0.4011 Loss fft: 10.3283\n","Time:  0.3518 Epoch: 002 Iter: 1400/39218 LR: 0.0000666667 Loss content:  0.4014 Loss fft: 10.2625\n","Time:  0.3519 Epoch: 002 Iter: 1500/39218 LR: 0.0000666667 Loss content:  0.4085 Loss fft: 10.3077\n","Time:  0.3517 Epoch: 002 Iter: 1600/39218 LR: 0.0000666667 Loss content:  0.4025 Loss fft: 10.1871\n","Time:  0.3520 Epoch: 002 Iter: 1700/39218 LR: 0.0000666667 Loss content:  0.4084 Loss fft: 10.2523\n","Time:  0.3517 Epoch: 002 Iter: 1800/39218 LR: 0.0000666667 Loss content:  0.3972 Loss fft: 10.2220\n","Time:  0.3517 Epoch: 002 Iter: 1900/39218 LR: 0.0000666667 Loss content:  0.4056 Loss fft: 10.3657\n","Time:  0.3520 Epoch: 002 Iter: 2000/39218 LR: 0.0000666667 Loss content:  0.4034 Loss fft: 10.2343\n","Time:  0.3519 Epoch: 002 Iter: 2100/39218 LR: 0.0000666667 Loss content:  0.4048 Loss fft: 10.2422\n","Time:  0.3517 Epoch: 002 Iter: 2200/39218 LR: 0.0000666667 Loss content:  0.4077 Loss fft: 10.1800\n","Time:  0.3519 Epoch: 002 Iter: 2300/39218 LR: 0.0000666667 Loss content:  0.4047 Loss fft: 10.1914\n","Time:  0.3518 Epoch: 002 Iter: 2400/39218 LR: 0.0000666667 Loss content:  0.4066 Loss fft: 10.1341\n","Time:  0.3552 Epoch: 002 Iter: 2500/39218 LR: 0.0000666667 Loss content:  0.4106 Loss fft: 10.3759\n","Time:  0.3517 Epoch: 002 Iter: 2600/39218 LR: 0.0000666667 Loss content:  0.4071 Loss fft: 10.4060\n","Time:  0.3517 Epoch: 002 Iter: 2700/39218 LR: 0.0000666667 Loss content:  0.4068 Loss fft: 10.4204\n","Time:  0.3518 Epoch: 002 Iter: 2800/39218 LR: 0.0000666667 Loss content:  0.4076 Loss fft: 10.2488\n","Time:  0.3518 Epoch: 002 Iter: 2900/39218 LR: 0.0000666667 Loss content:  0.4004 Loss fft: 10.1615\n","Time:  0.3519 Epoch: 002 Iter: 3000/39218 LR: 0.0000666667 Loss content:  0.3960 Loss fft: 10.3406\n","Time:  0.3519 Epoch: 002 Iter: 3100/39218 LR: 0.0000666667 Loss content:  0.4058 Loss fft: 10.4836\n","Time:  0.3519 Epoch: 002 Iter: 3200/39218 LR: 0.0000666667 Loss content:  0.4073 Loss fft: 10.4037\n","Time:  0.3520 Epoch: 002 Iter: 3300/39218 LR: 0.0000666667 Loss content:  0.3956 Loss fft: 10.1878\n","Time:  0.3519 Epoch: 002 Iter: 3400/39218 LR: 0.0000666667 Loss content:  0.4045 Loss fft: 10.2378\n","Time:  0.3518 Epoch: 002 Iter: 3500/39218 LR: 0.0000666667 Loss content:  0.4123 Loss fft: 10.4892\n","Time:  0.3518 Epoch: 002 Iter: 3600/39218 LR: 0.0000666667 Loss content:  0.4134 Loss fft: 10.3928\n","Time:  0.3519 Epoch: 002 Iter: 3700/39218 LR: 0.0000666667 Loss content:  0.4043 Loss fft: 10.5285\n","Time:  0.3519 Epoch: 002 Iter: 3800/39218 LR: 0.0000666667 Loss content:  0.4031 Loss fft: 10.2129\n","Time:  0.3518 Epoch: 002 Iter: 3900/39218 LR: 0.0000666667 Loss content:  0.4039 Loss fft: 10.1928\n","Time:  0.3521 Epoch: 002 Iter: 4000/39218 LR: 0.0000666667 Loss content:  0.4066 Loss fft: 10.2730\n","Time:  0.3518 Epoch: 002 Iter: 4100/39218 LR: 0.0000666667 Loss content:  0.4024 Loss fft: 10.3067\n","Time:  0.3518 Epoch: 002 Iter: 4200/39218 LR: 0.0000666667 Loss content:  0.3969 Loss fft: 10.1060\n","Time:  0.3517 Epoch: 002 Iter: 4300/39218 LR: 0.0000666667 Loss content:  0.4042 Loss fft: 10.1318\n","Time:  0.3519 Epoch: 002 Iter: 4400/39218 LR: 0.0000666667 Loss content:  0.3982 Loss fft: 10.3394\n","Time:  0.3521 Epoch: 002 Iter: 4500/39218 LR: 0.0000666667 Loss content:  0.4042 Loss fft: 10.2974\n","Time:  0.3518 Epoch: 002 Iter: 4600/39218 LR: 0.0000666667 Loss content:  0.4100 Loss fft: 10.2015\n","Time:  0.3555 Epoch: 002 Iter: 4700/39218 LR: 0.0000666667 Loss content:  0.4078 Loss fft: 10.2678\n","Time:  0.3517 Epoch: 002 Iter: 4800/39218 LR: 0.0000666667 Loss content:  0.4017 Loss fft: 10.2775\n","Time:  0.3520 Epoch: 002 Iter: 4900/39218 LR: 0.0000666667 Loss content:  0.4010 Loss fft: 10.2916\n","Time:  0.3517 Epoch: 002 Iter: 5000/39218 LR: 0.0000666667 Loss content:  0.4037 Loss fft: 10.2397\n","Time:  0.3519 Epoch: 002 Iter: 5100/39218 LR: 0.0000666667 Loss content:  0.4124 Loss fft: 10.4839\n","Time:  0.3518 Epoch: 002 Iter: 5200/39218 LR: 0.0000666667 Loss content:  0.4015 Loss fft: 10.1958\n","Time:  0.3519 Epoch: 002 Iter: 5300/39218 LR: 0.0000666667 Loss content:  0.4080 Loss fft: 10.3156\n","Time:  0.3518 Epoch: 002 Iter: 5400/39218 LR: 0.0000666667 Loss content:  0.4023 Loss fft: 10.2536\n","Time:  0.3519 Epoch: 002 Iter: 5500/39218 LR: 0.0000666667 Loss content:  0.4052 Loss fft: 10.5608\n","Time:  0.3520 Epoch: 002 Iter: 5600/39218 LR: 0.0000666667 Loss content:  0.4021 Loss fft: 10.3158\n","Time:  0.3517 Epoch: 002 Iter: 5700/39218 LR: 0.0000666667 Loss content:  0.3955 Loss fft: 10.1928\n","Time:  0.3520 Epoch: 002 Iter: 5800/39218 LR: 0.0000666667 Loss content:  0.3926 Loss fft: 10.1632\n","Time:  0.3519 Epoch: 002 Iter: 5900/39218 LR: 0.0000666667 Loss content:  0.3985 Loss fft: 10.2850\n","Time:  0.3520 Epoch: 002 Iter: 6000/39218 LR: 0.0000666667 Loss content:  0.4037 Loss fft: 10.2306\n","Time:  0.3521 Epoch: 002 Iter: 6100/39218 LR: 0.0000666667 Loss content:  0.3973 Loss fft: 10.3427\n","Time:  0.3518 Epoch: 002 Iter: 6200/39218 LR: 0.0000666667 Loss content:  0.4012 Loss fft: 10.2922\n","Time:  0.3520 Epoch: 002 Iter: 6300/39218 LR: 0.0000666667 Loss content:  0.3998 Loss fft: 10.4122\n","Time:  0.3521 Epoch: 002 Iter: 6400/39218 LR: 0.0000666667 Loss content:  0.3996 Loss fft: 10.3425\n","Time:  0.3519 Epoch: 002 Iter: 6500/39218 LR: 0.0000666667 Loss content:  0.4094 Loss fft: 10.4839\n","Time:  0.3520 Epoch: 002 Iter: 6600/39218 LR: 0.0000666667 Loss content:  0.3955 Loss fft:  9.9748\n","Time:  0.3521 Epoch: 002 Iter: 6700/39218 LR: 0.0000666667 Loss content:  0.4037 Loss fft: 10.2414\n","Time:  0.3519 Epoch: 002 Iter: 6800/39218 LR: 0.0000666667 Loss content:  0.4019 Loss fft: 10.2674\n","Time:  0.3518 Epoch: 002 Iter: 6900/39218 LR: 0.0000666667 Loss content:  0.3970 Loss fft: 10.3240\n","Time:  0.3556 Epoch: 002 Iter: 7000/39218 LR: 0.0000666667 Loss content:  0.4010 Loss fft: 10.1664\n","Time:  0.3519 Epoch: 002 Iter: 7100/39218 LR: 0.0000666667 Loss content:  0.3949 Loss fft: 10.3340\n","Time:  0.3519 Epoch: 002 Iter: 7200/39218 LR: 0.0000666667 Loss content:  0.4020 Loss fft: 10.4238\n","Time:  0.3518 Epoch: 002 Iter: 7300/39218 LR: 0.0000666667 Loss content:  0.3982 Loss fft: 10.3139\n","Time:  0.3518 Epoch: 002 Iter: 7400/39218 LR: 0.0000666667 Loss content:  0.3883 Loss fft: 10.1031\n","Time:  0.3521 Epoch: 002 Iter: 7500/39218 LR: 0.0000666667 Loss content:  0.3956 Loss fft: 10.2624\n","Time:  0.3518 Epoch: 002 Iter: 7600/39218 LR: 0.0000666667 Loss content:  0.4103 Loss fft: 10.2684\n","Time:  0.3521 Epoch: 002 Iter: 7700/39218 LR: 0.0000666667 Loss content:  0.4055 Loss fft: 10.2438\n","Time:  0.3517 Epoch: 002 Iter: 7800/39218 LR: 0.0000666667 Loss content:  0.4049 Loss fft: 10.5234\n","Time:  0.3519 Epoch: 002 Iter: 7900/39218 LR: 0.0000666667 Loss content:  0.4010 Loss fft: 10.2705\n","Time:  0.3517 Epoch: 002 Iter: 8000/39218 LR: 0.0000666667 Loss content:  0.4016 Loss fft: 10.3366\n","Time:  0.3519 Epoch: 002 Iter: 8100/39218 LR: 0.0000666667 Loss content:  0.3942 Loss fft: 10.2430\n","Time:  0.3517 Epoch: 002 Iter: 8200/39218 LR: 0.0000666667 Loss content:  0.4002 Loss fft: 10.2241\n","Time:  0.3517 Epoch: 002 Iter: 8300/39218 LR: 0.0000666667 Loss content:  0.3967 Loss fft: 10.2130\n","Time:  0.3521 Epoch: 002 Iter: 8400/39218 LR: 0.0000666667 Loss content:  0.3954 Loss fft: 10.1925\n","Time:  0.3519 Epoch: 002 Iter: 8500/39218 LR: 0.0000666667 Loss content:  0.4021 Loss fft: 10.2605\n","Time:  0.3518 Epoch: 002 Iter: 8600/39218 LR: 0.0000666667 Loss content:  0.3908 Loss fft: 10.1580\n","Time:  0.3519 Epoch: 002 Iter: 8700/39218 LR: 0.0000666667 Loss content:  0.4096 Loss fft: 10.2983\n","Time:  0.3518 Epoch: 002 Iter: 8800/39218 LR: 0.0000666667 Loss content:  0.4003 Loss fft: 10.3038\n","Time:  0.3521 Epoch: 002 Iter: 8900/39218 LR: 0.0000666667 Loss content:  0.3981 Loss fft: 10.1703\n","Time:  0.3518 Epoch: 002 Iter: 9000/39218 LR: 0.0000666667 Loss content:  0.3957 Loss fft: 10.2118\n","Time:  0.3520 Epoch: 002 Iter: 9100/39218 LR: 0.0000666667 Loss content:  0.4024 Loss fft: 10.2609\n","Time:  0.3519 Epoch: 002 Iter: 9200/39218 LR: 0.0000666667 Loss content:  0.3991 Loss fft: 10.1780\n","Time:  0.3554 Epoch: 002 Iter: 9300/39218 LR: 0.0000666667 Loss content:  0.4032 Loss fft: 10.3612\n","Time:  0.3520 Epoch: 002 Iter: 9400/39218 LR: 0.0000666667 Loss content:  0.4023 Loss fft: 10.3235\n","Time:  0.3520 Epoch: 002 Iter: 9500/39218 LR: 0.0000666667 Loss content:  0.3988 Loss fft: 10.3264\n","Time:  0.3519 Epoch: 002 Iter: 9600/39218 LR: 0.0000666667 Loss content:  0.3984 Loss fft: 10.0531\n","Time:  0.3520 Epoch: 002 Iter: 9700/39218 LR: 0.0000666667 Loss content:  0.3982 Loss fft: 10.3401\n","Time:  0.3520 Epoch: 002 Iter: 9800/39218 LR: 0.0000666667 Loss content:  0.4045 Loss fft: 10.3705\n","Time:  0.3519 Epoch: 002 Iter: 9900/39218 LR: 0.0000666667 Loss content:  0.3993 Loss fft: 10.3120\n","Time:  0.3517 Epoch: 002 Iter: 10000/39218 LR: 0.0000666667 Loss content:  0.3977 Loss fft: 10.1912\n","Time:  0.3517 Epoch: 002 Iter: 10100/39218 LR: 0.0000666667 Loss content:  0.4054 Loss fft: 10.4609\n","Time:  0.3520 Epoch: 002 Iter: 10200/39218 LR: 0.0000666667 Loss content:  0.3948 Loss fft: 10.3649\n","Time:  0.3518 Epoch: 002 Iter: 10300/39218 LR: 0.0000666667 Loss content:  0.4041 Loss fft: 10.3190\n","Time:  0.3520 Epoch: 002 Iter: 10400/39218 LR: 0.0000666667 Loss content:  0.3968 Loss fft: 10.1389\n","Time:  0.3518 Epoch: 002 Iter: 10500/39218 LR: 0.0000666667 Loss content:  0.4033 Loss fft: 10.3574\n","Time:  0.3517 Epoch: 002 Iter: 10600/39218 LR: 0.0000666667 Loss content:  0.4051 Loss fft: 10.4725\n","Time:  0.3519 Epoch: 002 Iter: 10700/39218 LR: 0.0000666667 Loss content:  0.4058 Loss fft: 10.3933\n","Time:  0.3520 Epoch: 002 Iter: 10800/39218 LR: 0.0000666667 Loss content:  0.3994 Loss fft: 10.2513\n","Time:  0.3519 Epoch: 002 Iter: 10900/39218 LR: 0.0000666667 Loss content:  0.4048 Loss fft: 10.3728\n","Time:  0.3520 Epoch: 002 Iter: 11000/39218 LR: 0.0000666667 Loss content:  0.4059 Loss fft: 10.3535\n","Time:  0.3518 Epoch: 002 Iter: 11100/39218 LR: 0.0000666667 Loss content:  0.4020 Loss fft: 10.4675\n","Time:  0.3517 Epoch: 002 Iter: 11200/39218 LR: 0.0000666667 Loss content:  0.4006 Loss fft: 10.2947\n","Time:  0.3518 Epoch: 002 Iter: 11300/39218 LR: 0.0000666667 Loss content:  0.3952 Loss fft: 10.1166\n","Time:  0.3519 Epoch: 002 Iter: 11400/39218 LR: 0.0000666667 Loss content:  0.4015 Loss fft: 10.1666\n","Time:  0.3519 Epoch: 002 Iter: 11500/39218 LR: 0.0000666667 Loss content:  0.3958 Loss fft: 10.4159\n","Time:  0.3551 Epoch: 002 Iter: 11600/39218 LR: 0.0000666667 Loss content:  0.4028 Loss fft: 10.2284\n","Time:  0.3519 Epoch: 002 Iter: 11700/39218 LR: 0.0000666667 Loss content:  0.4025 Loss fft: 10.3703\n","Time:  0.3519 Epoch: 002 Iter: 11800/39218 LR: 0.0000666667 Loss content:  0.4003 Loss fft: 10.0903\n","Time:  0.3520 Epoch: 002 Iter: 11900/39218 LR: 0.0000666667 Loss content:  0.3927 Loss fft: 10.1262\n","Time:  0.3517 Epoch: 002 Iter: 12000/39218 LR: 0.0000666667 Loss content:  0.4061 Loss fft: 10.4385\n","Time:  0.3517 Epoch: 002 Iter: 12100/39218 LR: 0.0000666667 Loss content:  0.3924 Loss fft: 10.1870\n","Time:  0.3518 Epoch: 002 Iter: 12200/39218 LR: 0.0000666667 Loss content:  0.4026 Loss fft: 10.3442\n","Time:  0.3524 Epoch: 002 Iter: 12300/39218 LR: 0.0000666667 Loss content:  0.3996 Loss fft: 10.3196\n","Time:  0.3519 Epoch: 002 Iter: 12400/39218 LR: 0.0000666667 Loss content:  0.4000 Loss fft: 10.1970\n","Time:  0.3517 Epoch: 002 Iter: 12500/39218 LR: 0.0000666667 Loss content:  0.4039 Loss fft: 10.1835\n","Time:  0.3519 Epoch: 002 Iter: 12600/39218 LR: 0.0000666667 Loss content:  0.3989 Loss fft: 10.2145\n","Time:  0.3521 Epoch: 002 Iter: 12700/39218 LR: 0.0000666667 Loss content:  0.4018 Loss fft: 10.3174\n","Time:  0.3517 Epoch: 002 Iter: 12800/39218 LR: 0.0000666667 Loss content:  0.3953 Loss fft: 10.1924\n","Time:  0.3517 Epoch: 002 Iter: 12900/39218 LR: 0.0000666667 Loss content:  0.3999 Loss fft: 10.2993\n","Time:  0.3520 Epoch: 002 Iter: 13000/39218 LR: 0.0000666667 Loss content:  0.4034 Loss fft: 10.1867\n","Time:  0.3518 Epoch: 002 Iter: 13100/39218 LR: 0.0000666667 Loss content:  0.3966 Loss fft: 10.2913\n","Time:  0.3520 Epoch: 002 Iter: 13200/39218 LR: 0.0000666667 Loss content:  0.3929 Loss fft: 10.1308\n","Time:  0.3519 Epoch: 002 Iter: 13300/39218 LR: 0.0000666667 Loss content:  0.4065 Loss fft: 10.3515\n","Time:  0.3520 Epoch: 002 Iter: 13400/39218 LR: 0.0000666667 Loss content:  0.4029 Loss fft: 10.3193\n","Time:  0.3518 Epoch: 002 Iter: 13500/39218 LR: 0.0000666667 Loss content:  0.3979 Loss fft: 10.2707\n","Time:  0.3519 Epoch: 002 Iter: 13600/39218 LR: 0.0000666667 Loss content:  0.3970 Loss fft: 10.1267\n","Time:  0.3519 Epoch: 002 Iter: 13700/39218 LR: 0.0000666667 Loss content:  0.3941 Loss fft: 10.1811\n","Time:  0.3552 Epoch: 002 Iter: 13800/39218 LR: 0.0000666667 Loss content:  0.3942 Loss fft: 10.2315\n","Time:  0.3514 Epoch: 002 Iter: 13900/39218 LR: 0.0000666667 Loss content:  0.4013 Loss fft: 10.2478\n","Time:  0.3517 Epoch: 002 Iter: 14000/39218 LR: 0.0000666667 Loss content:  0.4059 Loss fft: 10.4305\n","Time:  0.3517 Epoch: 002 Iter: 14100/39218 LR: 0.0000666667 Loss content:  0.4142 Loss fft: 10.4564\n","Time:  0.3519 Epoch: 002 Iter: 14200/39218 LR: 0.0000666667 Loss content:  0.4005 Loss fft: 10.4149\n","Time:  0.3519 Epoch: 002 Iter: 14300/39218 LR: 0.0000666667 Loss content:  0.3964 Loss fft: 10.2340\n","Time:  0.3517 Epoch: 002 Iter: 14400/39218 LR: 0.0000666667 Loss content:  0.3970 Loss fft: 10.1659\n","Time:  0.3523 Epoch: 002 Iter: 14500/39218 LR: 0.0000666667 Loss content:  0.3963 Loss fft: 10.3018\n","Time:  0.3518 Epoch: 002 Iter: 14600/39218 LR: 0.0000666667 Loss content:  0.4032 Loss fft: 10.3983\n","Time:  0.3519 Epoch: 002 Iter: 14700/39218 LR: 0.0000666667 Loss content:  0.3988 Loss fft: 10.3836\n","Time:  0.3519 Epoch: 002 Iter: 14800/39218 LR: 0.0000666667 Loss content:  0.4059 Loss fft: 10.4322\n","Time:  0.3517 Epoch: 002 Iter: 14900/39218 LR: 0.0000666667 Loss content:  0.4019 Loss fft: 10.3104\n","Time:  0.3519 Epoch: 002 Iter: 15000/39218 LR: 0.0000666667 Loss content:  0.3977 Loss fft: 10.2350\n","Time:  0.3518 Epoch: 002 Iter: 15100/39218 LR: 0.0000666667 Loss content:  0.3958 Loss fft: 10.1778\n","Time:  0.3518 Epoch: 002 Iter: 15200/39218 LR: 0.0000666667 Loss content:  0.4051 Loss fft: 10.2906\n","Time:  0.3517 Epoch: 002 Iter: 15300/39218 LR: 0.0000666667 Loss content:  0.3968 Loss fft: 10.4156\n","Time:  0.3517 Epoch: 002 Iter: 15400/39218 LR: 0.0000666667 Loss content:  0.3990 Loss fft: 10.2517\n","Time:  0.3519 Epoch: 002 Iter: 15500/39218 LR: 0.0000666667 Loss content:  0.4021 Loss fft: 10.2999\n","Time:  0.3520 Epoch: 002 Iter: 15600/39218 LR: 0.0000666667 Loss content:  0.4011 Loss fft: 10.2389\n","Time:  0.3520 Epoch: 002 Iter: 15700/39218 LR: 0.0000666667 Loss content:  0.4014 Loss fft: 10.2815\n","Time:  0.3520 Epoch: 002 Iter: 15800/39218 LR: 0.0000666667 Loss content:  0.3893 Loss fft: 10.2132\n","Time:  0.3518 Epoch: 002 Iter: 15900/39218 LR: 0.0000666667 Loss content:  0.3964 Loss fft: 10.0956\n","Time:  0.3518 Epoch: 002 Iter: 16000/39218 LR: 0.0000666667 Loss content:  0.3997 Loss fft: 10.2433\n","Time:  0.3553 Epoch: 002 Iter: 16100/39218 LR: 0.0000666667 Loss content:  0.3909 Loss fft: 10.1704\n","Time:  0.3518 Epoch: 002 Iter: 16200/39218 LR: 0.0000666667 Loss content:  0.4018 Loss fft: 10.3947\n","Time:  0.3517 Epoch: 002 Iter: 16300/39218 LR: 0.0000666667 Loss content:  0.3935 Loss fft: 10.2080\n","Time:  0.3514 Epoch: 002 Iter: 16400/39218 LR: 0.0000666667 Loss content:  0.3983 Loss fft: 10.2191\n","Time:  0.3514 Epoch: 002 Iter: 16500/39218 LR: 0.0000666667 Loss content:  0.3974 Loss fft: 10.0835\n","Time:  0.3514 Epoch: 002 Iter: 16600/39218 LR: 0.0000666667 Loss content:  0.4085 Loss fft: 10.3816\n","Time:  0.3516 Epoch: 002 Iter: 16700/39218 LR: 0.0000666667 Loss content:  0.3937 Loss fft: 10.1313\n","Time:  0.3512 Epoch: 002 Iter: 16800/39218 LR: 0.0000666667 Loss content:  0.3943 Loss fft: 10.1558\n","Time:  0.3514 Epoch: 002 Iter: 16900/39218 LR: 0.0000666667 Loss content:  0.4008 Loss fft: 10.3142\n","Time:  0.3514 Epoch: 002 Iter: 17000/39218 LR: 0.0000666667 Loss content:  0.3901 Loss fft: 10.0259\n","Time:  0.3513 Epoch: 002 Iter: 17100/39218 LR: 0.0000666667 Loss content:  0.3983 Loss fft: 10.2228\n","Time:  0.3516 Epoch: 002 Iter: 17200/39218 LR: 0.0000666667 Loss content:  0.3958 Loss fft: 10.3195\n","Time:  0.3513 Epoch: 002 Iter: 17300/39218 LR: 0.0000666667 Loss content:  0.4061 Loss fft: 10.2304\n","Time:  0.3519 Epoch: 002 Iter: 17400/39218 LR: 0.0000666667 Loss content:  0.4056 Loss fft: 10.3649\n","Time:  0.3515 Epoch: 002 Iter: 17500/39218 LR: 0.0000666667 Loss content:  0.3940 Loss fft: 10.1886\n","Time:  0.3512 Epoch: 002 Iter: 17600/39218 LR: 0.0000666667 Loss content:  0.3913 Loss fft: 10.3842\n","Time:  0.3515 Epoch: 002 Iter: 17700/39218 LR: 0.0000666667 Loss content:  0.3971 Loss fft: 10.4113\n","Time:  0.3515 Epoch: 002 Iter: 17800/39218 LR: 0.0000666667 Loss content:  0.4035 Loss fft: 10.4979\n","Time:  0.3514 Epoch: 002 Iter: 17900/39218 LR: 0.0000666667 Loss content:  0.3978 Loss fft: 10.2767\n","Time:  0.3513 Epoch: 002 Iter: 18000/39218 LR: 0.0000666667 Loss content:  0.3975 Loss fft: 10.3861\n","Time:  0.3514 Epoch: 002 Iter: 18100/39218 LR: 0.0000666667 Loss content:  0.3979 Loss fft: 10.2952\n","Time:  0.3515 Epoch: 002 Iter: 18200/39218 LR: 0.0000666667 Loss content:  0.3931 Loss fft: 10.3704\n","Time:  0.3514 Epoch: 002 Iter: 18300/39218 LR: 0.0000666667 Loss content:  0.3906 Loss fft: 10.2142\n","Time:  0.3544 Epoch: 002 Iter: 18400/39218 LR: 0.0000666667 Loss content:  0.4015 Loss fft: 10.0681\n","Time:  0.3512 Epoch: 002 Iter: 18500/39218 LR: 0.0000666667 Loss content:  0.4004 Loss fft: 10.3552\n","Time:  0.3516 Epoch: 002 Iter: 18600/39218 LR: 0.0000666667 Loss content:  0.3950 Loss fft: 10.2142\n","Time:  0.3514 Epoch: 002 Iter: 18700/39218 LR: 0.0000666667 Loss content:  0.3939 Loss fft: 10.2026\n","Time:  0.3516 Epoch: 002 Iter: 18800/39218 LR: 0.0000666667 Loss content:  0.3925 Loss fft: 10.2685\n","Time:  0.3514 Epoch: 002 Iter: 18900/39218 LR: 0.0000666667 Loss content:  0.4055 Loss fft: 10.2108\n","Time:  0.3514 Epoch: 002 Iter: 19000/39218 LR: 0.0000666667 Loss content:  0.3848 Loss fft: 10.0877\n","Time:  0.3516 Epoch: 002 Iter: 19100/39218 LR: 0.0000666667 Loss content:  0.3961 Loss fft: 10.1954\n","Time:  0.3513 Epoch: 002 Iter: 19200/39218 LR: 0.0000666667 Loss content:  0.3994 Loss fft: 10.2943\n","Time:  0.3514 Epoch: 002 Iter: 19300/39218 LR: 0.0000666667 Loss content:  0.3976 Loss fft: 10.0730\n","Time:  0.3514 Epoch: 002 Iter: 19400/39218 LR: 0.0000666667 Loss content:  0.3991 Loss fft: 10.3597\n","Time:  0.3515 Epoch: 002 Iter: 19500/39218 LR: 0.0000666667 Loss content:  0.3965 Loss fft: 10.3152\n","Time:  0.3513 Epoch: 002 Iter: 19600/39218 LR: 0.0000666667 Loss content:  0.4006 Loss fft: 10.2690\n","Time:  0.3516 Epoch: 002 Iter: 19700/39218 LR: 0.0000666667 Loss content:  0.3961 Loss fft: 10.3239\n","Time:  0.3516 Epoch: 002 Iter: 19800/39218 LR: 0.0000666667 Loss content:  0.3999 Loss fft: 10.3898\n","Time:  0.3515 Epoch: 002 Iter: 19900/39218 LR: 0.0000666667 Loss content:  0.3896 Loss fft: 10.2614\n","Time:  0.3515 Epoch: 002 Iter: 20000/39218 LR: 0.0000666667 Loss content:  0.3840 Loss fft: 10.1215\n","Time:  0.3517 Epoch: 002 Iter: 20100/39218 LR: 0.0000666667 Loss content:  0.3993 Loss fft: 10.2671\n","Time:  0.3514 Epoch: 002 Iter: 20200/39218 LR: 0.0000666667 Loss content:  0.4066 Loss fft: 10.3125\n","Time:  0.3514 Epoch: 002 Iter: 20300/39218 LR: 0.0000666667 Loss content:  0.3984 Loss fft: 10.2726\n","Time:  0.3514 Epoch: 002 Iter: 20400/39218 LR: 0.0000666667 Loss content:  0.3952 Loss fft: 10.4478\n","Time:  0.3514 Epoch: 002 Iter: 20500/39218 LR: 0.0000666667 Loss content:  0.4002 Loss fft: 10.3767\n","Time:  0.3514 Epoch: 002 Iter: 20600/39218 LR: 0.0000666667 Loss content:  0.3881 Loss fft: 10.1157\n","Time:  0.3546 Epoch: 002 Iter: 20700/39218 LR: 0.0000666667 Loss content:  0.3975 Loss fft: 10.3803\n","Time:  0.3514 Epoch: 002 Iter: 20800/39218 LR: 0.0000666667 Loss content:  0.3957 Loss fft: 10.1386\n","Time:  0.3514 Epoch: 002 Iter: 20900/39218 LR: 0.0000666667 Loss content:  0.3953 Loss fft: 10.2271\n","Time:  0.3514 Epoch: 002 Iter: 21000/39218 LR: 0.0000666667 Loss content:  0.3956 Loss fft: 10.1351\n","Time:  0.3517 Epoch: 002 Iter: 21100/39218 LR: 0.0000666667 Loss content:  0.3919 Loss fft:  9.9823\n","Time:  0.3516 Epoch: 002 Iter: 21200/39218 LR: 0.0000666667 Loss content:  0.3944 Loss fft: 10.0898\n","Time:  0.3514 Epoch: 002 Iter: 21300/39218 LR: 0.0000666667 Loss content:  0.3957 Loss fft: 10.0931\n","Time:  0.3514 Epoch: 002 Iter: 21400/39218 LR: 0.0000666667 Loss content:  0.4070 Loss fft: 10.2028\n","Time:  0.3515 Epoch: 002 Iter: 21500/39218 LR: 0.0000666667 Loss content:  0.3959 Loss fft:  9.9548\n","Time:  0.3513 Epoch: 002 Iter: 21600/39218 LR: 0.0000666667 Loss content:  0.3996 Loss fft: 10.3852\n","Time:  0.3514 Epoch: 002 Iter: 21700/39218 LR: 0.0000666667 Loss content:  0.3922 Loss fft: 10.1905\n","Time:  0.3516 Epoch: 002 Iter: 21800/39218 LR: 0.0000666667 Loss content:  0.4011 Loss fft: 10.3281\n","Time:  0.3516 Epoch: 002 Iter: 21900/39218 LR: 0.0000666667 Loss content:  0.3963 Loss fft: 10.3053\n","Time:  0.3516 Epoch: 002 Iter: 22000/39218 LR: 0.0000666667 Loss content:  0.3983 Loss fft: 10.2470\n","Time:  0.3515 Epoch: 002 Iter: 22100/39218 LR: 0.0000666667 Loss content:  0.3889 Loss fft: 10.0723\n","Time:  0.3513 Epoch: 002 Iter: 22200/39218 LR: 0.0000666667 Loss content:  0.3933 Loss fft: 10.3038\n","Time:  0.3514 Epoch: 002 Iter: 22300/39218 LR: 0.0000666667 Loss content:  0.3884 Loss fft: 10.1871\n","Time:  0.3514 Epoch: 002 Iter: 22400/39218 LR: 0.0000666667 Loss content:  0.3886 Loss fft: 10.1169\n","Time:  0.3514 Epoch: 002 Iter: 22500/39218 LR: 0.0000666667 Loss content:  0.4011 Loss fft: 10.2907\n","Time:  0.3514 Epoch: 002 Iter: 22600/39218 LR: 0.0000666667 Loss content:  0.3951 Loss fft: 10.3302\n","Time:  0.3513 Epoch: 002 Iter: 22700/39218 LR: 0.0000666667 Loss content:  0.4026 Loss fft: 10.2421\n","Time:  0.3515 Epoch: 002 Iter: 22800/39218 LR: 0.0000666667 Loss content:  0.3967 Loss fft: 10.2208\n","Time:  0.3546 Epoch: 002 Iter: 22900/39218 LR: 0.0000666667 Loss content:  0.3951 Loss fft: 10.2452\n","Time:  0.3516 Epoch: 002 Iter: 23000/39218 LR: 0.0000666667 Loss content:  0.3855 Loss fft:  9.9867\n","Time:  0.3512 Epoch: 002 Iter: 23100/39218 LR: 0.0000666667 Loss content:  0.4051 Loss fft: 10.3236\n","Time:  0.3514 Epoch: 002 Iter: 23200/39218 LR: 0.0000666667 Loss content:  0.3961 Loss fft: 10.1233\n","Time:  0.3514 Epoch: 002 Iter: 23300/39218 LR: 0.0000666667 Loss content:  0.3939 Loss fft:  9.9303\n","Time:  0.3514 Epoch: 002 Iter: 23400/39218 LR: 0.0000666667 Loss content:  0.3955 Loss fft: 10.2551\n","Time:  0.3515 Epoch: 002 Iter: 23500/39218 LR: 0.0000666667 Loss content:  0.3937 Loss fft: 10.3229\n","Time:  0.3513 Epoch: 002 Iter: 23600/39218 LR: 0.0000666667 Loss content:  0.3922 Loss fft: 10.0938\n","Time:  0.3513 Epoch: 002 Iter: 23700/39218 LR: 0.0000666667 Loss content:  0.3963 Loss fft: 10.1230\n","Time:  0.3515 Epoch: 002 Iter: 23800/39218 LR: 0.0000666667 Loss content:  0.3957 Loss fft: 10.3585\n","Time:  0.3513 Epoch: 002 Iter: 23900/39218 LR: 0.0000666667 Loss content:  0.3950 Loss fft: 10.1596\n","Time:  0.3515 Epoch: 002 Iter: 24000/39218 LR: 0.0000666667 Loss content:  0.3999 Loss fft: 10.2514\n","Time:  0.3514 Epoch: 002 Iter: 24100/39218 LR: 0.0000666667 Loss content:  0.3940 Loss fft: 10.2588\n","Time:  0.3514 Epoch: 002 Iter: 24200/39218 LR: 0.0000666667 Loss content:  0.3946 Loss fft: 10.0610\n","Time:  0.3515 Epoch: 002 Iter: 24300/39218 LR: 0.0000666667 Loss content:  0.3903 Loss fft: 10.1208\n","Time:  0.3514 Epoch: 002 Iter: 24400/39218 LR: 0.0000666667 Loss content:  0.3922 Loss fft: 10.1661\n","Time:  0.3514 Epoch: 002 Iter: 24500/39218 LR: 0.0000666667 Loss content:  0.3943 Loss fft: 10.3411\n","Time:  0.3514 Epoch: 002 Iter: 24600/39218 LR: 0.0000666667 Loss content:  0.3968 Loss fft: 10.1892\n","Time:  0.3516 Epoch: 002 Iter: 24700/39218 LR: 0.0000666667 Loss content:  0.3972 Loss fft: 10.4526\n","Time:  0.3516 Epoch: 002 Iter: 24800/39218 LR: 0.0000666667 Loss content:  0.3932 Loss fft: 10.2791\n","Time:  0.3514 Epoch: 002 Iter: 24900/39218 LR: 0.0000666667 Loss content:  0.4027 Loss fft: 10.3718\n","Time:  0.3513 Epoch: 002 Iter: 25000/39218 LR: 0.0000666667 Loss content:  0.3986 Loss fft: 10.2310\n","Time:  0.3513 Epoch: 002 Iter: 25100/39218 LR: 0.0000666667 Loss content:  0.4036 Loss fft: 10.2173\n","Time:  0.3550 Epoch: 002 Iter: 25200/39218 LR: 0.0000666667 Loss content:  0.3998 Loss fft: 10.2508\n","Time:  0.3514 Epoch: 002 Iter: 25300/39218 LR: 0.0000666667 Loss content:  0.3961 Loss fft: 10.3236\n","Time:  0.3514 Epoch: 002 Iter: 25400/39218 LR: 0.0000666667 Loss content:  0.3966 Loss fft: 10.0541\n","Time:  0.3514 Epoch: 002 Iter: 25500/39218 LR: 0.0000666667 Loss content:  0.3914 Loss fft: 10.2981\n","Time:  0.3515 Epoch: 002 Iter: 25600/39218 LR: 0.0000666667 Loss content:  0.4003 Loss fft: 10.3362\n","Time:  0.3515 Epoch: 002 Iter: 25700/39218 LR: 0.0000666667 Loss content:  0.3976 Loss fft: 10.2430\n","Time:  0.3515 Epoch: 002 Iter: 25800/39218 LR: 0.0000666667 Loss content:  0.3914 Loss fft: 10.2162\n","Time:  0.3515 Epoch: 002 Iter: 25900/39218 LR: 0.0000666667 Loss content:  0.3877 Loss fft: 10.1841\n","Time:  0.3514 Epoch: 002 Iter: 26000/39218 LR: 0.0000666667 Loss content:  0.3945 Loss fft: 10.2297\n","Time:  0.3514 Epoch: 002 Iter: 26100/39218 LR: 0.0000666667 Loss content:  0.3878 Loss fft: 10.0939\n","Time:  0.3514 Epoch: 002 Iter: 26200/39218 LR: 0.0000666667 Loss content:  0.4070 Loss fft: 10.4053\n","Time:  0.3514 Epoch: 002 Iter: 26300/39218 LR: 0.0000666667 Loss content:  0.3977 Loss fft: 10.4474\n","Time:  0.3513 Epoch: 002 Iter: 26400/39218 LR: 0.0000666667 Loss content:  0.3925 Loss fft: 10.2257\n","Time:  0.3514 Epoch: 002 Iter: 26500/39218 LR: 0.0000666667 Loss content:  0.3985 Loss fft: 10.3477\n","Time:  0.3515 Epoch: 002 Iter: 26600/39218 LR: 0.0000666667 Loss content:  0.4024 Loss fft: 10.4644\n","Time:  0.3517 Epoch: 002 Iter: 26700/39218 LR: 0.0000666667 Loss content:  0.3932 Loss fft: 10.1801\n","Time:  0.3512 Epoch: 002 Iter: 26800/39218 LR: 0.0000666667 Loss content:  0.3946 Loss fft: 10.2989\n","Time:  0.3513 Epoch: 002 Iter: 26900/39218 LR: 0.0000666667 Loss content:  0.3925 Loss fft: 10.2459\n","Time:  0.3513 Epoch: 002 Iter: 27000/39218 LR: 0.0000666667 Loss content:  0.3811 Loss fft: 10.0093\n","Time:  0.3515 Epoch: 002 Iter: 27100/39218 LR: 0.0000666667 Loss content:  0.3961 Loss fft: 10.1922\n","Time:  0.3515 Epoch: 002 Iter: 27200/39218 LR: 0.0000666667 Loss content:  0.3940 Loss fft: 10.2334\n","Time:  0.3514 Epoch: 002 Iter: 27300/39218 LR: 0.0000666667 Loss content:  0.3921 Loss fft: 10.2595\n","Time:  0.3515 Epoch: 002 Iter: 27400/39218 LR: 0.0000666667 Loss content:  0.3951 Loss fft: 10.2425\n","Time:  0.3547 Epoch: 002 Iter: 27500/39218 LR: 0.0000666667 Loss content:  0.3957 Loss fft: 10.1946\n","Time:  0.3514 Epoch: 002 Iter: 27600/39218 LR: 0.0000666667 Loss content:  0.3957 Loss fft: 10.1943\n","Time:  0.3515 Epoch: 002 Iter: 27700/39218 LR: 0.0000666667 Loss content:  0.3993 Loss fft: 10.3569\n","Time:  0.3515 Epoch: 002 Iter: 27800/39218 LR: 0.0000666667 Loss content:  0.3920 Loss fft: 10.2826\n","Time:  0.3516 Epoch: 002 Iter: 27900/39218 LR: 0.0000666667 Loss content:  0.4024 Loss fft: 10.4687\n","Time:  0.3515 Epoch: 002 Iter: 28000/39218 LR: 0.0000666667 Loss content:  0.3952 Loss fft: 10.1923\n","Time:  0.3514 Epoch: 002 Iter: 28100/39218 LR: 0.0000666667 Loss content:  0.3836 Loss fft: 10.1014\n","Time:  0.3514 Epoch: 002 Iter: 28200/39218 LR: 0.0000666667 Loss content:  0.3955 Loss fft: 10.2017\n","Time:  0.3513 Epoch: 002 Iter: 28300/39218 LR: 0.0000666667 Loss content:  0.3924 Loss fft: 10.2445\n","Time:  0.3513 Epoch: 002 Iter: 28400/39218 LR: 0.0000666667 Loss content:  0.3933 Loss fft: 10.2715\n","Time:  0.3513 Epoch: 002 Iter: 28500/39218 LR: 0.0000666667 Loss content:  0.4019 Loss fft: 10.2315\n","Time:  0.3514 Epoch: 002 Iter: 28600/39218 LR: 0.0000666667 Loss content:  0.3962 Loss fft: 10.2273\n","Time:  0.3514 Epoch: 002 Iter: 28700/39218 LR: 0.0000666667 Loss content:  0.3943 Loss fft: 10.2468\n","Time:  0.3513 Epoch: 002 Iter: 28800/39218 LR: 0.0000666667 Loss content:  0.3931 Loss fft: 10.1330\n","Time:  0.3514 Epoch: 002 Iter: 28900/39218 LR: 0.0000666667 Loss content:  0.3926 Loss fft: 10.2925\n","Time:  0.3516 Epoch: 002 Iter: 29000/39218 LR: 0.0000666667 Loss content:  0.3955 Loss fft: 10.2916\n","Time:  0.3515 Epoch: 002 Iter: 29100/39218 LR: 0.0000666667 Loss content:  0.3984 Loss fft: 10.1338\n","Time:  0.3515 Epoch: 002 Iter: 29200/39218 LR: 0.0000666667 Loss content:  0.3920 Loss fft: 10.2057\n","Time:  0.3515 Epoch: 002 Iter: 29300/39218 LR: 0.0000666667 Loss content:  0.3938 Loss fft: 10.1859\n","Time:  0.3512 Epoch: 002 Iter: 29400/39218 LR: 0.0000666667 Loss content:  0.3973 Loss fft: 10.2517\n","Time:  0.3516 Epoch: 002 Iter: 29500/39218 LR: 0.0000666667 Loss content:  0.3852 Loss fft: 10.1208\n","Time:  0.3514 Epoch: 002 Iter: 29600/39218 LR: 0.0000666667 Loss content:  0.3922 Loss fft: 10.3978\n","Time:  0.3546 Epoch: 002 Iter: 29700/39218 LR: 0.0000666667 Loss content:  0.3895 Loss fft: 10.0468\n","Time:  0.3514 Epoch: 002 Iter: 29800/39218 LR: 0.0000666667 Loss content:  0.3993 Loss fft: 10.0376\n","Time:  0.3515 Epoch: 002 Iter: 29900/39218 LR: 0.0000666667 Loss content:  0.3975 Loss fft: 10.2529\n","Time:  0.3516 Epoch: 002 Iter: 30000/39218 LR: 0.0000666667 Loss content:  0.4006 Loss fft: 10.4882\n","Time:  0.3514 Epoch: 002 Iter: 30100/39218 LR: 0.0000666667 Loss content:  0.4008 Loss fft: 10.2564\n","Time:  0.3515 Epoch: 002 Iter: 30200/39218 LR: 0.0000666667 Loss content:  0.3964 Loss fft: 10.3598\n","Time:  0.3516 Epoch: 002 Iter: 30300/39218 LR: 0.0000666667 Loss content:  0.3932 Loss fft: 10.2414\n","Time:  0.3515 Epoch: 002 Iter: 30400/39218 LR: 0.0000666667 Loss content:  0.3887 Loss fft: 10.1346\n","Time:  0.3514 Epoch: 002 Iter: 30500/39218 LR: 0.0000666667 Loss content:  0.3931 Loss fft: 10.1268\n","Time:  0.3513 Epoch: 002 Iter: 30600/39218 LR: 0.0000666667 Loss content:  0.3977 Loss fft: 10.3946\n","Time:  0.3516 Epoch: 002 Iter: 30700/39218 LR: 0.0000666667 Loss content:  0.3845 Loss fft: 10.1665\n","Time:  0.3514 Epoch: 002 Iter: 30800/39218 LR: 0.0000666667 Loss content:  0.3970 Loss fft: 10.2967\n","Time:  0.3514 Epoch: 002 Iter: 30900/39218 LR: 0.0000666667 Loss content:  0.3943 Loss fft: 10.2601\n","Time:  0.3516 Epoch: 002 Iter: 31000/39218 LR: 0.0000666667 Loss content:  0.3882 Loss fft: 10.0546\n","Time:  0.3515 Epoch: 002 Iter: 31100/39218 LR: 0.0000666667 Loss content:  0.3958 Loss fft: 10.3143\n","Time:  0.3514 Epoch: 002 Iter: 31200/39218 LR: 0.0000666667 Loss content:  0.3832 Loss fft: 10.1180\n","Time:  0.3515 Epoch: 002 Iter: 31300/39218 LR: 0.0000666667 Loss content:  0.3910 Loss fft: 10.1883\n","Time:  0.3516 Epoch: 002 Iter: 31400/39218 LR: 0.0000666667 Loss content:  0.3889 Loss fft: 10.2974\n","Time:  0.3514 Epoch: 002 Iter: 31500/39218 LR: 0.0000666667 Loss content:  0.3938 Loss fft: 10.1434\n","Time:  0.3515 Epoch: 002 Iter: 31600/39218 LR: 0.0000666667 Loss content:  0.3937 Loss fft: 10.1814\n","Time:  0.3513 Epoch: 002 Iter: 31700/39218 LR: 0.0000666667 Loss content:  0.3969 Loss fft: 10.4251\n","Time:  0.3514 Epoch: 002 Iter: 31800/39218 LR: 0.0000666667 Loss content:  0.3927 Loss fft: 10.0805\n","Time:  0.3515 Epoch: 002 Iter: 31900/39218 LR: 0.0000666667 Loss content:  0.3862 Loss fft: 10.1467\n","Time:  0.3545 Epoch: 002 Iter: 32000/39218 LR: 0.0000666667 Loss content:  0.3961 Loss fft: 10.4117\n","Time:  0.3516 Epoch: 002 Iter: 32100/39218 LR: 0.0000666667 Loss content:  0.3954 Loss fft: 10.1848\n","Time:  0.3513 Epoch: 002 Iter: 32200/39218 LR: 0.0000666667 Loss content:  0.3945 Loss fft: 10.2717\n","Time:  0.3515 Epoch: 002 Iter: 32300/39218 LR: 0.0000666667 Loss content:  0.3905 Loss fft: 10.2255\n","Time:  0.3516 Epoch: 002 Iter: 32400/39218 LR: 0.0000666667 Loss content:  0.3961 Loss fft: 10.3132\n","Time:  0.3514 Epoch: 002 Iter: 32500/39218 LR: 0.0000666667 Loss content:  0.3964 Loss fft: 10.1701\n","Time:  0.3514 Epoch: 002 Iter: 32600/39218 LR: 0.0000666667 Loss content:  0.3967 Loss fft: 10.1829\n","Time:  0.3514 Epoch: 002 Iter: 32700/39218 LR: 0.0000666667 Loss content:  0.3868 Loss fft: 10.1455\n","Time:  0.3514 Epoch: 002 Iter: 32800/39218 LR: 0.0000666667 Loss content:  0.3920 Loss fft: 10.3288\n","Time:  0.3515 Epoch: 002 Iter: 32900/39218 LR: 0.0000666667 Loss content:  0.3934 Loss fft: 10.1640\n","Time:  0.3513 Epoch: 002 Iter: 33000/39218 LR: 0.0000666667 Loss content:  0.3897 Loss fft: 10.0643\n","Time:  0.3514 Epoch: 002 Iter: 33100/39218 LR: 0.0000666667 Loss content:  0.4023 Loss fft: 10.3721\n","Time:  0.3514 Epoch: 002 Iter: 33200/39218 LR: 0.0000666667 Loss content:  0.3908 Loss fft: 10.3024\n","Time:  0.3512 Epoch: 002 Iter: 33300/39218 LR: 0.0000666667 Loss content:  0.3890 Loss fft: 10.1662\n","Time:  0.3513 Epoch: 002 Iter: 33400/39218 LR: 0.0000666667 Loss content:  0.3911 Loss fft: 10.2283\n","Time:  0.3516 Epoch: 002 Iter: 33500/39218 LR: 0.0000666667 Loss content:  0.3948 Loss fft: 10.0170\n","Time:  0.3514 Epoch: 002 Iter: 33600/39218 LR: 0.0000666667 Loss content:  0.4035 Loss fft: 10.2960\n","Time:  0.3516 Epoch: 002 Iter: 33700/39218 LR: 0.0000666667 Loss content:  0.3792 Loss fft: 10.1311\n","Time:  0.3515 Epoch: 002 Iter: 33800/39218 LR: 0.0000666667 Loss content:  0.3899 Loss fft: 10.1688\n","Time:  0.3517 Epoch: 002 Iter: 33900/39218 LR: 0.0000666667 Loss content:  0.3927 Loss fft: 10.2328\n","Time:  0.3515 Epoch: 002 Iter: 34000/39218 LR: 0.0000666667 Loss content:  0.4074 Loss fft: 10.3444\n","Time:  0.3514 Epoch: 002 Iter: 34100/39218 LR: 0.0000666667 Loss content:  0.3919 Loss fft: 10.0675\n","Time:  0.3515 Epoch: 002 Iter: 34200/39218 LR: 0.0000666667 Loss content:  0.3930 Loss fft: 10.3283\n","Time:  0.3544 Epoch: 002 Iter: 34300/39218 LR: 0.0000666667 Loss content:  0.3944 Loss fft: 10.1490\n","Time:  0.3514 Epoch: 002 Iter: 34400/39218 LR: 0.0000666667 Loss content:  0.3950 Loss fft: 10.2511\n","Time:  0.3513 Epoch: 002 Iter: 34500/39218 LR: 0.0000666667 Loss content:  0.4012 Loss fft: 10.2866\n","Time:  0.3514 Epoch: 002 Iter: 34600/39218 LR: 0.0000666667 Loss content:  0.3903 Loss fft: 10.1276\n","Time:  0.3515 Epoch: 002 Iter: 34700/39218 LR: 0.0000666667 Loss content:  0.3881 Loss fft:  9.9878\n","Time:  0.3514 Epoch: 002 Iter: 34800/39218 LR: 0.0000666667 Loss content:  0.3936 Loss fft: 10.2070\n","Time:  0.3514 Epoch: 002 Iter: 34900/39218 LR: 0.0000666667 Loss content:  0.3960 Loss fft: 10.1659\n","Time:  0.3514 Epoch: 002 Iter: 35000/39218 LR: 0.0000666667 Loss content:  0.4038 Loss fft: 10.3273\n","Time:  0.3518 Epoch: 002 Iter: 35100/39218 LR: 0.0000666667 Loss content:  0.3893 Loss fft: 10.2762\n","Time:  0.3514 Epoch: 002 Iter: 35200/39218 LR: 0.0000666667 Loss content:  0.3897 Loss fft:  9.9579\n","Time:  0.3514 Epoch: 002 Iter: 35300/39218 LR: 0.0000666667 Loss content:  0.3985 Loss fft: 10.1674\n","Time:  0.3513 Epoch: 002 Iter: 35400/39218 LR: 0.0000666667 Loss content:  0.3851 Loss fft: 10.2172\n","Time:  0.3514 Epoch: 002 Iter: 35500/39218 LR: 0.0000666667 Loss content:  0.3841 Loss fft:  9.9718\n","Time:  0.3515 Epoch: 002 Iter: 35600/39218 LR: 0.0000666667 Loss content:  0.3916 Loss fft: 10.1510\n","Time:  0.3514 Epoch: 002 Iter: 35700/39218 LR: 0.0000666667 Loss content:  0.3954 Loss fft:  9.9106\n","Time:  0.3514 Epoch: 002 Iter: 35800/39218 LR: 0.0000666667 Loss content:  0.3879 Loss fft: 10.0966\n","Time:  0.3517 Epoch: 002 Iter: 35900/39218 LR: 0.0000666667 Loss content:  0.3873 Loss fft: 10.1627\n","Time:  0.3515 Epoch: 002 Iter: 36000/39218 LR: 0.0000666667 Loss content:  0.3937 Loss fft: 10.0791\n","Time:  0.3513 Epoch: 002 Iter: 36100/39218 LR: 0.0000666667 Loss content:  0.3857 Loss fft: 10.2671\n","Time:  0.3515 Epoch: 002 Iter: 36200/39218 LR: 0.0000666667 Loss content:  0.3901 Loss fft: 10.1459\n","Time:  0.3516 Epoch: 002 Iter: 36300/39218 LR: 0.0000666667 Loss content:  0.3893 Loss fft: 10.0513\n","Time:  0.3514 Epoch: 002 Iter: 36400/39218 LR: 0.0000666667 Loss content:  0.3952 Loss fft: 10.2079\n","Time:  0.3513 Epoch: 002 Iter: 36500/39218 LR: 0.0000666667 Loss content:  0.3837 Loss fft: 10.0915\n","Time:  0.3548 Epoch: 002 Iter: 36600/39218 LR: 0.0000666667 Loss content:  0.3861 Loss fft: 10.1890\n","Time:  0.3515 Epoch: 002 Iter: 36700/39218 LR: 0.0000666667 Loss content:  0.3922 Loss fft: 10.1313\n","Time:  0.3514 Epoch: 002 Iter: 36800/39218 LR: 0.0000666667 Loss content:  0.3941 Loss fft: 10.3535\n","Time:  0.3515 Epoch: 002 Iter: 36900/39218 LR: 0.0000666667 Loss content:  0.3910 Loss fft: 10.2454\n","Time:  0.3515 Epoch: 002 Iter: 37000/39218 LR: 0.0000666667 Loss content:  0.3938 Loss fft: 10.2423\n","Time:  0.3516 Epoch: 002 Iter: 37100/39218 LR: 0.0000666667 Loss content:  0.3894 Loss fft: 10.0385\n","Time:  0.3515 Epoch: 002 Iter: 37200/39218 LR: 0.0000666667 Loss content:  0.3934 Loss fft: 10.2512\n","Time:  0.3514 Epoch: 002 Iter: 37300/39218 LR: 0.0000666667 Loss content:  0.3903 Loss fft: 10.1198\n","Time:  0.3513 Epoch: 002 Iter: 37400/39218 LR: 0.0000666667 Loss content:  0.3809 Loss fft: 10.1032\n","Time:  0.3515 Epoch: 002 Iter: 37500/39218 LR: 0.0000666667 Loss content:  0.3841 Loss fft: 10.1655\n","Time:  0.3515 Epoch: 002 Iter: 37600/39218 LR: 0.0000666667 Loss content:  0.3887 Loss fft: 10.2534\n","Time:  0.3515 Epoch: 002 Iter: 37700/39218 LR: 0.0000666667 Loss content:  0.3851 Loss fft:  9.9163\n","Time:  0.3515 Epoch: 002 Iter: 37800/39218 LR: 0.0000666667 Loss content:  0.3925 Loss fft: 10.2272\n","Time:  0.3515 Epoch: 002 Iter: 37900/39218 LR: 0.0000666667 Loss content:  0.3910 Loss fft:  9.9132\n","Time:  0.3512 Epoch: 002 Iter: 38000/39218 LR: 0.0000666667 Loss content:  0.3866 Loss fft: 10.2468\n","Time:  0.3515 Epoch: 002 Iter: 38100/39218 LR: 0.0000666667 Loss content:  0.3783 Loss fft: 10.1792\n","Time:  0.3514 Epoch: 002 Iter: 38200/39218 LR: 0.0000666667 Loss content:  0.3891 Loss fft: 10.1545\n","Time:  0.3514 Epoch: 002 Iter: 38300/39218 LR: 0.0000666667 Loss content:  0.3948 Loss fft: 10.1201\n","Time:  0.3516 Epoch: 002 Iter: 38400/39218 LR: 0.0000666667 Loss content:  0.3974 Loss fft: 10.1879\n","Time:  0.3514 Epoch: 002 Iter: 38500/39218 LR: 0.0000666667 Loss content:  0.3894 Loss fft: 10.1110\n","Time:  0.3515 Epoch: 002 Iter: 38600/39218 LR: 0.0000666667 Loss content:  0.3902 Loss fft: 10.1332\n","Time:  0.3515 Epoch: 002 Iter: 38700/39218 LR: 0.0000666667 Loss content:  0.3923 Loss fft: 10.3297\n","Time:  0.3545 Epoch: 002 Iter: 38800/39218 LR: 0.0000666667 Loss content:  0.3892 Loss fft: 10.2847\n","Time:  0.3513 Epoch: 002 Iter: 38900/39218 LR: 0.0000666667 Loss content:  0.3905 Loss fft: 10.1467\n","Time:  0.3513 Epoch: 002 Iter: 39000/39218 LR: 0.0000666667 Loss content:  0.3936 Loss fft: 10.1160\n","Time:  0.3515 Epoch: 002 Iter: 39100/39218 LR: 0.0000666667 Loss content:  0.3967 Loss fft: 10.0087\n","Time:  0.3514 Epoch: 002 Iter: 39200/39218 LR: 0.0000666667 Loss content:  0.3876 Loss fft: 10.0944\n","EPOCH: 02\n","Elapsed time: 137.98 Epoch Pixel Loss:  0.3973 Epoch FFT Loss: 10.2438\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Time:  0.3654 Epoch: 003 Iter:  100/39218 LR: 0.0001000000 Loss content:  0.3853 Loss fft: 10.1935\n","Time:  0.3512 Epoch: 003 Iter:  200/39218 LR: 0.0001000000 Loss content:  0.3842 Loss fft: 10.0196\n","Time:  0.3514 Epoch: 003 Iter:  300/39218 LR: 0.0001000000 Loss content:  0.3941 Loss fft: 10.1490\n","Time:  0.3514 Epoch: 003 Iter:  400/39218 LR: 0.0001000000 Loss content:  0.3905 Loss fft: 10.1467\n","Time:  0.3514 Epoch: 003 Iter:  500/39218 LR: 0.0001000000 Loss content:  0.3905 Loss fft: 10.1947\n","Time:  0.3514 Epoch: 003 Iter:  600/39218 LR: 0.0001000000 Loss content:  0.3933 Loss fft: 10.1845\n","Time:  0.3515 Epoch: 003 Iter:  700/39218 LR: 0.0001000000 Loss content:  0.3893 Loss fft: 10.0955\n","Time:  0.3514 Epoch: 003 Iter:  800/39218 LR: 0.0001000000 Loss content:  0.3976 Loss fft: 10.1812\n","Time:  0.3513 Epoch: 003 Iter:  900/39218 LR: 0.0001000000 Loss content:  0.3880 Loss fft: 10.3318\n","Time:  0.3513 Epoch: 003 Iter: 1000/39218 LR: 0.0001000000 Loss content:  0.3889 Loss fft: 10.2191\n","Time:  0.3514 Epoch: 003 Iter: 1100/39218 LR: 0.0001000000 Loss content:  0.3901 Loss fft: 10.2818\n","Time:  0.3514 Epoch: 003 Iter: 1200/39218 LR: 0.0001000000 Loss content:  0.3986 Loss fft: 10.3857\n","Time:  0.3514 Epoch: 003 Iter: 1300/39218 LR: 0.0001000000 Loss content:  0.3916 Loss fft: 10.1364\n","Time:  0.3515 Epoch: 003 Iter: 1400/39218 LR: 0.0001000000 Loss content:  0.3955 Loss fft: 10.2109\n","Time:  0.3515 Epoch: 003 Iter: 1500/39218 LR: 0.0001000000 Loss content:  0.3899 Loss fft: 10.2612\n","Time:  0.3514 Epoch: 003 Iter: 1600/39218 LR: 0.0001000000 Loss content:  0.3915 Loss fft: 10.0777\n","Time:  0.3516 Epoch: 003 Iter: 1700/39218 LR: 0.0001000000 Loss content:  0.3858 Loss fft: 10.0489\n","Time:  0.3564 Epoch: 003 Iter: 1800/39218 LR: 0.0001000000 Loss content:  0.3888 Loss fft: 10.1385\n","Time:  0.3514 Epoch: 003 Iter: 1900/39218 LR: 0.0001000000 Loss content:  0.3850 Loss fft: 10.1129\n","Time:  0.3515 Epoch: 003 Iter: 2000/39218 LR: 0.0001000000 Loss content:  0.3950 Loss fft: 10.2488\n","Time:  0.3513 Epoch: 003 Iter: 2100/39218 LR: 0.0001000000 Loss content:  0.3956 Loss fft: 10.2439\n","Time:  0.3514 Epoch: 003 Iter: 2200/39218 LR: 0.0001000000 Loss content:  0.3866 Loss fft: 10.0253\n","Time:  0.3513 Epoch: 003 Iter: 2300/39218 LR: 0.0001000000 Loss content:  0.3803 Loss fft: 10.0931\n","Time:  0.3515 Epoch: 003 Iter: 2400/39218 LR: 0.0001000000 Loss content:  0.3934 Loss fft: 10.3031\n","Time:  0.3514 Epoch: 003 Iter: 2500/39218 LR: 0.0001000000 Loss content:  0.3960 Loss fft: 10.1922\n","Time:  0.3514 Epoch: 003 Iter: 2600/39218 LR: 0.0001000000 Loss content:  0.3898 Loss fft: 10.3336\n","Time:  0.3516 Epoch: 003 Iter: 2700/39218 LR: 0.0001000000 Loss content:  0.3816 Loss fft: 10.0328\n","Time:  0.3516 Epoch: 003 Iter: 2800/39218 LR: 0.0001000000 Loss content:  0.3834 Loss fft: 10.1313\n","Time:  0.3516 Epoch: 003 Iter: 2900/39218 LR: 0.0001000000 Loss content:  0.3922 Loss fft: 10.1438\n","Time:  0.3517 Epoch: 003 Iter: 3000/39218 LR: 0.0001000000 Loss content:  0.3963 Loss fft: 10.1746\n","Time:  0.3515 Epoch: 003 Iter: 3100/39218 LR: 0.0001000000 Loss content:  0.3892 Loss fft: 10.0700\n","Time:  0.3515 Epoch: 003 Iter: 3200/39218 LR: 0.0001000000 Loss content:  0.3920 Loss fft: 10.2329\n","Time:  0.3514 Epoch: 003 Iter: 3300/39218 LR: 0.0001000000 Loss content:  0.3902 Loss fft: 10.3225\n","Time:  0.3513 Epoch: 003 Iter: 3400/39218 LR: 0.0001000000 Loss content:  0.3871 Loss fft: 10.2567\n","Time:  0.3515 Epoch: 003 Iter: 3500/39218 LR: 0.0001000000 Loss content:  0.3892 Loss fft: 10.2656\n","Time:  0.3514 Epoch: 003 Iter: 3600/39218 LR: 0.0001000000 Loss content:  0.3880 Loss fft: 10.1752\n","Time:  0.3516 Epoch: 003 Iter: 3700/39218 LR: 0.0001000000 Loss content:  0.3945 Loss fft: 10.2763\n","Time:  0.3514 Epoch: 003 Iter: 3800/39218 LR: 0.0001000000 Loss content:  0.3848 Loss fft: 10.2678\n","Time:  0.3514 Epoch: 003 Iter: 3900/39218 LR: 0.0001000000 Loss content:  0.3950 Loss fft: 10.1263\n","Time:  0.3515 Epoch: 003 Iter: 4000/39218 LR: 0.0001000000 Loss content:  0.3856 Loss fft: 10.1428\n","Time:  0.3546 Epoch: 003 Iter: 4100/39218 LR: 0.0001000000 Loss content:  0.3895 Loss fft: 10.3456\n","Time:  0.3515 Epoch: 003 Iter: 4200/39218 LR: 0.0001000000 Loss content:  0.3965 Loss fft: 10.3811\n","Time:  0.3516 Epoch: 003 Iter: 4300/39218 LR: 0.0001000000 Loss content:  0.3941 Loss fft: 10.2093\n","Time:  0.3515 Epoch: 003 Iter: 4400/39218 LR: 0.0001000000 Loss content:  0.3912 Loss fft: 10.2425\n","Time:  0.3519 Epoch: 003 Iter: 4500/39218 LR: 0.0001000000 Loss content:  0.3918 Loss fft: 10.1811\n","Time:  0.3517 Epoch: 003 Iter: 4600/39218 LR: 0.0001000000 Loss content:  0.3907 Loss fft: 10.4716\n","Time:  0.3516 Epoch: 003 Iter: 4700/39218 LR: 0.0001000000 Loss content:  0.3893 Loss fft:  9.9219\n","Time:  0.3517 Epoch: 003 Iter: 4800/39218 LR: 0.0001000000 Loss content:  0.3870 Loss fft: 10.1750\n","Time:  0.3516 Epoch: 003 Iter: 4900/39218 LR: 0.0001000000 Loss content:  0.3895 Loss fft:  9.9433\n","Time:  0.3516 Epoch: 003 Iter: 5000/39218 LR: 0.0001000000 Loss content:  0.3848 Loss fft: 10.1090\n","Time:  0.3518 Epoch: 003 Iter: 5100/39218 LR: 0.0001000000 Loss content:  0.3883 Loss fft:  9.9100\n","Time:  0.3517 Epoch: 003 Iter: 5200/39218 LR: 0.0001000000 Loss content:  0.3950 Loss fft: 10.1711\n","Time:  0.3518 Epoch: 003 Iter: 5300/39218 LR: 0.0001000000 Loss content:  0.3845 Loss fft: 10.0609\n","Time:  0.3518 Epoch: 003 Iter: 5400/39218 LR: 0.0001000000 Loss content:  0.3849 Loss fft: 10.1341\n","Time:  0.3518 Epoch: 003 Iter: 5500/39218 LR: 0.0001000000 Loss content:  0.3799 Loss fft: 10.2227\n","Time:  0.3519 Epoch: 003 Iter: 5600/39218 LR: 0.0001000000 Loss content:  0.3911 Loss fft: 10.2300\n","Time:  0.3517 Epoch: 003 Iter: 5700/39218 LR: 0.0001000000 Loss content:  0.3882 Loss fft: 10.0839\n","Time:  0.3519 Epoch: 003 Iter: 5800/39218 LR: 0.0001000000 Loss content:  0.3972 Loss fft: 10.2642\n","Time:  0.3518 Epoch: 003 Iter: 5900/39218 LR: 0.0001000000 Loss content:  0.3869 Loss fft: 10.1780\n","Time:  0.3518 Epoch: 003 Iter: 6000/39218 LR: 0.0001000000 Loss content:  0.3881 Loss fft: 10.1920\n","Time:  0.3518 Epoch: 003 Iter: 6100/39218 LR: 0.0001000000 Loss content:  0.3878 Loss fft: 10.2386\n","Time:  0.3517 Epoch: 003 Iter: 6200/39218 LR: 0.0001000000 Loss content:  0.3842 Loss fft: 10.1009\n","Time:  0.3517 Epoch: 003 Iter: 6300/39218 LR: 0.0001000000 Loss content:  0.3840 Loss fft: 10.0082\n","Time:  0.3550 Epoch: 003 Iter: 6400/39218 LR: 0.0001000000 Loss content:  0.3942 Loss fft: 10.2284\n","Time:  0.3517 Epoch: 003 Iter: 6500/39218 LR: 0.0001000000 Loss content:  0.3901 Loss fft: 10.3016\n","Time:  0.3516 Epoch: 003 Iter: 6600/39218 LR: 0.0001000000 Loss content:  0.3928 Loss fft: 10.2230\n","Time:  0.3515 Epoch: 003 Iter: 6700/39218 LR: 0.0001000000 Loss content:  0.3950 Loss fft: 10.2905\n","Time:  0.3520 Epoch: 003 Iter: 6800/39218 LR: 0.0001000000 Loss content:  0.3854 Loss fft: 10.3210\n","Time:  0.3516 Epoch: 003 Iter: 6900/39218 LR: 0.0001000000 Loss content:  0.3923 Loss fft: 10.2115\n","Time:  0.3515 Epoch: 003 Iter: 7000/39218 LR: 0.0001000000 Loss content:  0.3902 Loss fft: 10.0635\n","Time:  0.3515 Epoch: 003 Iter: 7100/39218 LR: 0.0001000000 Loss content:  0.3845 Loss fft: 10.0919\n","Time:  0.3518 Epoch: 003 Iter: 7200/39218 LR: 0.0001000000 Loss content:  0.3932 Loss fft: 10.3479\n","Time:  0.3517 Epoch: 003 Iter: 7300/39218 LR: 0.0001000000 Loss content:  0.3800 Loss fft: 10.0151\n","Time:  0.3516 Epoch: 003 Iter: 7400/39218 LR: 0.0001000000 Loss content:  0.3850 Loss fft: 10.1205\n","Time:  0.3518 Epoch: 003 Iter: 7500/39218 LR: 0.0001000000 Loss content:  0.3886 Loss fft: 10.0897\n","Time:  0.3517 Epoch: 003 Iter: 7600/39218 LR: 0.0001000000 Loss content:  0.3915 Loss fft: 10.2562\n","Time:  0.3515 Epoch: 003 Iter: 7700/39218 LR: 0.0001000000 Loss content:  0.3864 Loss fft: 10.2321\n","Time:  0.3517 Epoch: 003 Iter: 7800/39218 LR: 0.0001000000 Loss content:  0.3864 Loss fft: 10.1452\n","Time:  0.3515 Epoch: 003 Iter: 7900/39218 LR: 0.0001000000 Loss content:  0.3783 Loss fft: 10.0368\n","Time:  0.3515 Epoch: 003 Iter: 8000/39218 LR: 0.0001000000 Loss content:  0.3845 Loss fft:  9.9653\n","Time:  0.3517 Epoch: 003 Iter: 8100/39218 LR: 0.0001000000 Loss content:  0.3775 Loss fft: 10.0198\n","Time:  0.3518 Epoch: 003 Iter: 8200/39218 LR: 0.0001000000 Loss content:  0.4006 Loss fft: 10.4028\n","Time:  0.3520 Epoch: 003 Iter: 8300/39218 LR: 0.0001000000 Loss content:  0.3855 Loss fft: 10.0976\n","Time:  0.3516 Epoch: 003 Iter: 8400/39218 LR: 0.0001000000 Loss content:  0.3864 Loss fft: 10.2043\n","Time:  0.3518 Epoch: 003 Iter: 8500/39218 LR: 0.0001000000 Loss content:  0.3954 Loss fft: 10.3440\n","Time:  0.3516 Epoch: 003 Iter: 8600/39218 LR: 0.0001000000 Loss content:  0.3823 Loss fft: 10.1146\n","Time:  0.3552 Epoch: 003 Iter: 8700/39218 LR: 0.0001000000 Loss content:  0.3931 Loss fft: 10.1155\n","Time:  0.3515 Epoch: 003 Iter: 8800/39218 LR: 0.0001000000 Loss content:  0.3965 Loss fft: 10.3216\n","Time:  0.3515 Epoch: 003 Iter: 8900/39218 LR: 0.0001000000 Loss content:  0.3869 Loss fft: 10.2472\n","Time:  0.3514 Epoch: 003 Iter: 9000/39218 LR: 0.0001000000 Loss content:  0.3889 Loss fft:  9.9595\n","Time:  0.3516 Epoch: 003 Iter: 9100/39218 LR: 0.0001000000 Loss content:  0.3837 Loss fft: 10.1077\n","Time:  0.3515 Epoch: 003 Iter: 9200/39218 LR: 0.0001000000 Loss content:  0.3924 Loss fft: 10.1776\n","Time:  0.3515 Epoch: 003 Iter: 9300/39218 LR: 0.0001000000 Loss content:  0.3879 Loss fft: 10.0868\n","Time:  0.3516 Epoch: 003 Iter: 9400/39218 LR: 0.0001000000 Loss content:  0.3842 Loss fft: 10.0403\n","Time:  0.3515 Epoch: 003 Iter: 9500/39218 LR: 0.0001000000 Loss content:  0.3816 Loss fft: 10.1023\n","Time:  0.3516 Epoch: 003 Iter: 9600/39218 LR: 0.0001000000 Loss content:  0.3808 Loss fft: 10.1153\n","Time:  0.3521 Epoch: 003 Iter: 9700/39218 LR: 0.0001000000 Loss content:  0.3848 Loss fft: 10.1852\n","Time:  0.3517 Epoch: 003 Iter: 9800/39218 LR: 0.0001000000 Loss content:  0.3852 Loss fft: 10.1311\n","Time:  0.3515 Epoch: 003 Iter: 9900/39218 LR: 0.0001000000 Loss content:  0.3853 Loss fft: 10.2039\n","Time:  0.3516 Epoch: 003 Iter: 10000/39218 LR: 0.0001000000 Loss content:  0.3864 Loss fft: 10.1389\n","Time:  0.3517 Epoch: 003 Iter: 10100/39218 LR: 0.0001000000 Loss content:  0.3873 Loss fft: 10.2137\n","Time:  0.3516 Epoch: 003 Iter: 10200/39218 LR: 0.0001000000 Loss content:  0.3790 Loss fft: 10.2274\n","Time:  0.3514 Epoch: 003 Iter: 10300/39218 LR: 0.0001000000 Loss content:  0.3853 Loss fft: 10.1918\n","Time:  0.3515 Epoch: 003 Iter: 10400/39218 LR: 0.0001000000 Loss content:  0.3931 Loss fft: 10.2471\n","Time:  0.3516 Epoch: 003 Iter: 10500/39218 LR: 0.0001000000 Loss content:  0.3832 Loss fft: 10.1789\n","Time:  0.3515 Epoch: 003 Iter: 10600/39218 LR: 0.0001000000 Loss content:  0.3806 Loss fft:  9.9969\n","Time:  0.3515 Epoch: 003 Iter: 10700/39218 LR: 0.0001000000 Loss content:  0.3818 Loss fft:  9.9673\n","Time:  0.3517 Epoch: 003 Iter: 10800/39218 LR: 0.0001000000 Loss content:  0.3846 Loss fft: 10.1442\n","Time:  0.3514 Epoch: 003 Iter: 10900/39218 LR: 0.0001000000 Loss content:  0.3815 Loss fft: 10.2211\n","Time:  0.3554 Epoch: 003 Iter: 11000/39218 LR: 0.0001000000 Loss content:  0.3842 Loss fft:  9.9793\n","Time:  0.3516 Epoch: 003 Iter: 11100/39218 LR: 0.0001000000 Loss content:  0.3907 Loss fft: 10.0101\n","Time:  0.3515 Epoch: 003 Iter: 11200/39218 LR: 0.0001000000 Loss content:  0.3839 Loss fft: 10.1878\n","Time:  0.3515 Epoch: 003 Iter: 11300/39218 LR: 0.0001000000 Loss content:  0.3797 Loss fft: 10.0125\n","Time:  0.3515 Epoch: 003 Iter: 11400/39218 LR: 0.0001000000 Loss content:  0.3797 Loss fft: 10.0720\n","Time:  0.3516 Epoch: 003 Iter: 11500/39218 LR: 0.0001000000 Loss content:  0.3912 Loss fft: 10.2637\n","Time:  0.3517 Epoch: 003 Iter: 11600/39218 LR: 0.0001000000 Loss content:  0.3937 Loss fft: 10.2671\n","Time:  0.3518 Epoch: 003 Iter: 11700/39218 LR: 0.0001000000 Loss content:  0.3839 Loss fft: 10.0122\n","Time:  0.3518 Epoch: 003 Iter: 11800/39218 LR: 0.0001000000 Loss content:  0.3892 Loss fft: 10.0744\n","Time:  0.3516 Epoch: 003 Iter: 11900/39218 LR: 0.0001000000 Loss content:  0.3854 Loss fft: 10.1568\n","Time:  0.3516 Epoch: 003 Iter: 12000/39218 LR: 0.0001000000 Loss content:  0.3851 Loss fft: 10.2902\n","Time:  0.3517 Epoch: 003 Iter: 12100/39218 LR: 0.0001000000 Loss content:  0.3828 Loss fft: 10.1238\n","Time:  0.3517 Epoch: 003 Iter: 12200/39218 LR: 0.0001000000 Loss content:  0.3926 Loss fft: 10.2274\n","Time:  0.3518 Epoch: 003 Iter: 12300/39218 LR: 0.0001000000 Loss content:  0.3889 Loss fft: 10.1261\n","Time:  0.3516 Epoch: 003 Iter: 12400/39218 LR: 0.0001000000 Loss content:  0.3806 Loss fft: 10.0321\n","Time:  0.3519 Epoch: 003 Iter: 12500/39218 LR: 0.0001000000 Loss content:  0.3778 Loss fft: 10.1015\n","Time:  0.3517 Epoch: 003 Iter: 12600/39218 LR: 0.0001000000 Loss content:  0.3871 Loss fft: 10.2061\n","Time:  0.3515 Epoch: 003 Iter: 12700/39218 LR: 0.0001000000 Loss content:  0.3815 Loss fft: 10.1657\n","Time:  0.3517 Epoch: 003 Iter: 12800/39218 LR: 0.0001000000 Loss content:  0.3907 Loss fft: 10.1765\n","Time:  0.3518 Epoch: 003 Iter: 12900/39218 LR: 0.0001000000 Loss content:  0.3780 Loss fft:  9.9712\n","Time:  0.3514 Epoch: 003 Iter: 13000/39218 LR: 0.0001000000 Loss content:  0.3822 Loss fft: 10.0534\n","Time:  0.3515 Epoch: 003 Iter: 13100/39218 LR: 0.0001000000 Loss content:  0.3722 Loss fft:  9.9953\n","Time:  0.3550 Epoch: 003 Iter: 13200/39218 LR: 0.0001000000 Loss content:  0.3799 Loss fft: 10.1666\n","Time:  0.3517 Epoch: 003 Iter: 13300/39218 LR: 0.0001000000 Loss content:  0.3904 Loss fft: 10.2195\n","Time:  0.3516 Epoch: 003 Iter: 13400/39218 LR: 0.0001000000 Loss content:  0.3987 Loss fft: 10.3094\n","Time:  0.3516 Epoch: 003 Iter: 13500/39218 LR: 0.0001000000 Loss content:  0.3809 Loss fft:  9.9667\n","Time:  0.3515 Epoch: 003 Iter: 13600/39218 LR: 0.0001000000 Loss content:  0.3859 Loss fft: 10.0024\n","Time:  0.3516 Epoch: 003 Iter: 13700/39218 LR: 0.0001000000 Loss content:  0.3814 Loss fft: 10.1110\n","Time:  0.3517 Epoch: 003 Iter: 13800/39218 LR: 0.0001000000 Loss content:  0.3862 Loss fft: 10.1982\n","Time:  0.3516 Epoch: 003 Iter: 13900/39218 LR: 0.0001000000 Loss content:  0.3727 Loss fft: 10.0481\n","Time:  0.3519 Epoch: 003 Iter: 14000/39218 LR: 0.0001000000 Loss content:  0.3749 Loss fft: 10.0185\n","Time:  0.3518 Epoch: 003 Iter: 14100/39218 LR: 0.0001000000 Loss content:  0.3865 Loss fft: 10.3254\n","Time:  0.3519 Epoch: 003 Iter: 14200/39218 LR: 0.0001000000 Loss content:  0.3835 Loss fft: 10.1933\n","Time:  0.3517 Epoch: 003 Iter: 14300/39218 LR: 0.0001000000 Loss content:  0.3873 Loss fft: 10.1779\n","Time:  0.3516 Epoch: 003 Iter: 14400/39218 LR: 0.0001000000 Loss content:  0.3830 Loss fft:  9.9278\n","Time:  0.3518 Epoch: 003 Iter: 14500/39218 LR: 0.0001000000 Loss content:  0.3880 Loss fft: 10.1410\n","Time:  0.3517 Epoch: 003 Iter: 14600/39218 LR: 0.0001000000 Loss content:  0.3943 Loss fft: 10.2644\n","Time:  0.3517 Epoch: 003 Iter: 14700/39218 LR: 0.0001000000 Loss content:  0.3847 Loss fft: 10.0033\n","Time:  0.3516 Epoch: 003 Iter: 14800/39218 LR: 0.0001000000 Loss content:  0.3734 Loss fft: 10.0468\n","Time:  0.3517 Epoch: 003 Iter: 14900/39218 LR: 0.0001000000 Loss content:  0.3787 Loss fft: 10.1229\n","Time:  0.3516 Epoch: 003 Iter: 15000/39218 LR: 0.0001000000 Loss content:  0.3851 Loss fft: 10.2717\n","Time:  0.3517 Epoch: 003 Iter: 15100/39218 LR: 0.0001000000 Loss content:  0.3833 Loss fft: 10.0396\n","Time:  0.3519 Epoch: 003 Iter: 15200/39218 LR: 0.0001000000 Loss content:  0.3774 Loss fft:  9.9804\n","Time:  0.3517 Epoch: 003 Iter: 15300/39218 LR: 0.0001000000 Loss content:  0.3849 Loss fft: 10.3001\n","Time:  0.3516 Epoch: 003 Iter: 15400/39218 LR: 0.0001000000 Loss content:  0.3801 Loss fft: 10.3152\n","Time:  0.3550 Epoch: 003 Iter: 15500/39218 LR: 0.0001000000 Loss content:  0.3846 Loss fft: 10.0724\n","Time:  0.3518 Epoch: 003 Iter: 15600/39218 LR: 0.0001000000 Loss content:  0.3792 Loss fft: 10.0368\n","Time:  0.3517 Epoch: 003 Iter: 15700/39218 LR: 0.0001000000 Loss content:  0.3869 Loss fft: 10.0390\n","Time:  0.3516 Epoch: 003 Iter: 15800/39218 LR: 0.0001000000 Loss content:  0.3808 Loss fft: 10.1837\n","Time:  0.3516 Epoch: 003 Iter: 15900/39218 LR: 0.0001000000 Loss content:  0.3912 Loss fft: 10.2312\n","Time:  0.3518 Epoch: 003 Iter: 16000/39218 LR: 0.0001000000 Loss content:  0.3834 Loss fft: 10.1012\n","Time:  0.3517 Epoch: 003 Iter: 16100/39218 LR: 0.0001000000 Loss content:  0.3840 Loss fft: 10.1290\n","Time:  0.3517 Epoch: 003 Iter: 16200/39218 LR: 0.0001000000 Loss content:  0.3777 Loss fft: 10.0583\n","Time:  0.3518 Epoch: 003 Iter: 16300/39218 LR: 0.0001000000 Loss content:  0.3893 Loss fft: 10.1784\n","Time:  0.3519 Epoch: 003 Iter: 16400/39218 LR: 0.0001000000 Loss content:  0.3758 Loss fft: 10.0608\n","Time:  0.3519 Epoch: 003 Iter: 16500/39218 LR: 0.0001000000 Loss content:  0.3922 Loss fft: 10.0950\n","Time:  0.3518 Epoch: 003 Iter: 16600/39218 LR: 0.0001000000 Loss content:  0.3875 Loss fft: 10.1006\n","Time:  0.3516 Epoch: 003 Iter: 16700/39218 LR: 0.0001000000 Loss content:  0.3876 Loss fft: 10.1044\n","Time:  0.3518 Epoch: 003 Iter: 16800/39218 LR: 0.0001000000 Loss content:  0.3936 Loss fft: 10.3386\n","Time:  0.3518 Epoch: 003 Iter: 16900/39218 LR: 0.0001000000 Loss content:  0.3798 Loss fft: 10.1469\n","Time:  0.3517 Epoch: 003 Iter: 17000/39218 LR: 0.0001000000 Loss content:  0.3921 Loss fft: 10.1235\n","Time:  0.3519 Epoch: 003 Iter: 17100/39218 LR: 0.0001000000 Loss content:  0.3823 Loss fft: 10.2743\n","Time:  0.3519 Epoch: 003 Iter: 17200/39218 LR: 0.0001000000 Loss content:  0.3844 Loss fft: 10.1663\n","Time:  0.3519 Epoch: 003 Iter: 17300/39218 LR: 0.0001000000 Loss content:  0.3911 Loss fft: 10.2212\n","Time:  0.3518 Epoch: 003 Iter: 17400/39218 LR: 0.0001000000 Loss content:  0.3856 Loss fft: 10.2737\n","Time:  0.3518 Epoch: 003 Iter: 17500/39218 LR: 0.0001000000 Loss content:  0.3835 Loss fft:  9.9106\n","Time:  0.3516 Epoch: 003 Iter: 17600/39218 LR: 0.0001000000 Loss content:  0.3854 Loss fft: 10.0667\n","Time:  0.3518 Epoch: 003 Iter: 17700/39218 LR: 0.0001000000 Loss content:  0.3825 Loss fft: 10.0209\n","Time:  0.3553 Epoch: 003 Iter: 17800/39218 LR: 0.0001000000 Loss content:  0.3829 Loss fft: 10.0985\n","Time:  0.3517 Epoch: 003 Iter: 17900/39218 LR: 0.0001000000 Loss content:  0.3865 Loss fft: 10.0831\n","Time:  0.3518 Epoch: 003 Iter: 18000/39218 LR: 0.0001000000 Loss content:  0.3809 Loss fft:  9.9972\n","Time:  0.3519 Epoch: 003 Iter: 18100/39218 LR: 0.0001000000 Loss content:  0.3814 Loss fft: 10.1852\n","Time:  0.3518 Epoch: 003 Iter: 18200/39218 LR: 0.0001000000 Loss content:  0.3916 Loss fft: 10.2479\n","Time:  0.3519 Epoch: 003 Iter: 18300/39218 LR: 0.0001000000 Loss content:  0.3825 Loss fft: 10.0999\n","Time:  0.3519 Epoch: 003 Iter: 18400/39218 LR: 0.0001000000 Loss content:  0.3869 Loss fft: 10.1227\n","Time:  0.3516 Epoch: 003 Iter: 18500/39218 LR: 0.0001000000 Loss content:  0.3863 Loss fft: 10.1296\n","Time:  0.3515 Epoch: 003 Iter: 18600/39218 LR: 0.0001000000 Loss content:  0.3754 Loss fft:  9.9206\n","Time:  0.3517 Epoch: 003 Iter: 18700/39218 LR: 0.0001000000 Loss content:  0.3758 Loss fft: 10.0339\n","Time:  0.3518 Epoch: 003 Iter: 18800/39218 LR: 0.0001000000 Loss content:  0.3804 Loss fft: 10.0763\n","Time:  0.3517 Epoch: 003 Iter: 18900/39218 LR: 0.0001000000 Loss content:  0.3826 Loss fft: 10.1382\n","Time:  0.3516 Epoch: 003 Iter: 19000/39218 LR: 0.0001000000 Loss content:  0.3848 Loss fft: 10.0586\n","Time:  0.3519 Epoch: 003 Iter: 19100/39218 LR: 0.0001000000 Loss content:  0.3721 Loss fft:  9.9529\n","Time:  0.3517 Epoch: 003 Iter: 19200/39218 LR: 0.0001000000 Loss content:  0.3817 Loss fft: 10.1571\n","Time:  0.3519 Epoch: 003 Iter: 19300/39218 LR: 0.0001000000 Loss content:  0.3790 Loss fft: 10.0492\n","Time:  0.3519 Epoch: 003 Iter: 19400/39218 LR: 0.0001000000 Loss content:  0.3853 Loss fft:  9.8992\n","Time:  0.3517 Epoch: 003 Iter: 19500/39218 LR: 0.0001000000 Loss content:  0.3884 Loss fft: 10.2239\n","Time:  0.3518 Epoch: 003 Iter: 19600/39218 LR: 0.0001000000 Loss content:  0.3859 Loss fft: 10.2132\n","Time:  0.3519 Epoch: 003 Iter: 19700/39218 LR: 0.0001000000 Loss content:  0.3802 Loss fft: 10.1717\n","Time:  0.3517 Epoch: 003 Iter: 19800/39218 LR: 0.0001000000 Loss content:  0.3779 Loss fft: 10.1182\n","Time:  0.3517 Epoch: 003 Iter: 19900/39218 LR: 0.0001000000 Loss content:  0.3848 Loss fft: 10.2100\n","Time:  0.3519 Epoch: 003 Iter: 20000/39218 LR: 0.0001000000 Loss content:  0.3782 Loss fft: 10.0192\n","Time:  0.3554 Epoch: 003 Iter: 20100/39218 LR: 0.0001000000 Loss content:  0.3740 Loss fft: 10.0801\n","Time:  0.3518 Epoch: 003 Iter: 20200/39218 LR: 0.0001000000 Loss content:  0.3791 Loss fft: 10.1257\n","Time:  0.3519 Epoch: 003 Iter: 20300/39218 LR: 0.0001000000 Loss content:  0.3762 Loss fft: 10.1759\n","Time:  0.3517 Epoch: 003 Iter: 20400/39218 LR: 0.0001000000 Loss content:  0.3847 Loss fft: 10.0704\n","Time:  0.3518 Epoch: 003 Iter: 20500/39218 LR: 0.0001000000 Loss content:  0.3710 Loss fft: 10.1713\n","Time:  0.3517 Epoch: 003 Iter: 20600/39218 LR: 0.0001000000 Loss content:  0.3756 Loss fft:  9.9620\n","Time:  0.3515 Epoch: 003 Iter: 20700/39218 LR: 0.0001000000 Loss content:  0.3879 Loss fft: 10.0046\n","Time:  0.3517 Epoch: 003 Iter: 20800/39218 LR: 0.0001000000 Loss content:  0.3841 Loss fft: 10.1840\n","Time:  0.3518 Epoch: 003 Iter: 20900/39218 LR: 0.0001000000 Loss content:  0.3697 Loss fft:  9.9389\n","Time:  0.3520 Epoch: 003 Iter: 21000/39218 LR: 0.0001000000 Loss content:  0.3859 Loss fft: 10.2279\n","Time:  0.3521 Epoch: 003 Iter: 21100/39218 LR: 0.0001000000 Loss content:  0.3855 Loss fft: 10.1464\n","Time:  0.3520 Epoch: 003 Iter: 21200/39218 LR: 0.0001000000 Loss content:  0.3864 Loss fft: 10.1332\n","Time:  0.3521 Epoch: 003 Iter: 21300/39218 LR: 0.0001000000 Loss content:  0.3843 Loss fft: 10.2238\n","Time:  0.3518 Epoch: 003 Iter: 21400/39218 LR: 0.0001000000 Loss content:  0.3814 Loss fft: 10.1857\n","Time:  0.3518 Epoch: 003 Iter: 21500/39218 LR: 0.0001000000 Loss content:  0.3884 Loss fft: 10.2685\n","Time:  0.3517 Epoch: 003 Iter: 21600/39218 LR: 0.0001000000 Loss content:  0.3772 Loss fft: 10.0388\n","Time:  0.3517 Epoch: 003 Iter: 21700/39218 LR: 0.0001000000 Loss content:  0.3725 Loss fft: 10.0198\n","Time:  0.3518 Epoch: 003 Iter: 21800/39218 LR: 0.0001000000 Loss content:  0.3860 Loss fft: 10.2226\n","Time:  0.3518 Epoch: 003 Iter: 21900/39218 LR: 0.0001000000 Loss content:  0.3785 Loss fft: 10.1495\n","Time:  0.3522 Epoch: 003 Iter: 22000/39218 LR: 0.0001000000 Loss content:  0.3900 Loss fft: 10.2601\n","Time:  0.3521 Epoch: 003 Iter: 22100/39218 LR: 0.0001000000 Loss content:  0.3825 Loss fft:  9.9763\n","Time:  0.3519 Epoch: 003 Iter: 22200/39218 LR: 0.0001000000 Loss content:  0.3813 Loss fft:  9.9707\n","Time:  0.3554 Epoch: 003 Iter: 22300/39218 LR: 0.0001000000 Loss content:  0.3832 Loss fft: 10.1777\n","Time:  0.3519 Epoch: 003 Iter: 22400/39218 LR: 0.0001000000 Loss content:  0.3858 Loss fft: 10.1476\n","Time:  0.3516 Epoch: 003 Iter: 22500/39218 LR: 0.0001000000 Loss content:  0.3819 Loss fft: 10.1728\n","Time:  0.3515 Epoch: 003 Iter: 22600/39218 LR: 0.0001000000 Loss content:  0.3804 Loss fft: 10.0048\n","Time:  0.3517 Epoch: 003 Iter: 22700/39218 LR: 0.0001000000 Loss content:  0.3788 Loss fft: 10.0078\n","Time:  0.3516 Epoch: 003 Iter: 22800/39218 LR: 0.0001000000 Loss content:  0.3823 Loss fft:  9.9922\n","Time:  0.3516 Epoch: 003 Iter: 22900/39218 LR: 0.0001000000 Loss content:  0.3811 Loss fft: 10.0579\n","Time:  0.3517 Epoch: 003 Iter: 23000/39218 LR: 0.0001000000 Loss content:  0.3838 Loss fft:  9.9984\n","Time:  0.3517 Epoch: 003 Iter: 23100/39218 LR: 0.0001000000 Loss content:  0.3841 Loss fft: 10.0821\n","Time:  0.3518 Epoch: 003 Iter: 23200/39218 LR: 0.0001000000 Loss content:  0.3805 Loss fft: 10.0449\n","Time:  0.3517 Epoch: 003 Iter: 23300/39218 LR: 0.0001000000 Loss content:  0.3812 Loss fft: 10.1087\n","Time:  0.3516 Epoch: 003 Iter: 23400/39218 LR: 0.0001000000 Loss content:  0.3774 Loss fft: 10.1630\n","Time:  0.3517 Epoch: 003 Iter: 23500/39218 LR: 0.0001000000 Loss content:  0.3912 Loss fft: 10.2297\n","Time:  0.3516 Epoch: 003 Iter: 23600/39218 LR: 0.0001000000 Loss content:  0.3859 Loss fft: 10.0953\n","Time:  0.3516 Epoch: 003 Iter: 23700/39218 LR: 0.0001000000 Loss content:  0.3766 Loss fft: 10.1087\n","Time:  0.3517 Epoch: 003 Iter: 23800/39218 LR: 0.0001000000 Loss content:  0.3743 Loss fft: 10.2393\n","Time:  0.3518 Epoch: 003 Iter: 23900/39218 LR: 0.0001000000 Loss content:  0.3824 Loss fft: 10.0166\n","Time:  0.3517 Epoch: 003 Iter: 24000/39218 LR: 0.0001000000 Loss content:  0.3798 Loss fft: 10.2183\n","Time:  0.3520 Epoch: 003 Iter: 24100/39218 LR: 0.0001000000 Loss content:  0.3751 Loss fft: 10.0552\n","Time:  0.3518 Epoch: 003 Iter: 24200/39218 LR: 0.0001000000 Loss content:  0.3844 Loss fft: 10.1103\n","Time:  0.3517 Epoch: 003 Iter: 24300/39218 LR: 0.0001000000 Loss content:  0.3796 Loss fft:  9.9705\n","Time:  0.3518 Epoch: 003 Iter: 24400/39218 LR: 0.0001000000 Loss content:  0.3670 Loss fft:  9.9989\n","Time:  0.3519 Epoch: 003 Iter: 24500/39218 LR: 0.0001000000 Loss content:  0.3755 Loss fft: 10.1145\n","Time:  0.3551 Epoch: 003 Iter: 24600/39218 LR: 0.0001000000 Loss content:  0.3786 Loss fft: 10.1503\n","Time:  0.3518 Epoch: 003 Iter: 24700/39218 LR: 0.0001000000 Loss content:  0.3779 Loss fft: 10.0315\n","Time:  0.3518 Epoch: 003 Iter: 24800/39218 LR: 0.0001000000 Loss content:  0.3757 Loss fft: 10.0302\n","Time:  0.3519 Epoch: 003 Iter: 24900/39218 LR: 0.0001000000 Loss content:  0.3832 Loss fft: 10.0873\n","Time:  0.3518 Epoch: 003 Iter: 25000/39218 LR: 0.0001000000 Loss content:  0.3863 Loss fft: 10.1755\n","Time:  0.3517 Epoch: 003 Iter: 25100/39218 LR: 0.0001000000 Loss content:  0.3721 Loss fft:  9.8398\n","Time:  0.3516 Epoch: 003 Iter: 25200/39218 LR: 0.0001000000 Loss content:  0.3846 Loss fft: 10.2311\n","Time:  0.3516 Epoch: 003 Iter: 25300/39218 LR: 0.0001000000 Loss content:  0.3852 Loss fft: 10.1544\n","Time:  0.3516 Epoch: 003 Iter: 25400/39218 LR: 0.0001000000 Loss content:  0.3766 Loss fft: 10.0646\n","Time:  0.3517 Epoch: 003 Iter: 25500/39218 LR: 0.0001000000 Loss content:  0.3812 Loss fft: 10.1722\n","Time:  0.3518 Epoch: 003 Iter: 25600/39218 LR: 0.0001000000 Loss content:  0.3711 Loss fft:  9.8488\n","Time:  0.3518 Epoch: 003 Iter: 25700/39218 LR: 0.0001000000 Loss content:  0.3756 Loss fft:  9.9063\n","Time:  0.3518 Epoch: 003 Iter: 25800/39218 LR: 0.0001000000 Loss content:  0.3760 Loss fft: 10.0604\n","Time:  0.3517 Epoch: 003 Iter: 25900/39218 LR: 0.0001000000 Loss content:  0.3790 Loss fft: 10.0764\n","Time:  0.3516 Epoch: 003 Iter: 26000/39218 LR: 0.0001000000 Loss content:  0.3740 Loss fft: 10.1096\n","Time:  0.3518 Epoch: 003 Iter: 26100/39218 LR: 0.0001000000 Loss content:  0.3742 Loss fft: 10.0669\n","Time:  0.3517 Epoch: 003 Iter: 26200/39218 LR: 0.0001000000 Loss content:  0.3855 Loss fft: 10.2437\n","Time:  0.3517 Epoch: 003 Iter: 26300/39218 LR: 0.0001000000 Loss content:  0.3736 Loss fft: 10.0182\n","Time:  0.3518 Epoch: 003 Iter: 26400/39218 LR: 0.0001000000 Loss content:  0.3771 Loss fft: 10.1062\n","Time:  0.3519 Epoch: 003 Iter: 26500/39218 LR: 0.0001000000 Loss content:  0.3819 Loss fft:  9.9667\n","Time:  0.3519 Epoch: 003 Iter: 26600/39218 LR: 0.0001000000 Loss content:  0.3806 Loss fft: 10.1054\n","Time:  0.3519 Epoch: 003 Iter: 26700/39218 LR: 0.0001000000 Loss content:  0.3722 Loss fft: 10.0549\n","Time:  0.3517 Epoch: 003 Iter: 26800/39218 LR: 0.0001000000 Loss content:  0.3770 Loss fft:  9.9987\n","Time:  0.3553 Epoch: 003 Iter: 26900/39218 LR: 0.0001000000 Loss content:  0.3863 Loss fft: 10.1662\n","Time:  0.3518 Epoch: 003 Iter: 27000/39218 LR: 0.0001000000 Loss content:  0.3807 Loss fft: 10.1401\n","Time:  0.3518 Epoch: 003 Iter: 27100/39218 LR: 0.0001000000 Loss content:  0.3820 Loss fft: 10.1762\n","Time:  0.3516 Epoch: 003 Iter: 27200/39218 LR: 0.0001000000 Loss content:  0.3825 Loss fft: 10.1041\n","Time:  0.3514 Epoch: 003 Iter: 27300/39218 LR: 0.0001000000 Loss content:  0.3838 Loss fft: 10.2262\n","Time:  0.3518 Epoch: 003 Iter: 27400/39218 LR: 0.0001000000 Loss content:  0.3810 Loss fft: 10.0217\n","Time:  0.3515 Epoch: 003 Iter: 27500/39218 LR: 0.0001000000 Loss content:  0.3821 Loss fft: 10.0090\n","Time:  0.3519 Epoch: 003 Iter: 27600/39218 LR: 0.0001000000 Loss content:  0.3877 Loss fft: 10.0386\n","Time:  0.3518 Epoch: 003 Iter: 27700/39218 LR: 0.0001000000 Loss content:  0.3838 Loss fft: 10.1086\n","Time:  0.3521 Epoch: 003 Iter: 27800/39218 LR: 0.0001000000 Loss content:  0.3820 Loss fft: 10.3172\n","Time:  0.3517 Epoch: 003 Iter: 27900/39218 LR: 0.0001000000 Loss content:  0.3823 Loss fft: 10.1427\n","Time:  0.3517 Epoch: 003 Iter: 28000/39218 LR: 0.0001000000 Loss content:  0.3721 Loss fft:  9.9538\n","Time:  0.3517 Epoch: 003 Iter: 28100/39218 LR: 0.0001000000 Loss content:  0.3777 Loss fft: 10.0342\n","Time:  0.3517 Epoch: 003 Iter: 28200/39218 LR: 0.0001000000 Loss content:  0.3697 Loss fft:  9.9894\n","Time:  0.3517 Epoch: 003 Iter: 28300/39218 LR: 0.0001000000 Loss content:  0.3693 Loss fft:  9.9454\n","Time:  0.3517 Epoch: 003 Iter: 28400/39218 LR: 0.0001000000 Loss content:  0.3706 Loss fft:  9.8980\n","Time:  0.3518 Epoch: 003 Iter: 28500/39218 LR: 0.0001000000 Loss content:  0.3837 Loss fft: 10.1697\n","Time:  0.3517 Epoch: 003 Iter: 28600/39218 LR: 0.0001000000 Loss content:  0.3711 Loss fft:  9.8602\n","Time:  0.3517 Epoch: 003 Iter: 28700/39218 LR: 0.0001000000 Loss content:  0.3770 Loss fft: 10.2028\n","Time:  0.3518 Epoch: 003 Iter: 28800/39218 LR: 0.0001000000 Loss content:  0.3848 Loss fft: 10.1189\n","Time:  0.3516 Epoch: 003 Iter: 28900/39218 LR: 0.0001000000 Loss content:  0.3770 Loss fft: 10.2168\n","Time:  0.3516 Epoch: 003 Iter: 29000/39218 LR: 0.0001000000 Loss content:  0.3729 Loss fft: 10.1115\n","Time:  0.3516 Epoch: 003 Iter: 29100/39218 LR: 0.0001000000 Loss content:  0.3773 Loss fft:  9.8700\n","Time:  0.3552 Epoch: 003 Iter: 29200/39218 LR: 0.0001000000 Loss content:  0.3780 Loss fft: 10.0418\n","Time:  0.3517 Epoch: 003 Iter: 29300/39218 LR: 0.0001000000 Loss content:  0.3794 Loss fft: 10.0450\n","Time:  0.3515 Epoch: 003 Iter: 29400/39218 LR: 0.0001000000 Loss content:  0.3873 Loss fft: 10.1142\n","Time:  0.3517 Epoch: 003 Iter: 29500/39218 LR: 0.0001000000 Loss content:  0.3744 Loss fft: 10.1740\n","Time:  0.3518 Epoch: 003 Iter: 29600/39218 LR: 0.0001000000 Loss content:  0.3772 Loss fft: 10.1127\n","Time:  0.3518 Epoch: 003 Iter: 29700/39218 LR: 0.0001000000 Loss content:  0.3821 Loss fft: 10.2383\n","Time:  0.3518 Epoch: 003 Iter: 29800/39218 LR: 0.0001000000 Loss content:  0.3817 Loss fft: 10.0627\n","Time:  0.3517 Epoch: 003 Iter: 29900/39218 LR: 0.0001000000 Loss content:  0.3791 Loss fft: 10.1599\n","Time:  0.3516 Epoch: 003 Iter: 30000/39218 LR: 0.0001000000 Loss content:  0.3710 Loss fft: 10.0739\n","Time:  0.3519 Epoch: 003 Iter: 30100/39218 LR: 0.0001000000 Loss content:  0.3845 Loss fft: 10.2461\n","Time:  0.3516 Epoch: 003 Iter: 30200/39218 LR: 0.0001000000 Loss content:  0.3673 Loss fft:  9.9413\n","Time:  0.3516 Epoch: 003 Iter: 30300/39218 LR: 0.0001000000 Loss content:  0.3785 Loss fft: 10.1788\n","Time:  0.3519 Epoch: 003 Iter: 30400/39218 LR: 0.0001000000 Loss content:  0.3794 Loss fft: 10.1128\n","Time:  0.3518 Epoch: 003 Iter: 30500/39218 LR: 0.0001000000 Loss content:  0.3791 Loss fft: 10.2541\n","Time:  0.3517 Epoch: 003 Iter: 30600/39218 LR: 0.0001000000 Loss content:  0.3870 Loss fft: 10.0733\n","Time:  0.3517 Epoch: 003 Iter: 30700/39218 LR: 0.0001000000 Loss content:  0.3762 Loss fft: 10.0095\n","Time:  0.3516 Epoch: 003 Iter: 30800/39218 LR: 0.0001000000 Loss content:  0.3764 Loss fft: 10.0527\n","Time:  0.3518 Epoch: 003 Iter: 30900/39218 LR: 0.0001000000 Loss content:  0.3700 Loss fft:  9.9711\n","Time:  0.3517 Epoch: 003 Iter: 31000/39218 LR: 0.0001000000 Loss content:  0.3770 Loss fft: 10.1020\n","Time:  0.3515 Epoch: 003 Iter: 31100/39218 LR: 0.0001000000 Loss content:  0.3786 Loss fft: 10.2203\n","Time:  0.3518 Epoch: 003 Iter: 31200/39218 LR: 0.0001000000 Loss content:  0.3840 Loss fft: 10.1032\n","Time:  0.3516 Epoch: 003 Iter: 31300/39218 LR: 0.0001000000 Loss content:  0.3759 Loss fft: 10.1493\n","Time:  0.3516 Epoch: 003 Iter: 31400/39218 LR: 0.0001000000 Loss content:  0.3845 Loss fft: 10.2284\n","Time:  0.3551 Epoch: 003 Iter: 31500/39218 LR: 0.0001000000 Loss content:  0.3845 Loss fft: 10.1217\n","Time:  0.3516 Epoch: 003 Iter: 31600/39218 LR: 0.0001000000 Loss content:  0.3805 Loss fft:  9.9874\n","Time:  0.3519 Epoch: 003 Iter: 31700/39218 LR: 0.0001000000 Loss content:  0.3800 Loss fft: 10.0744\n","Time:  0.3518 Epoch: 003 Iter: 31800/39218 LR: 0.0001000000 Loss content:  0.3715 Loss fft: 10.0139\n","Time:  0.3517 Epoch: 003 Iter: 31900/39218 LR: 0.0001000000 Loss content:  0.3780 Loss fft: 10.0071\n","Time:  0.3517 Epoch: 003 Iter: 32000/39218 LR: 0.0001000000 Loss content:  0.3725 Loss fft: 10.0041\n","Time:  0.3517 Epoch: 003 Iter: 32100/39218 LR: 0.0001000000 Loss content:  0.3678 Loss fft:  9.8819\n","Time:  0.3518 Epoch: 003 Iter: 32200/39218 LR: 0.0001000000 Loss content:  0.3721 Loss fft: 10.1812\n","Time:  0.3519 Epoch: 003 Iter: 32300/39218 LR: 0.0001000000 Loss content:  0.3787 Loss fft: 10.0914\n","Time:  0.3519 Epoch: 003 Iter: 32400/39218 LR: 0.0001000000 Loss content:  0.3755 Loss fft: 10.0115\n","Time:  0.3518 Epoch: 003 Iter: 32500/39218 LR: 0.0001000000 Loss content:  0.3756 Loss fft: 10.1321\n","Time:  0.3517 Epoch: 003 Iter: 32600/39218 LR: 0.0001000000 Loss content:  0.3672 Loss fft:  9.9551\n","Time:  0.3515 Epoch: 003 Iter: 32700/39218 LR: 0.0001000000 Loss content:  0.3833 Loss fft: 10.1124\n","Time:  0.3516 Epoch: 003 Iter: 32800/39218 LR: 0.0001000000 Loss content:  0.3724 Loss fft:  9.9308\n","Time:  0.3517 Epoch: 003 Iter: 32900/39218 LR: 0.0001000000 Loss content:  0.3767 Loss fft:  9.9091\n","Time:  0.3515 Epoch: 003 Iter: 33000/39218 LR: 0.0001000000 Loss content:  0.3772 Loss fft: 10.1301\n","Time:  0.3515 Epoch: 003 Iter: 33100/39218 LR: 0.0001000000 Loss content:  0.3792 Loss fft: 10.1395\n","Time:  0.3515 Epoch: 003 Iter: 33200/39218 LR: 0.0001000000 Loss content:  0.3765 Loss fft: 10.0098\n","Time:  0.3517 Epoch: 003 Iter: 33300/39218 LR: 0.0001000000 Loss content:  0.3689 Loss fft: 10.0782\n","Time:  0.3517 Epoch: 003 Iter: 33400/39218 LR: 0.0001000000 Loss content:  0.3834 Loss fft: 10.0279\n","Time:  0.3516 Epoch: 003 Iter: 33500/39218 LR: 0.0001000000 Loss content:  0.3737 Loss fft: 10.2449\n","Time:  0.3517 Epoch: 003 Iter: 33600/39218 LR: 0.0001000000 Loss content:  0.3784 Loss fft: 10.0167\n","Time:  0.3554 Epoch: 003 Iter: 33700/39218 LR: 0.0001000000 Loss content:  0.3711 Loss fft: 10.1295\n","Time:  0.3518 Epoch: 003 Iter: 33800/39218 LR: 0.0001000000 Loss content:  0.3687 Loss fft: 10.0573\n","Time:  0.3518 Epoch: 003 Iter: 33900/39218 LR: 0.0001000000 Loss content:  0.3732 Loss fft: 10.0733\n","Time:  0.3516 Epoch: 003 Iter: 34000/39218 LR: 0.0001000000 Loss content:  0.3726 Loss fft: 10.2053\n","Time:  0.3522 Epoch: 003 Iter: 34100/39218 LR: 0.0001000000 Loss content:  0.3773 Loss fft: 10.1025\n","Time:  0.3517 Epoch: 003 Iter: 34200/39218 LR: 0.0001000000 Loss content:  0.3757 Loss fft: 10.0025\n","Time:  0.3519 Epoch: 003 Iter: 34300/39218 LR: 0.0001000000 Loss content:  0.3696 Loss fft: 10.0926\n","Time:  0.3517 Epoch: 003 Iter: 34400/39218 LR: 0.0001000000 Loss content:  0.3849 Loss fft: 10.0553\n","Time:  0.3518 Epoch: 003 Iter: 34500/39218 LR: 0.0001000000 Loss content:  0.3714 Loss fft: 10.0861\n","Time:  0.3517 Epoch: 003 Iter: 34600/39218 LR: 0.0001000000 Loss content:  0.3766 Loss fft: 10.0032\n","Time:  0.3516 Epoch: 003 Iter: 34700/39218 LR: 0.0001000000 Loss content:  0.3694 Loss fft: 10.0200\n","Time:  0.3518 Epoch: 003 Iter: 34800/39218 LR: 0.0001000000 Loss content:  0.3750 Loss fft: 10.0497\n","Time:  0.3519 Epoch: 003 Iter: 34900/39218 LR: 0.0001000000 Loss content:  0.3885 Loss fft: 10.0846\n","Time:  0.3517 Epoch: 003 Iter: 35000/39218 LR: 0.0001000000 Loss content:  0.3749 Loss fft: 10.2927\n","Time:  0.3514 Epoch: 003 Iter: 35100/39218 LR: 0.0001000000 Loss content:  0.3810 Loss fft: 10.1532\n","Time:  0.3516 Epoch: 003 Iter: 35200/39218 LR: 0.0001000000 Loss content:  0.3751 Loss fft: 10.2377\n","Time:  0.3519 Epoch: 003 Iter: 35300/39218 LR: 0.0001000000 Loss content:  0.3853 Loss fft: 10.2173\n","Time:  0.3518 Epoch: 003 Iter: 35400/39218 LR: 0.0001000000 Loss content:  0.3704 Loss fft: 10.0414\n","Time:  0.3516 Epoch: 003 Iter: 35500/39218 LR: 0.0001000000 Loss content:  0.3722 Loss fft: 10.0497\n","Time:  0.3516 Epoch: 003 Iter: 35600/39218 LR: 0.0001000000 Loss content:  0.3785 Loss fft:  9.9192\n","Time:  0.3518 Epoch: 003 Iter: 35700/39218 LR: 0.0001000000 Loss content:  0.3756 Loss fft: 10.0721\n","Time:  0.3518 Epoch: 003 Iter: 35800/39218 LR: 0.0001000000 Loss content:  0.3769 Loss fft: 10.1345\n","Time:  0.3516 Epoch: 003 Iter: 35900/39218 LR: 0.0001000000 Loss content:  0.3696 Loss fft: 10.0672\n","Time:  0.3551 Epoch: 003 Iter: 36000/39218 LR: 0.0001000000 Loss content:  0.3772 Loss fft: 10.0748\n","Time:  0.3516 Epoch: 003 Iter: 36100/39218 LR: 0.0001000000 Loss content:  0.3826 Loss fft: 10.2105\n","Time:  0.3521 Epoch: 003 Iter: 36200/39218 LR: 0.0001000000 Loss content:  0.3644 Loss fft:  9.9328\n","Time:  0.3518 Epoch: 003 Iter: 36300/39218 LR: 0.0001000000 Loss content:  0.3761 Loss fft:  9.8459\n","Time:  0.3518 Epoch: 003 Iter: 36400/39218 LR: 0.0001000000 Loss content:  0.3725 Loss fft:  9.9968\n","Time:  0.3519 Epoch: 003 Iter: 36500/39218 LR: 0.0001000000 Loss content:  0.3834 Loss fft: 10.2167\n","Time:  0.3518 Epoch: 003 Iter: 36600/39218 LR: 0.0001000000 Loss content:  0.3827 Loss fft: 10.0741\n","Time:  0.3517 Epoch: 003 Iter: 36700/39218 LR: 0.0001000000 Loss content:  0.3838 Loss fft: 10.1626\n","Time:  0.3516 Epoch: 003 Iter: 36800/39218 LR: 0.0001000000 Loss content:  0.3772 Loss fft: 10.0050\n","Time:  0.3516 Epoch: 003 Iter: 36900/39218 LR: 0.0001000000 Loss content:  0.3770 Loss fft:  9.9474\n","Time:  0.3517 Epoch: 003 Iter: 37000/39218 LR: 0.0001000000 Loss content:  0.3882 Loss fft: 10.1376\n","Time:  0.3517 Epoch: 003 Iter: 37100/39218 LR: 0.0001000000 Loss content:  0.3886 Loss fft: 10.1550\n","Time:  0.3519 Epoch: 003 Iter: 37200/39218 LR: 0.0001000000 Loss content:  0.3738 Loss fft: 10.1051\n","Time:  0.3518 Epoch: 003 Iter: 37300/39218 LR: 0.0001000000 Loss content:  0.3704 Loss fft:  9.9050\n","Time:  0.3518 Epoch: 003 Iter: 37400/39218 LR: 0.0001000000 Loss content:  0.3626 Loss fft:  9.9000\n","Time:  0.3518 Epoch: 003 Iter: 37500/39218 LR: 0.0001000000 Loss content:  0.3687 Loss fft:  9.9587\n","Time:  0.3518 Epoch: 003 Iter: 37600/39218 LR: 0.0001000000 Loss content:  0.3665 Loss fft: 10.0072\n","Time:  0.3517 Epoch: 003 Iter: 37700/39218 LR: 0.0001000000 Loss content:  0.3732 Loss fft: 10.1173\n","Time:  0.3517 Epoch: 003 Iter: 37800/39218 LR: 0.0001000000 Loss content:  0.3653 Loss fft:  9.7881\n","Time:  0.3518 Epoch: 003 Iter: 37900/39218 LR: 0.0001000000 Loss content:  0.3701 Loss fft:  9.9304\n","Time:  0.3520 Epoch: 003 Iter: 38000/39218 LR: 0.0001000000 Loss content:  0.3715 Loss fft:  9.9668\n","Time:  0.3518 Epoch: 003 Iter: 38100/39218 LR: 0.0001000000 Loss content:  0.3713 Loss fft: 10.0638\n","Time:  0.3517 Epoch: 003 Iter: 38200/39218 LR: 0.0001000000 Loss content:  0.3757 Loss fft: 10.0699\n","Time:  0.3554 Epoch: 003 Iter: 38300/39218 LR: 0.0001000000 Loss content:  0.3714 Loss fft:  9.9221\n","Time:  0.3518 Epoch: 003 Iter: 38400/39218 LR: 0.0001000000 Loss content:  0.3747 Loss fft: 10.0726\n","Time:  0.3516 Epoch: 003 Iter: 38500/39218 LR: 0.0001000000 Loss content:  0.3699 Loss fft:  9.9581\n","Time:  0.3516 Epoch: 003 Iter: 38600/39218 LR: 0.0001000000 Loss content:  0.3749 Loss fft: 10.0586\n","Time:  0.3518 Epoch: 003 Iter: 38700/39218 LR: 0.0001000000 Loss content:  0.3752 Loss fft: 10.2146\n","Time:  0.3517 Epoch: 003 Iter: 38800/39218 LR: 0.0001000000 Loss content:  0.3731 Loss fft:  9.9710\n","Time:  0.3516 Epoch: 003 Iter: 38900/39218 LR: 0.0001000000 Loss content:  0.3766 Loss fft:  9.9534\n","Time:  0.3515 Epoch: 003 Iter: 39000/39218 LR: 0.0001000000 Loss content:  0.3740 Loss fft:  9.9244\n","Time:  0.3519 Epoch: 003 Iter: 39100/39218 LR: 0.0001000000 Loss content:  0.3727 Loss fft:  9.9316\n","Time:  0.3517 Epoch: 003 Iter: 39200/39218 LR: 0.0001000000 Loss content:  0.3765 Loss fft:  9.9490\n","EPOCH: 03\n","Elapsed time: 138.01 Epoch Pixel Loss:  0.3822 Epoch FFT Loss: 10.1120\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:809: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Time:  0.3666 Epoch: 004 Iter:  100/39218 LR: 0.0001000000 Loss content:  0.3696 Loss fft: 10.0339\n","Time:  0.3516 Epoch: 004 Iter:  200/39218 LR: 0.0001000000 Loss content:  0.3658 Loss fft:  9.9462\n","Time:  0.3517 Epoch: 004 Iter:  300/39218 LR: 0.0001000000 Loss content:  0.3811 Loss fft: 10.1063\n","Time:  0.3518 Epoch: 004 Iter:  400/39218 LR: 0.0001000000 Loss content:  0.3757 Loss fft:  9.8284\n","Time:  0.3517 Epoch: 004 Iter:  500/39218 LR: 0.0001000000 Loss content:  0.3607 Loss fft:  9.9488\n","Time:  0.3517 Epoch: 004 Iter:  600/39218 LR: 0.0001000000 Loss content:  0.3726 Loss fft:  9.9992\n","Time:  0.3516 Epoch: 004 Iter:  700/39218 LR: 0.0001000000 Loss content:  0.3724 Loss fft: 10.0445\n","Time:  0.3516 Epoch: 004 Iter:  800/39218 LR: 0.0001000000 Loss content:  0.3724 Loss fft: 10.0268\n","Time:  0.3518 Epoch: 004 Iter:  900/39218 LR: 0.0001000000 Loss content:  0.3809 Loss fft: 10.1147\n","Time:  0.3516 Epoch: 004 Iter: 1000/39218 LR: 0.0001000000 Loss content:  0.3770 Loss fft: 10.1029\n","Time:  0.3517 Epoch: 004 Iter: 1100/39218 LR: 0.0001000000 Loss content:  0.3756 Loss fft: 10.2371\n","Time:  0.3518 Epoch: 004 Iter: 1200/39218 LR: 0.0001000000 Loss content:  0.3681 Loss fft:  9.8137\n","Time:  0.3567 Epoch: 004 Iter: 1300/39218 LR: 0.0001000000 Loss content:  0.3781 Loss fft: 10.1203\n","Time:  0.3516 Epoch: 004 Iter: 1400/39218 LR: 0.0001000000 Loss content:  0.3772 Loss fft: 10.0481\n","Time:  0.3517 Epoch: 004 Iter: 1500/39218 LR: 0.0001000000 Loss content:  0.3728 Loss fft: 10.1237\n","Time:  0.3514 Epoch: 004 Iter: 1600/39218 LR: 0.0001000000 Loss content:  0.3864 Loss fft: 10.1678\n","Time:  0.3517 Epoch: 004 Iter: 1700/39218 LR: 0.0001000000 Loss content:  0.3682 Loss fft: 10.0809\n","Time:  0.3516 Epoch: 004 Iter: 1800/39218 LR: 0.0001000000 Loss content:  0.3649 Loss fft:  9.9740\n","Time:  0.3517 Epoch: 004 Iter: 1900/39218 LR: 0.0001000000 Loss content:  0.3775 Loss fft: 10.2089\n","Time:  0.3517 Epoch: 004 Iter: 2000/39218 LR: 0.0001000000 Loss content:  0.3819 Loss fft: 10.1752\n","Time:  0.3517 Epoch: 004 Iter: 2100/39218 LR: 0.0001000000 Loss content:  0.3702 Loss fft:  9.9331\n","Time:  0.3519 Epoch: 004 Iter: 2200/39218 LR: 0.0001000000 Loss content:  0.3767 Loss fft: 10.0569\n","Time:  0.3517 Epoch: 004 Iter: 2300/39218 LR: 0.0001000000 Loss content:  0.3742 Loss fft: 10.1353\n","Time:  0.3518 Epoch: 004 Iter: 2400/39218 LR: 0.0001000000 Loss content:  0.3645 Loss fft:  9.9343\n","Time:  0.3517 Epoch: 004 Iter: 2500/39218 LR: 0.0001000000 Loss content:  0.3785 Loss fft:  9.8437\n","Time:  0.3517 Epoch: 004 Iter: 2600/39218 LR: 0.0001000000 Loss content:  0.3733 Loss fft: 10.0476\n","Time:  0.3516 Epoch: 004 Iter: 2700/39218 LR: 0.0001000000 Loss content:  0.3705 Loss fft:  9.9589\n","Time:  0.3516 Epoch: 004 Iter: 2800/39218 LR: 0.0001000000 Loss content:  0.3738 Loss fft: 10.1177\n","Time:  0.3517 Epoch: 004 Iter: 2900/39218 LR: 0.0001000000 Loss content:  0.3732 Loss fft:  9.9151\n","Time:  0.3518 Epoch: 004 Iter: 3000/39218 LR: 0.0001000000 Loss content:  0.3698 Loss fft:  9.9807\n","Time:  0.3518 Epoch: 004 Iter: 3100/39218 LR: 0.0001000000 Loss content:  0.3693 Loss fft:  9.9963\n","Time:  0.3517 Epoch: 004 Iter: 3200/39218 LR: 0.0001000000 Loss content:  0.3707 Loss fft: 10.0460\n","Time:  0.3517 Epoch: 004 Iter: 3300/39218 LR: 0.0001000000 Loss content:  0.3809 Loss fft:  9.8740\n","Time:  0.3517 Epoch: 004 Iter: 3400/39218 LR: 0.0001000000 Loss content:  0.3764 Loss fft: 10.1977\n","Time:  0.3517 Epoch: 004 Iter: 3500/39218 LR: 0.0001000000 Loss content:  0.3732 Loss fft:  9.8129\n","Time:  0.3550 Epoch: 004 Iter: 3600/39218 LR: 0.0001000000 Loss content:  0.3753 Loss fft: 10.0601\n","Time:  0.3516 Epoch: 004 Iter: 3700/39218 LR: 0.0001000000 Loss content:  0.3762 Loss fft:  9.9725\n","Time:  0.3519 Epoch: 004 Iter: 3800/39218 LR: 0.0001000000 Loss content:  0.3792 Loss fft: 10.0959\n","Time:  0.3516 Epoch: 004 Iter: 3900/39218 LR: 0.0001000000 Loss content:  0.3794 Loss fft:  9.9935\n","Time:  0.3517 Epoch: 004 Iter: 4000/39218 LR: 0.0001000000 Loss content:  0.3757 Loss fft:  9.9944\n","Time:  0.3518 Epoch: 004 Iter: 4100/39218 LR: 0.0001000000 Loss content:  0.3762 Loss fft: 10.0448\n","Time:  0.3519 Epoch: 004 Iter: 4200/39218 LR: 0.0001000000 Loss content:  0.3667 Loss fft: 10.0240\n","Time:  0.3517 Epoch: 004 Iter: 4300/39218 LR: 0.0001000000 Loss content:  0.3809 Loss fft: 10.1563\n","Time:  0.3516 Epoch: 004 Iter: 4400/39218 LR: 0.0001000000 Loss content:  0.3690 Loss fft:  9.9763\n","Time:  0.3517 Epoch: 004 Iter: 4500/39218 LR: 0.0001000000 Loss content:  0.3799 Loss fft: 10.2561\n","Time:  0.3516 Epoch: 004 Iter: 4600/39218 LR: 0.0001000000 Loss content:  0.3697 Loss fft: 10.0190\n","Time:  0.3518 Epoch: 004 Iter: 4700/39218 LR: 0.0001000000 Loss content:  0.3720 Loss fft: 10.0016\n","Time:  0.3517 Epoch: 004 Iter: 4800/39218 LR: 0.0001000000 Loss content:  0.3671 Loss fft:  9.9748\n","Time:  0.3518 Epoch: 004 Iter: 4900/39218 LR: 0.0001000000 Loss content:  0.3724 Loss fft:  9.9625\n","Time:  0.3519 Epoch: 004 Iter: 5000/39218 LR: 0.0001000000 Loss content:  0.3748 Loss fft:  9.9982\n","Time:  0.3518 Epoch: 004 Iter: 5100/39218 LR: 0.0001000000 Loss content:  0.3767 Loss fft: 10.0288\n","Time:  0.3519 Epoch: 004 Iter: 5200/39218 LR: 0.0001000000 Loss content:  0.3806 Loss fft: 10.0754\n","Time:  0.3516 Epoch: 004 Iter: 5300/39218 LR: 0.0001000000 Loss content:  0.3667 Loss fft: 10.0248\n","Time:  0.3517 Epoch: 004 Iter: 5400/39218 LR: 0.0001000000 Loss content:  0.3761 Loss fft: 10.0975\n","Time:  0.3515 Epoch: 004 Iter: 5500/39218 LR: 0.0001000000 Loss content:  0.3851 Loss fft: 10.1448\n","Time:  0.3516 Epoch: 004 Iter: 5600/39218 LR: 0.0001000000 Loss content:  0.3710 Loss fft: 10.0486\n","Time:  0.3517 Epoch: 004 Iter: 5700/39218 LR: 0.0001000000 Loss content:  0.3710 Loss fft: 10.0486\n","Time:  0.3518 Epoch: 004 Iter: 5800/39218 LR: 0.0001000000 Loss content:  0.3679 Loss fft:  9.9282\n","Time:  0.3554 Epoch: 004 Iter: 5900/39218 LR: 0.0001000000 Loss content:  0.3742 Loss fft:  9.9962\n","Time:  0.3517 Epoch: 004 Iter: 6000/39218 LR: 0.0001000000 Loss content:  0.3721 Loss fft: 10.0202\n","Time:  0.3517 Epoch: 004 Iter: 6100/39218 LR: 0.0001000000 Loss content:  0.3715 Loss fft: 10.0054\n","Time:  0.3519 Epoch: 004 Iter: 6200/39218 LR: 0.0001000000 Loss content:  0.3695 Loss fft: 10.0751\n","Time:  0.3517 Epoch: 004 Iter: 6300/39218 LR: 0.0001000000 Loss content:  0.3727 Loss fft: 10.0367\n","Time:  0.3516 Epoch: 004 Iter: 6400/39218 LR: 0.0001000000 Loss content:  0.3771 Loss fft: 10.1738\n","Time:  0.3517 Epoch: 004 Iter: 6500/39218 LR: 0.0001000000 Loss content:  0.3752 Loss fft: 10.0427\n","Time:  0.3516 Epoch: 004 Iter: 6600/39218 LR: 0.0001000000 Loss content:  0.3724 Loss fft:  9.9086\n","Time:  0.3516 Epoch: 004 Iter: 6700/39218 LR: 0.0001000000 Loss content:  0.3766 Loss fft: 10.0697\n","Time:  0.3516 Epoch: 004 Iter: 6800/39218 LR: 0.0001000000 Loss content:  0.3737 Loss fft:  9.9128\n","Time:  0.3518 Epoch: 004 Iter: 6900/39218 LR: 0.0001000000 Loss content:  0.3666 Loss fft: 10.0092\n","Time:  0.3516 Epoch: 004 Iter: 7000/39218 LR: 0.0001000000 Loss content:  0.3631 Loss fft:  9.8574\n","Time:  0.3518 Epoch: 004 Iter: 7100/39218 LR: 0.0001000000 Loss content:  0.3775 Loss fft: 10.4153\n","Time:  0.3521 Epoch: 004 Iter: 7200/39218 LR: 0.0001000000 Loss content:  0.3733 Loss fft:  9.8349\n","Time:  0.3517 Epoch: 004 Iter: 7300/39218 LR: 0.0001000000 Loss content:  0.3677 Loss fft: 10.0329\n","Time:  0.3517 Epoch: 004 Iter: 7400/39218 LR: 0.0001000000 Loss content:  0.3746 Loss fft: 10.0741\n","Time:  0.3515 Epoch: 004 Iter: 7500/39218 LR: 0.0001000000 Loss content:  0.3722 Loss fft: 10.1143\n","Time:  0.3517 Epoch: 004 Iter: 7600/39218 LR: 0.0001000000 Loss content:  0.3674 Loss fft: 10.0940\n","Time:  0.3517 Epoch: 004 Iter: 7700/39218 LR: 0.0001000000 Loss content:  0.3650 Loss fft:  9.9367\n","Time:  0.3515 Epoch: 004 Iter: 7800/39218 LR: 0.0001000000 Loss content:  0.3738 Loss fft:  9.9861\n","Time:  0.3516 Epoch: 004 Iter: 7900/39218 LR: 0.0001000000 Loss content:  0.3618 Loss fft:  9.9208\n","Time:  0.3517 Epoch: 004 Iter: 8000/39218 LR: 0.0001000000 Loss content:  0.3649 Loss fft:  9.9819\n","Time:  0.3515 Epoch: 004 Iter: 8100/39218 LR: 0.0001000000 Loss content:  0.3649 Loss fft: 10.1070\n","Time:  0.3551 Epoch: 004 Iter: 8200/39218 LR: 0.0001000000 Loss content:  0.3623 Loss fft:  9.7140\n","Time:  0.3516 Epoch: 004 Iter: 8300/39218 LR: 0.0001000000 Loss content:  0.3676 Loss fft: 10.0174\n","Time:  0.3518 Epoch: 004 Iter: 8400/39218 LR: 0.0001000000 Loss content:  0.3639 Loss fft:  9.8625\n","Time:  0.3520 Epoch: 004 Iter: 8500/39218 LR: 0.0001000000 Loss content:  0.3737 Loss fft:  9.9373\n","Time:  0.3519 Epoch: 004 Iter: 8600/39218 LR: 0.0001000000 Loss content:  0.3705 Loss fft: 10.1748\n","Time:  0.3519 Epoch: 004 Iter: 8700/39218 LR: 0.0001000000 Loss content:  0.3753 Loss fft: 10.0973\n","Time:  0.3518 Epoch: 004 Iter: 8800/39218 LR: 0.0001000000 Loss content:  0.3660 Loss fft:  9.9642\n","Time:  0.3519 Epoch: 004 Iter: 8900/39218 LR: 0.0001000000 Loss content:  0.3711 Loss fft:  9.9915\n","Time:  0.3518 Epoch: 004 Iter: 9000/39218 LR: 0.0001000000 Loss content:  0.3541 Loss fft:  9.9883\n","Time:  0.3515 Epoch: 004 Iter: 9100/39218 LR: 0.0001000000 Loss content:  0.3676 Loss fft:  9.8528\n","Time:  0.3518 Epoch: 004 Iter: 9200/39218 LR: 0.0001000000 Loss content:  0.3650 Loss fft:  9.7747\n","Time:  0.3518 Epoch: 004 Iter: 9300/39218 LR: 0.0001000000 Loss content:  0.3624 Loss fft:  9.7814\n","Time:  0.3516 Epoch: 004 Iter: 9400/39218 LR: 0.0001000000 Loss content:  0.3740 Loss fft: 10.0317\n","Time:  0.3517 Epoch: 004 Iter: 9500/39218 LR: 0.0001000000 Loss content:  0.3762 Loss fft:  9.9627\n","Time:  0.3518 Epoch: 004 Iter: 9600/39218 LR: 0.0001000000 Loss content:  0.3632 Loss fft: 10.0686\n","Time:  0.3516 Epoch: 004 Iter: 9700/39218 LR: 0.0001000000 Loss content:  0.3750 Loss fft: 10.0961\n","Time:  0.3517 Epoch: 004 Iter: 9800/39218 LR: 0.0001000000 Loss content:  0.3709 Loss fft:  9.8882\n","Time:  0.3516 Epoch: 004 Iter: 9900/39218 LR: 0.0001000000 Loss content:  0.3750 Loss fft:  9.9810\n","Time:  0.3517 Epoch: 004 Iter: 10000/39218 LR: 0.0001000000 Loss content:  0.3782 Loss fft: 10.1003\n","Time:  0.3518 Epoch: 004 Iter: 10100/39218 LR: 0.0001000000 Loss content:  0.3597 Loss fft: 10.0286\n","Time:  0.3518 Epoch: 004 Iter: 10200/39218 LR: 0.0001000000 Loss content:  0.3733 Loss fft:  9.8682\n","Time:  0.3519 Epoch: 004 Iter: 10300/39218 LR: 0.0001000000 Loss content:  0.3661 Loss fft: 10.0667\n","Time:  0.3551 Epoch: 004 Iter: 10400/39218 LR: 0.0001000000 Loss content:  0.3702 Loss fft: 10.0376\n","Time:  0.3516 Epoch: 004 Iter: 10500/39218 LR: 0.0001000000 Loss content:  0.3703 Loss fft: 10.2334\n","Time:  0.3517 Epoch: 004 Iter: 10600/39218 LR: 0.0001000000 Loss content:  0.3817 Loss fft: 10.2843\n","Time:  0.3515 Epoch: 004 Iter: 10700/39218 LR: 0.0001000000 Loss content:  0.3643 Loss fft:  9.9979\n","Time:  0.3514 Epoch: 004 Iter: 10800/39218 LR: 0.0001000000 Loss content:  0.3652 Loss fft:  9.9635\n","Time:  0.3516 Epoch: 004 Iter: 10900/39218 LR: 0.0001000000 Loss content:  0.3837 Loss fft: 10.2295\n","Time:  0.3520 Epoch: 004 Iter: 11000/39218 LR: 0.0001000000 Loss content:  0.3635 Loss fft:  9.8966\n","Time:  0.3517 Epoch: 004 Iter: 11100/39218 LR: 0.0001000000 Loss content:  0.3673 Loss fft: 10.0433\n","Time:  0.3518 Epoch: 004 Iter: 11200/39218 LR: 0.0001000000 Loss content:  0.3700 Loss fft: 10.0459\n","Time:  0.3517 Epoch: 004 Iter: 11300/39218 LR: 0.0001000000 Loss content:  0.3728 Loss fft:  9.9788\n","Time:  0.3516 Epoch: 004 Iter: 11400/39218 LR: 0.0001000000 Loss content:  0.3831 Loss fft: 10.0434\n","Time:  0.3518 Epoch: 004 Iter: 11500/39218 LR: 0.0001000000 Loss content:  0.3728 Loss fft: 10.0570\n","Time:  0.3517 Epoch: 004 Iter: 11600/39218 LR: 0.0001000000 Loss content:  0.3738 Loss fft:  9.9670\n","Time:  0.3518 Epoch: 004 Iter: 11700/39218 LR: 0.0001000000 Loss content:  0.3739 Loss fft:  9.9837\n","Time:  0.3517 Epoch: 004 Iter: 11800/39218 LR: 0.0001000000 Loss content:  0.3778 Loss fft: 10.1314\n","Time:  0.3519 Epoch: 004 Iter: 11900/39218 LR: 0.0001000000 Loss content:  0.3674 Loss fft:  9.9521\n","Time:  0.3517 Epoch: 004 Iter: 12000/39218 LR: 0.0001000000 Loss content:  0.3699 Loss fft:  9.9634\n","Time:  0.3519 Epoch: 004 Iter: 12100/39218 LR: 0.0001000000 Loss content:  0.3707 Loss fft: 10.0477\n","Time:  0.3519 Epoch: 004 Iter: 12200/39218 LR: 0.0001000000 Loss content:  0.3692 Loss fft: 10.0388\n","Time:  0.3516 Epoch: 004 Iter: 12300/39218 LR: 0.0001000000 Loss content:  0.3610 Loss fft:  9.9555\n","Time:  0.3517 Epoch: 004 Iter: 12400/39218 LR: 0.0001000000 Loss content:  0.3716 Loss fft:  9.9268\n","Time:  0.3516 Epoch: 004 Iter: 12500/39218 LR: 0.0001000000 Loss content:  0.3708 Loss fft: 10.0431\n","Time:  0.3515 Epoch: 004 Iter: 12600/39218 LR: 0.0001000000 Loss content:  0.3713 Loss fft: 10.0512\n","Time:  0.3549 Epoch: 004 Iter: 12700/39218 LR: 0.0001000000 Loss content:  0.3643 Loss fft:  9.9433\n","Time:  0.3517 Epoch: 004 Iter: 12800/39218 LR: 0.0001000000 Loss content:  0.3799 Loss fft:  9.9737\n","Time:  0.3517 Epoch: 004 Iter: 12900/39218 LR: 0.0001000000 Loss content:  0.3644 Loss fft:  9.9687\n","Time:  0.3517 Epoch: 004 Iter: 13000/39218 LR: 0.0001000000 Loss content:  0.3645 Loss fft:  9.9839\n","Time:  0.3517 Epoch: 004 Iter: 13100/39218 LR: 0.0001000000 Loss content:  0.3648 Loss fft:  9.9255\n","Time:  0.3517 Epoch: 004 Iter: 13200/39218 LR: 0.0001000000 Loss content:  0.3754 Loss fft:  9.8369\n","Time:  0.3516 Epoch: 004 Iter: 13300/39218 LR: 0.0001000000 Loss content:  0.3779 Loss fft: 10.1094\n","Time:  0.3516 Epoch: 004 Iter: 13400/39218 LR: 0.0001000000 Loss content:  0.3784 Loss fft: 10.0825\n","Time:  0.3517 Epoch: 004 Iter: 13500/39218 LR: 0.0001000000 Loss content:  0.3647 Loss fft: 10.0208\n","Time:  0.3517 Epoch: 004 Iter: 13600/39218 LR: 0.0001000000 Loss content:  0.3643 Loss fft:  9.8926\n","Time:  0.3515 Epoch: 004 Iter: 13700/39218 LR: 0.0001000000 Loss content:  0.3613 Loss fft:  9.8700\n","Time:  0.3517 Epoch: 004 Iter: 13800/39218 LR: 0.0001000000 Loss content:  0.3774 Loss fft:  9.9413\n","Time:  0.3517 Epoch: 004 Iter: 13900/39218 LR: 0.0001000000 Loss content:  0.3777 Loss fft: 10.0108\n","Time:  0.3517 Epoch: 004 Iter: 14000/39218 LR: 0.0001000000 Loss content:  0.3739 Loss fft: 10.1088\n","Time:  0.3518 Epoch: 004 Iter: 14100/39218 LR: 0.0001000000 Loss content:  0.3717 Loss fft: 10.1223\n","Time:  0.3519 Epoch: 004 Iter: 14200/39218 LR: 0.0001000000 Loss content:  0.3738 Loss fft: 10.0582\n","Time:  0.3518 Epoch: 004 Iter: 14300/39218 LR: 0.0001000000 Loss content:  0.3690 Loss fft: 10.0773\n","Time:  0.3516 Epoch: 004 Iter: 14400/39218 LR: 0.0001000000 Loss content:  0.3696 Loss fft:  9.9909\n","Time:  0.3516 Epoch: 004 Iter: 14500/39218 LR: 0.0001000000 Loss content:  0.3725 Loss fft: 10.0220\n","Time:  0.3516 Epoch: 004 Iter: 14600/39218 LR: 0.0001000000 Loss content:  0.3701 Loss fft:  9.9152\n","Time:  0.3514 Epoch: 004 Iter: 14700/39218 LR: 0.0001000000 Loss content:  0.3634 Loss fft: 10.0177\n","Time:  0.3517 Epoch: 004 Iter: 14800/39218 LR: 0.0001000000 Loss content:  0.3698 Loss fft:  9.9489\n","Time:  0.3516 Epoch: 004 Iter: 14900/39218 LR: 0.0001000000 Loss content:  0.3743 Loss fft:  9.9634\n","Time:  0.3551 Epoch: 004 Iter: 15000/39218 LR: 0.0001000000 Loss content:  0.3705 Loss fft: 10.0897\n","Time:  0.3518 Epoch: 004 Iter: 15100/39218 LR: 0.0001000000 Loss content:  0.3719 Loss fft:  9.9613\n","Time:  0.3516 Epoch: 004 Iter: 15200/39218 LR: 0.0001000000 Loss content:  0.3702 Loss fft: 10.2020\n","Time:  0.3517 Epoch: 004 Iter: 15300/39218 LR: 0.0001000000 Loss content:  0.3582 Loss fft:  9.9113\n","Time:  0.3516 Epoch: 004 Iter: 15400/39218 LR: 0.0001000000 Loss content:  0.3705 Loss fft: 10.0550\n","Time:  0.3516 Epoch: 004 Iter: 15500/39218 LR: 0.0001000000 Loss content:  0.3707 Loss fft: 10.1312\n","Time:  0.3520 Epoch: 004 Iter: 15600/39218 LR: 0.0001000000 Loss content:  0.3582 Loss fft:  9.7291\n","Time:  0.3516 Epoch: 004 Iter: 15700/39218 LR: 0.0001000000 Loss content:  0.3684 Loss fft:  9.9918\n","Time:  0.3516 Epoch: 004 Iter: 15800/39218 LR: 0.0001000000 Loss content:  0.3686 Loss fft:  9.8695\n","Time:  0.3517 Epoch: 004 Iter: 15900/39218 LR: 0.0001000000 Loss content:  0.3584 Loss fft:  9.9258\n","Time:  0.3514 Epoch: 004 Iter: 16000/39218 LR: 0.0001000000 Loss content:  0.3720 Loss fft:  9.9659\n","Time:  0.3517 Epoch: 004 Iter: 16100/39218 LR: 0.0001000000 Loss content:  0.3743 Loss fft:  9.8649\n","Time:  0.3516 Epoch: 004 Iter: 16200/39218 LR: 0.0001000000 Loss content:  0.3647 Loss fft:  9.9446\n","Time:  0.3517 Epoch: 004 Iter: 16300/39218 LR: 0.0001000000 Loss content:  0.3765 Loss fft:  9.9338\n","Time:  0.3520 Epoch: 004 Iter: 16400/39218 LR: 0.0001000000 Loss content:  0.3642 Loss fft:  9.9827\n","Time:  0.3517 Epoch: 004 Iter: 16500/39218 LR: 0.0001000000 Loss content:  0.3764 Loss fft: 10.1640\n","Time:  0.3518 Epoch: 004 Iter: 16600/39218 LR: 0.0001000000 Loss content:  0.3690 Loss fft:  9.8497\n","Time:  0.3517 Epoch: 004 Iter: 16700/39218 LR: 0.0001000000 Loss content:  0.3641 Loss fft: 10.0030\n","Time:  0.3515 Epoch: 004 Iter: 16800/39218 LR: 0.0001000000 Loss content:  0.3723 Loss fft:  9.9891\n","Time:  0.3515 Epoch: 004 Iter: 16900/39218 LR: 0.0001000000 Loss content:  0.3706 Loss fft: 10.0662\n","Time:  0.3517 Epoch: 004 Iter: 17000/39218 LR: 0.0001000000 Loss content:  0.3610 Loss fft:  9.8628\n","Time:  0.3517 Epoch: 004 Iter: 17100/39218 LR: 0.0001000000 Loss content:  0.3680 Loss fft: 10.0273\n","Time:  0.3517 Epoch: 004 Iter: 17200/39218 LR: 0.0001000000 Loss content:  0.3698 Loss fft: 10.0080\n","Time:  0.3552 Epoch: 004 Iter: 17300/39218 LR: 0.0001000000 Loss content:  0.3671 Loss fft:  9.8588\n","Time:  0.3516 Epoch: 004 Iter: 17400/39218 LR: 0.0001000000 Loss content:  0.3674 Loss fft: 10.1052\n","Time:  0.3518 Epoch: 004 Iter: 17500/39218 LR: 0.0001000000 Loss content:  0.3644 Loss fft:  9.8849\n","Time:  0.3517 Epoch: 004 Iter: 17600/39218 LR: 0.0001000000 Loss content:  0.3706 Loss fft:  9.9813\n","Time:  0.3516 Epoch: 004 Iter: 17700/39218 LR: 0.0001000000 Loss content:  0.3713 Loss fft: 10.1071\n","Time:  0.3517 Epoch: 004 Iter: 17800/39218 LR: 0.0001000000 Loss content:  0.3648 Loss fft:  9.8208\n","Time:  0.3516 Epoch: 004 Iter: 17900/39218 LR: 0.0001000000 Loss content:  0.3727 Loss fft: 10.1976\n","Time:  0.3515 Epoch: 004 Iter: 18000/39218 LR: 0.0001000000 Loss content:  0.3686 Loss fft:  9.9965\n","Time:  0.3518 Epoch: 004 Iter: 18100/39218 LR: 0.0001000000 Loss content:  0.3688 Loss fft:  9.9527\n","Time:  0.3518 Epoch: 004 Iter: 18200/39218 LR: 0.0001000000 Loss content:  0.3592 Loss fft: 10.1378\n","Time:  0.3518 Epoch: 004 Iter: 18300/39218 LR: 0.0001000000 Loss content:  0.3786 Loss fft: 10.0297\n","Time:  0.3517 Epoch: 004 Iter: 18400/39218 LR: 0.0001000000 Loss content:  0.3594 Loss fft:  9.8164\n","Time:  0.3516 Epoch: 004 Iter: 18500/39218 LR: 0.0001000000 Loss content:  0.3687 Loss fft: 10.0296\n","Time:  0.3517 Epoch: 004 Iter: 18600/39218 LR: 0.0001000000 Loss content:  0.3639 Loss fft:  9.8904\n","Time:  0.3518 Epoch: 004 Iter: 18700/39218 LR: 0.0001000000 Loss content:  0.3677 Loss fft:  9.9939\n","Time:  0.3518 Epoch: 004 Iter: 18800/39218 LR: 0.0001000000 Loss content:  0.3779 Loss fft: 10.1554\n","Time:  0.3517 Epoch: 004 Iter: 18900/39218 LR: 0.0001000000 Loss content:  0.3754 Loss fft: 10.0942\n","Time:  0.3518 Epoch: 004 Iter: 19000/39218 LR: 0.0001000000 Loss content:  0.3565 Loss fft:  9.7925\n","Time:  0.3518 Epoch: 004 Iter: 19100/39218 LR: 0.0001000000 Loss content:  0.3787 Loss fft: 10.1626\n","Time:  0.3518 Epoch: 004 Iter: 19200/39218 LR: 0.0001000000 Loss content:  0.3680 Loss fft:  9.8849\n","Time:  0.3516 Epoch: 004 Iter: 19300/39218 LR: 0.0001000000 Loss content:  0.3629 Loss fft:  9.8639\n","Time:  0.3516 Epoch: 004 Iter: 19400/39218 LR: 0.0001000000 Loss content:  0.3635 Loss fft:  9.9327\n","Time:  0.3552 Epoch: 004 Iter: 19500/39218 LR: 0.0001000000 Loss content:  0.3661 Loss fft:  9.8847\n","Time:  0.3519 Epoch: 004 Iter: 19600/39218 LR: 0.0001000000 Loss content:  0.3633 Loss fft: 10.0016\n","Time:  0.3518 Epoch: 004 Iter: 19700/39218 LR: 0.0001000000 Loss content:  0.3654 Loss fft: 10.0478\n","Time:  0.3516 Epoch: 004 Iter: 19800/39218 LR: 0.0001000000 Loss content:  0.3668 Loss fft:  9.9745\n","Time:  0.3516 Epoch: 004 Iter: 19900/39218 LR: 0.0001000000 Loss content:  0.3664 Loss fft:  9.9816\n","Time:  0.3516 Epoch: 004 Iter: 20000/39218 LR: 0.0001000000 Loss content:  0.3585 Loss fft:  9.7804\n","Time:  0.3518 Epoch: 004 Iter: 20100/39218 LR: 0.0001000000 Loss content:  0.3688 Loss fft: 10.0285\n","Time:  0.3516 Epoch: 004 Iter: 20200/39218 LR: 0.0001000000 Loss content:  0.3770 Loss fft: 10.0273\n","Time:  0.3515 Epoch: 004 Iter: 20300/39218 LR: 0.0001000000 Loss content:  0.3729 Loss fft: 10.0872\n","Time:  0.3518 Epoch: 004 Iter: 20400/39218 LR: 0.0001000000 Loss content:  0.3737 Loss fft:  9.9270\n","Time:  0.3518 Epoch: 004 Iter: 20500/39218 LR: 0.0001000000 Loss content:  0.3699 Loss fft: 10.1165\n","Time:  0.3516 Epoch: 004 Iter: 20600/39218 LR: 0.0001000000 Loss content:  0.3655 Loss fft:  9.8800\n","Time:  0.3518 Epoch: 004 Iter: 20700/39218 LR: 0.0001000000 Loss content:  0.3690 Loss fft:  9.9924\n","Time:  0.3518 Epoch: 004 Iter: 20800/39218 LR: 0.0001000000 Loss content:  0.3651 Loss fft: 10.0069\n","Time:  0.3518 Epoch: 004 Iter: 20900/39218 LR: 0.0001000000 Loss content:  0.3654 Loss fft:  9.9809\n","Time:  0.3516 Epoch: 004 Iter: 21000/39218 LR: 0.0001000000 Loss content:  0.3551 Loss fft:  9.7676\n","Time:  0.3518 Epoch: 004 Iter: 21100/39218 LR: 0.0001000000 Loss content:  0.3697 Loss fft:  9.8998\n","Time:  0.3519 Epoch: 004 Iter: 21200/39218 LR: 0.0001000000 Loss content:  0.3719 Loss fft: 10.1651\n","Time:  0.3519 Epoch: 004 Iter: 21300/39218 LR: 0.0001000000 Loss content:  0.3666 Loss fft:  9.9331\n","Time:  0.3516 Epoch: 004 Iter: 21400/39218 LR: 0.0001000000 Loss content:  0.3731 Loss fft: 10.2372\n","Time:  0.3519 Epoch: 004 Iter: 21500/39218 LR: 0.0001000000 Loss content:  0.3644 Loss fft:  9.9487\n","Time:  0.3518 Epoch: 004 Iter: 21600/39218 LR: 0.0001000000 Loss content:  0.3726 Loss fft: 10.4082\n","Time:  0.3516 Epoch: 004 Iter: 21700/39218 LR: 0.0001000000 Loss content:  0.3716 Loss fft:  9.9365\n","Time:  0.3552 Epoch: 004 Iter: 21800/39218 LR: 0.0001000000 Loss content:  0.3682 Loss fft:  9.8723\n","Time:  0.3518 Epoch: 004 Iter: 21900/39218 LR: 0.0001000000 Loss content:  0.3682 Loss fft: 10.0859\n","Time:  0.3517 Epoch: 004 Iter: 22000/39218 LR: 0.0001000000 Loss content:  0.3665 Loss fft: 10.0505\n","Time:  0.3517 Epoch: 004 Iter: 22100/39218 LR: 0.0001000000 Loss content:  0.3695 Loss fft: 10.0325\n","Time:  0.3516 Epoch: 004 Iter: 22200/39218 LR: 0.0001000000 Loss content:  0.3578 Loss fft:  9.8097\n","Time:  0.3517 Epoch: 004 Iter: 22300/39218 LR: 0.0001000000 Loss content:  0.3726 Loss fft: 10.0324\n","Time:  0.3518 Epoch: 004 Iter: 22400/39218 LR: 0.0001000000 Loss content:  0.3643 Loss fft:  9.8642\n","Time:  0.3515 Epoch: 004 Iter: 22500/39218 LR: 0.0001000000 Loss content:  0.3707 Loss fft: 10.1546\n","Time:  0.3518 Epoch: 004 Iter: 22600/39218 LR: 0.0001000000 Loss content:  0.3679 Loss fft:  9.8938\n","Time:  0.3519 Epoch: 004 Iter: 22700/39218 LR: 0.0001000000 Loss content:  0.3659 Loss fft:  9.9799\n","Time:  0.3518 Epoch: 004 Iter: 22800/39218 LR: 0.0001000000 Loss content:  0.3653 Loss fft:  9.9832\n","Time:  0.3517 Epoch: 004 Iter: 22900/39218 LR: 0.0001000000 Loss content:  0.3660 Loss fft:  9.9317\n","Time:  0.3519 Epoch: 004 Iter: 23000/39218 LR: 0.0001000000 Loss content:  0.3661 Loss fft: 10.0603\n","Time:  0.3517 Epoch: 004 Iter: 23100/39218 LR: 0.0001000000 Loss content:  0.3709 Loss fft:  9.9140\n","Time:  0.3517 Epoch: 004 Iter: 23200/39218 LR: 0.0001000000 Loss content:  0.3678 Loss fft: 10.0708\n","Time:  0.3516 Epoch: 004 Iter: 23300/39218 LR: 0.0001000000 Loss content:  0.3621 Loss fft:  9.9238\n","Time:  0.3518 Epoch: 004 Iter: 23400/39218 LR: 0.0001000000 Loss content:  0.3682 Loss fft:  9.8575\n","Time:  0.3517 Epoch: 004 Iter: 23500/39218 LR: 0.0001000000 Loss content:  0.3664 Loss fft: 10.0300\n","Time:  0.3519 Epoch: 004 Iter: 23600/39218 LR: 0.0001000000 Loss content:  0.3646 Loss fft:  9.8010\n","Time:  0.3519 Epoch: 004 Iter: 23700/39218 LR: 0.0001000000 Loss content:  0.3685 Loss fft:  9.9344\n","Time:  0.3520 Epoch: 004 Iter: 23800/39218 LR: 0.0001000000 Loss content:  0.3575 Loss fft:  9.8601\n","Time:  0.3520 Epoch: 004 Iter: 23900/39218 LR: 0.0001000000 Loss content:  0.3590 Loss fft:  9.9573\n","Time:  0.3515 Epoch: 004 Iter: 24000/39218 LR: 0.0001000000 Loss content:  0.3740 Loss fft:  9.9836\n","Time:  0.3552 Epoch: 004 Iter: 24100/39218 LR: 0.0001000000 Loss content:  0.3651 Loss fft:  9.8774\n","Time:  0.3518 Epoch: 004 Iter: 24200/39218 LR: 0.0001000000 Loss content:  0.3690 Loss fft:  9.9336\n","Time:  0.3518 Epoch: 004 Iter: 24300/39218 LR: 0.0001000000 Loss content:  0.3624 Loss fft:  9.8967\n","Time:  0.3518 Epoch: 004 Iter: 24400/39218 LR: 0.0001000000 Loss content:  0.3685 Loss fft:  9.9856\n","Time:  0.3518 Epoch: 004 Iter: 24500/39218 LR: 0.0001000000 Loss content:  0.3699 Loss fft: 10.1075\n","Time:  0.3518 Epoch: 004 Iter: 24600/39218 LR: 0.0001000000 Loss content:  0.3743 Loss fft:  9.9863\n","Time:  0.3519 Epoch: 004 Iter: 24700/39218 LR: 0.0001000000 Loss content:  0.3729 Loss fft:  9.9350\n","Time:  0.3518 Epoch: 004 Iter: 24800/39218 LR: 0.0001000000 Loss content:  0.3596 Loss fft: 10.0973\n","Time:  0.3517 Epoch: 004 Iter: 24900/39218 LR: 0.0001000000 Loss content:  0.3665 Loss fft:  9.8233\n","Time:  0.3518 Epoch: 004 Iter: 25000/39218 LR: 0.0001000000 Loss content:  0.3607 Loss fft:  9.7531\n","Time:  0.3519 Epoch: 004 Iter: 25100/39218 LR: 0.0001000000 Loss content:  0.3630 Loss fft:  9.9002\n","Time:  0.3518 Epoch: 004 Iter: 25200/39218 LR: 0.0001000000 Loss content:  0.3700 Loss fft: 10.0304\n","Time:  0.3517 Epoch: 004 Iter: 25300/39218 LR: 0.0001000000 Loss content:  0.3692 Loss fft:  9.9305\n","Time:  0.3516 Epoch: 004 Iter: 25400/39218 LR: 0.0001000000 Loss content:  0.3680 Loss fft: 10.0466\n","Time:  0.3520 Epoch: 004 Iter: 25500/39218 LR: 0.0001000000 Loss content:  0.3702 Loss fft: 10.0926\n","Time:  0.3522 Epoch: 004 Iter: 25600/39218 LR: 0.0001000000 Loss content:  0.3713 Loss fft:  9.9599\n","Time:  0.3518 Epoch: 004 Iter: 25700/39218 LR: 0.0001000000 Loss content:  0.3693 Loss fft: 10.0248\n","Time:  0.3517 Epoch: 004 Iter: 25800/39218 LR: 0.0001000000 Loss content:  0.3614 Loss fft:  9.9946\n","Time:  0.3517 Epoch: 004 Iter: 25900/39218 LR: 0.0001000000 Loss content:  0.3666 Loss fft:  9.8265\n","Time:  0.3518 Epoch: 004 Iter: 26000/39218 LR: 0.0001000000 Loss content:  0.3632 Loss fft:  9.8354\n","Time:  0.3515 Epoch: 004 Iter: 26100/39218 LR: 0.0001000000 Loss content:  0.3670 Loss fft: 10.0359\n","Time:  0.3519 Epoch: 004 Iter: 26200/39218 LR: 0.0001000000 Loss content:  0.3644 Loss fft:  9.9809\n","Time:  0.3517 Epoch: 004 Iter: 26300/39218 LR: 0.0001000000 Loss content:  0.3626 Loss fft: 10.0292\n","Time:  0.3552 Epoch: 004 Iter: 26400/39218 LR: 0.0001000000 Loss content:  0.3757 Loss fft:  9.9573\n","Time:  0.3519 Epoch: 004 Iter: 26500/39218 LR: 0.0001000000 Loss content:  0.3793 Loss fft: 10.0537\n","Time:  0.3518 Epoch: 004 Iter: 26600/39218 LR: 0.0001000000 Loss content:  0.3686 Loss fft: 10.0731\n","Time:  0.3518 Epoch: 004 Iter: 26700/39218 LR: 0.0001000000 Loss content:  0.3623 Loss fft:  9.9346\n","Time:  0.3519 Epoch: 004 Iter: 26800/39218 LR: 0.0001000000 Loss content:  0.3736 Loss fft:  9.8278\n","Time:  0.3517 Epoch: 004 Iter: 26900/39218 LR: 0.0001000000 Loss content:  0.3677 Loss fft: 10.0648\n","Time:  0.3519 Epoch: 004 Iter: 27000/39218 LR: 0.0001000000 Loss content:  0.3634 Loss fft:  9.9161\n","Time:  0.3518 Epoch: 004 Iter: 27100/39218 LR: 0.0001000000 Loss content:  0.3580 Loss fft:  9.9611\n","Time:  0.3517 Epoch: 004 Iter: 27200/39218 LR: 0.0001000000 Loss content:  0.3600 Loss fft: 10.0604\n","Time:  0.3517 Epoch: 004 Iter: 27300/39218 LR: 0.0001000000 Loss content:  0.3728 Loss fft:  9.9572\n","Time:  0.3516 Epoch: 004 Iter: 27400/39218 LR: 0.0001000000 Loss content:  0.3709 Loss fft:  9.9769\n","Time:  0.3522 Epoch: 004 Iter: 27500/39218 LR: 0.0001000000 Loss content:  0.3668 Loss fft:  9.9695\n","Time:  0.3516 Epoch: 004 Iter: 27600/39218 LR: 0.0001000000 Loss content:  0.3648 Loss fft:  9.9126\n","Time:  0.3517 Epoch: 004 Iter: 27700/39218 LR: 0.0001000000 Loss content:  0.3590 Loss fft:  9.8154\n","Time:  0.3517 Epoch: 004 Iter: 27800/39218 LR: 0.0001000000 Loss content:  0.3520 Loss fft:  9.8420\n","Time:  0.3515 Epoch: 004 Iter: 27900/39218 LR: 0.0001000000 Loss content:  0.3737 Loss fft: 10.0276\n","Time:  0.3519 Epoch: 004 Iter: 28000/39218 LR: 0.0001000000 Loss content:  0.3655 Loss fft:  9.9616\n","Time:  0.3520 Epoch: 004 Iter: 28100/39218 LR: 0.0001000000 Loss content:  0.3649 Loss fft:  9.8736\n","Time:  0.3517 Epoch: 004 Iter: 28200/39218 LR: 0.0001000000 Loss content:  0.3635 Loss fft:  9.8652\n","Time:  0.3517 Epoch: 004 Iter: 28300/39218 LR: 0.0001000000 Loss content:  0.3659 Loss fft:  9.8702\n","Time:  0.3518 Epoch: 004 Iter: 28400/39218 LR: 0.0001000000 Loss content:  0.3661 Loss fft:  9.7124\n","Time:  0.3519 Epoch: 004 Iter: 28500/39218 LR: 0.0001000000 Loss content:  0.3627 Loss fft:  9.8989\n","Time:  0.3518 Epoch: 004 Iter: 28600/39218 LR: 0.0001000000 Loss content:  0.3570 Loss fft:  9.8092\n","Time:  0.3567 Epoch: 004 Iter: 28700/39218 LR: 0.0001000000 Loss content:  0.3622 Loss fft:  9.7572\n","Time:  0.3519 Epoch: 004 Iter: 28800/39218 LR: 0.0001000000 Loss content:  0.3663 Loss fft:  9.9272\n","Time:  0.3517 Epoch: 004 Iter: 28900/39218 LR: 0.0001000000 Loss content:  0.3743 Loss fft:  9.9776\n","Time:  0.3516 Epoch: 004 Iter: 29000/39218 LR: 0.0001000000 Loss content:  0.3694 Loss fft:  9.9264\n","Time:  0.3516 Epoch: 004 Iter: 29100/39218 LR: 0.0001000000 Loss content:  0.3700 Loss fft: 10.0863\n","Time:  0.3517 Epoch: 004 Iter: 29200/39218 LR: 0.0001000000 Loss content:  0.3577 Loss fft: 10.0407\n","Time:  0.3518 Epoch: 004 Iter: 29300/39218 LR: 0.0001000000 Loss content:  0.3672 Loss fft: 10.1021\n","Time:  0.3516 Epoch: 004 Iter: 29400/39218 LR: 0.0001000000 Loss content:  0.3764 Loss fft: 10.1276\n","Time:  0.3519 Epoch: 004 Iter: 29500/39218 LR: 0.0001000000 Loss content:  0.3607 Loss fft:  9.8253\n","Time:  0.3516 Epoch: 004 Iter: 29600/39218 LR: 0.0001000000 Loss content:  0.3684 Loss fft:  9.9369\n","Time:  0.3515 Epoch: 004 Iter: 29700/39218 LR: 0.0001000000 Loss content:  0.3684 Loss fft:  9.9483\n","Time:  0.3515 Epoch: 004 Iter: 29800/39218 LR: 0.0001000000 Loss content:  0.3633 Loss fft:  9.8633\n","Time:  0.3517 Epoch: 004 Iter: 29900/39218 LR: 0.0001000000 Loss content:  0.3631 Loss fft:  9.9547\n","Time:  0.3514 Epoch: 004 Iter: 30000/39218 LR: 0.0001000000 Loss content:  0.3673 Loss fft:  9.8645\n","Time:  0.3519 Epoch: 004 Iter: 30100/39218 LR: 0.0001000000 Loss content:  0.3618 Loss fft: 10.0539\n","Time:  0.3516 Epoch: 004 Iter: 30200/39218 LR: 0.0001000000 Loss content:  0.3686 Loss fft:  9.8816\n","Time:  0.3515 Epoch: 004 Iter: 30300/39218 LR: 0.0001000000 Loss content:  0.3622 Loss fft:  9.8324\n","Time:  0.3515 Epoch: 004 Iter: 30400/39218 LR: 0.0001000000 Loss content:  0.3658 Loss fft:  9.7941\n","Time:  0.3517 Epoch: 004 Iter: 30500/39218 LR: 0.0001000000 Loss content:  0.3686 Loss fft: 10.0345\n","Time:  0.3517 Epoch: 004 Iter: 30600/39218 LR: 0.0001000000 Loss content:  0.3601 Loss fft:  9.9449\n","Time:  0.3515 Epoch: 004 Iter: 30700/39218 LR: 0.0001000000 Loss content:  0.3597 Loss fft: 10.1150\n","Time:  0.3515 Epoch: 004 Iter: 30800/39218 LR: 0.0001000000 Loss content:  0.3635 Loss fft:  9.8550\n","Time:  0.3551 Epoch: 004 Iter: 30900/39218 LR: 0.0001000000 Loss content:  0.3677 Loss fft:  9.9567\n","Time:  0.3516 Epoch: 004 Iter: 31000/39218 LR: 0.0001000000 Loss content:  0.3584 Loss fft:  9.7761\n","Time:  0.3514 Epoch: 004 Iter: 31100/39218 LR: 0.0001000000 Loss content:  0.3686 Loss fft: 10.0367\n","Time:  0.3516 Epoch: 004 Iter: 31200/39218 LR: 0.0001000000 Loss content:  0.3734 Loss fft: 10.1720\n","Time:  0.3516 Epoch: 004 Iter: 31300/39218 LR: 0.0001000000 Loss content:  0.3763 Loss fft: 10.0032\n","Time:  0.3518 Epoch: 004 Iter: 31400/39218 LR: 0.0001000000 Loss content:  0.3717 Loss fft: 10.1947\n","Time:  0.3516 Epoch: 004 Iter: 31500/39218 LR: 0.0001000000 Loss content:  0.3583 Loss fft:  9.8679\n","Time:  0.3517 Epoch: 004 Iter: 31600/39218 LR: 0.0001000000 Loss content:  0.3671 Loss fft:  9.9747\n","Time:  0.3516 Epoch: 004 Iter: 31700/39218 LR: 0.0001000000 Loss content:  0.3696 Loss fft:  9.9932\n","Time:  0.3515 Epoch: 004 Iter: 31800/39218 LR: 0.0001000000 Loss content:  0.3658 Loss fft:  9.9693\n","Time:  0.3514 Epoch: 004 Iter: 31900/39218 LR: 0.0001000000 Loss content:  0.3679 Loss fft:  9.8950\n","Time:  0.3515 Epoch: 004 Iter: 32000/39218 LR: 0.0001000000 Loss content:  0.3637 Loss fft:  9.9737\n","Time:  0.3514 Epoch: 004 Iter: 32100/39218 LR: 0.0001000000 Loss content:  0.3587 Loss fft:  9.7404\n","Time:  0.3514 Epoch: 004 Iter: 32200/39218 LR: 0.0001000000 Loss content:  0.3651 Loss fft:  9.9871\n","Time:  0.3514 Epoch: 004 Iter: 32300/39218 LR: 0.0001000000 Loss content:  0.3598 Loss fft:  9.6737\n","Time:  0.3516 Epoch: 004 Iter: 32400/39218 LR: 0.0001000000 Loss content:  0.3618 Loss fft:  9.9376\n","Time:  0.3514 Epoch: 004 Iter: 32500/39218 LR: 0.0001000000 Loss content:  0.3671 Loss fft: 10.0442\n","Time:  0.3516 Epoch: 004 Iter: 32600/39218 LR: 0.0001000000 Loss content:  0.3709 Loss fft: 10.1055\n","Time:  0.3517 Epoch: 004 Iter: 32700/39218 LR: 0.0001000000 Loss content:  0.3715 Loss fft: 10.0300\n","Time:  0.3516 Epoch: 004 Iter: 32800/39218 LR: 0.0001000000 Loss content:  0.3603 Loss fft:  9.9538\n","Time:  0.3513 Epoch: 004 Iter: 32900/39218 LR: 0.0001000000 Loss content:  0.3650 Loss fft:  9.9936\n","Time:  0.3514 Epoch: 004 Iter: 33000/39218 LR: 0.0001000000 Loss content:  0.3645 Loss fft:  9.9421\n","Time:  0.3514 Epoch: 004 Iter: 33100/39218 LR: 0.0001000000 Loss content:  0.3599 Loss fft:  9.8673\n","Time:  0.3548 Epoch: 004 Iter: 33200/39218 LR: 0.0001000000 Loss content:  0.3631 Loss fft:  9.8088\n","Time:  0.3517 Epoch: 004 Iter: 33300/39218 LR: 0.0001000000 Loss content:  0.3668 Loss fft:  9.9164\n","Time:  0.3515 Epoch: 004 Iter: 33400/39218 LR: 0.0001000000 Loss content:  0.3633 Loss fft:  9.9166\n","Time:  0.3515 Epoch: 004 Iter: 33500/39218 LR: 0.0001000000 Loss content:  0.3580 Loss fft:  9.8818\n","Time:  0.3515 Epoch: 004 Iter: 33600/39218 LR: 0.0001000000 Loss content:  0.3646 Loss fft:  9.9155\n","Time:  0.3515 Epoch: 004 Iter: 33700/39218 LR: 0.0001000000 Loss content:  0.3681 Loss fft: 10.0858\n","Time:  0.3515 Epoch: 004 Iter: 33800/39218 LR: 0.0001000000 Loss content:  0.3623 Loss fft:  9.8543\n","Time:  0.3514 Epoch: 004 Iter: 33900/39218 LR: 0.0001000000 Loss content:  0.3526 Loss fft: 10.0967\n","Time:  0.3515 Epoch: 004 Iter: 34000/39218 LR: 0.0001000000 Loss content:  0.3693 Loss fft: 10.0768\n","Time:  0.3516 Epoch: 004 Iter: 34100/39218 LR: 0.0001000000 Loss content:  0.3610 Loss fft:  9.8145\n","Time:  0.3515 Epoch: 004 Iter: 34200/39218 LR: 0.0001000000 Loss content:  0.3582 Loss fft:  9.6547\n","Time:  0.3516 Epoch: 004 Iter: 34300/39218 LR: 0.0001000000 Loss content:  0.3611 Loss fft:  9.9123\n","Time:  0.3515 Epoch: 004 Iter: 34400/39218 LR: 0.0001000000 Loss content:  0.3593 Loss fft:  9.8655\n","Time:  0.3516 Epoch: 004 Iter: 34500/39218 LR: 0.0001000000 Loss content:  0.3599 Loss fft:  9.8545\n","Time:  0.3518 Epoch: 004 Iter: 34600/39218 LR: 0.0001000000 Loss content:  0.3629 Loss fft:  9.9537\n","Time:  0.3516 Epoch: 004 Iter: 34700/39218 LR: 0.0001000000 Loss content:  0.3728 Loss fft: 10.0899\n","Time:  0.3516 Epoch: 004 Iter: 34800/39218 LR: 0.0001000000 Loss content:  0.3632 Loss fft:  9.9470\n","Time:  0.3516 Epoch: 004 Iter: 34900/39218 LR: 0.0001000000 Loss content:  0.3647 Loss fft:  9.9622\n","Time:  0.3516 Epoch: 004 Iter: 35000/39218 LR: 0.0001000000 Loss content:  0.3595 Loss fft:  9.8325\n","Time:  0.3518 Epoch: 004 Iter: 35100/39218 LR: 0.0001000000 Loss content:  0.3640 Loss fft:  9.9310\n","Time:  0.3515 Epoch: 004 Iter: 35200/39218 LR: 0.0001000000 Loss content:  0.3680 Loss fft: 10.0126\n","Time:  0.3517 Epoch: 004 Iter: 35300/39218 LR: 0.0001000000 Loss content:  0.3596 Loss fft:  9.9790\n","Time:  0.3515 Epoch: 004 Iter: 35400/39218 LR: 0.0001000000 Loss content:  0.3623 Loss fft:  9.9873\n","Time:  0.3550 Epoch: 004 Iter: 35500/39218 LR: 0.0001000000 Loss content:  0.3639 Loss fft:  9.9164\n","Time:  0.3514 Epoch: 004 Iter: 35600/39218 LR: 0.0001000000 Loss content:  0.3622 Loss fft:  9.8933\n","Time:  0.3517 Epoch: 004 Iter: 35700/39218 LR: 0.0001000000 Loss content:  0.3690 Loss fft:  9.8606\n"]}],"source":["import os\n","import torch\n","import torchvision.transforms as transforms\n","# from torch.backends import cudnn # Uncomment if you need it\n","\n","class Args:\n","    model_name = 'IRNeXt'\n","    mode = 'train'\n","    data_dir = path\n","\n","    # Train\n","    batch_size = 8\n","    learning_rate = 1e-4\n","    weight_decay = 0\n","    num_epoch = 5\n","    print_freq = 100\n","    num_worker = 8\n","    save_freq = 10\n","    valid_freq = 10\n","    momentum_value = 0.4\n","\n","    resume = '/content/drive/MyDrive/OUTMYresults0_4/IRNeXt/ITS/model.pkl'\n","    #/content/drive/MyDrive/OUTMYresults0_4/IRNeXt/ITS/model.pkl\n","    # Test\n","    test_model = '/content/drive/MyDrive/OUTMYresults0_4/IRNeXt/ITS/Final.pkl'\n","    save_image = False\n","\n","    # Directories (set these as per your requirement)\n","    model_save_dir = os.path.join('/content/drive/MyDrive/OUTMYresults0_4/', 'IRNeXt', 'ITS/')\n","    result_dir = os.path.join('/content/drive/MyDrive/OUTMYresults0_4/', model_name, 'test')\n","\n","def main(args):\n","    # CUDNN\n","    # cudnn.benchmark = True # Uncomment if you need it\n","\n","    if not os.path.exists('/content/drive/MyDrive/OUTMYresults0_4/'):\n","        os.makedirs(args.model_save_dir)\n","    if not os.path.exists('/content/drive/MyDrive/OUTMYresults0_4/' + args.model_name + '/'):\n","        os.makedirs('/content/drive/MyDrive/OUTMYresults0_4/' + args.model_name + '/')\n","    if not os.path.exists(args.model_save_dir):\n","        os.makedirs(args.model_save_dir)\n","    if not os.path.exists(args.result_dir):\n","        os.makedirs(args.result_dir)\n","\n","    model = build_net()  # Make sure to define build_net or import it if it's defined elsewhere\n","    print(model)\n","\n","    if torch.cuda.is_available():\n","        model.cuda()\n","    if args.mode == 'train':\n","        _train(model, args)  # Make sure to define _train or import it if it's defined elsewhere\n","\n","    elif args.mode == 'test':\n","        _eval(model, args)   # Make sure to define _eval or import it if it's defined elsewhere\n","\n","# Replace parser.parse_args() with an instance of the Args class\n","args = Args()\n","if not os.path.exists(args.model_save_dir):\n","    os.makedirs(args.model_save_dir)\n","# Copying files (make sure these paths are correct)\n","command = 'cp ' + 'models/layers.py ' + args.model_save_dir\n","os.system(command)\n","command = 'cp ' + 'models/IRNeXt.py ' + args.model_save_dir\n","os.system(command)\n","command = 'cp ' + 'train.py ' + args.model_save_dir\n","os.system(command)\n","command = 'cp ' + 'main.py ' + args.model_save_dir\n","os.system(command)\n","print(args)\n","main(args)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1700736698522,"user":{"displayName":"General","userId":"01731632481034566404"},"user_tz":-480},"id":"18oaNy3YCgdT"},"outputs":[],"source":["state_last = torch.load('/content/drive/MyDrive/MYresults/IRNeXt/ITS/model.pkl')"]},{"cell_type":"markdown","metadata":{"id":"wP44pkAG8ISn"},"source":["Test\n","=="]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26469,"status":"ok","timestamp":1700736809203,"user":{"displayName":"General","userId":"01731632481034566404"},"user_tz":-480},"id":"eXDFmei28IfB","outputId":"df76b7bf-8ee4-4441-e4e6-a741906e5122"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u003c__main__.Args object at 0x7ce666656b60\u003e\n","IRNeXt(\n","  (Encoder): ModuleList(\n","    (0): EBlock(\n","      (layers): Sequential(\n","        (0): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (1): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (2): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (3): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): DeepPoolLayer(\n","              (pools): ModuleList(\n","                (0): AvgPool2d(kernel_size=8, stride=8, padding=0)\n","                (1): AvgPool2d(kernel_size=4, stride=4, padding=0)\n","                (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","              )\n","              (convs): ModuleList(\n","                (0-2): 3 x Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              )\n","              (dynas): ModuleList(\n","                (0-2): 3 x dynamic_filter(\n","                  (conv): Conv2d(32, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (act): Tanh()\n","                  (pad): ReflectionPad2d((1, 1, 1, 1))\n","                  (ap): AdaptiveAvgPool2d(output_size=(1, 1))\n","                  (gap): AdaptiveAvgPool2d(output_size=1)\n","                )\n","              )\n","              (relu): GELU(approximate='none')\n","              (conv_sum): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            )\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (1): EBlock(\n","      (layers): Sequential(\n","        (0): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (1): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (2): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (3): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): DeepPoolLayer(\n","              (pools): ModuleList(\n","                (0): AvgPool2d(kernel_size=8, stride=8, padding=0)\n","                (1): AvgPool2d(kernel_size=4, stride=4, padding=0)\n","                (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","              )\n","              (convs): ModuleList(\n","                (0-2): 3 x Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              )\n","              (dynas): ModuleList(\n","                (0-2): 3 x dynamic_filter(\n","                  (conv): Conv2d(64, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (act): Tanh()\n","                  (pad): ReflectionPad2d((1, 1, 1, 1))\n","                  (ap): AdaptiveAvgPool2d(output_size=(1, 1))\n","                  (gap): AdaptiveAvgPool2d(output_size=1)\n","                )\n","              )\n","              (relu): GELU(approximate='none')\n","              (conv_sum): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            )\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (2): EBlock(\n","      (layers): Sequential(\n","        (0): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (1): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (2): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (3): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): DeepPoolLayer(\n","              (pools): ModuleList(\n","                (0): AvgPool2d(kernel_size=8, stride=8, padding=0)\n","                (1): AvgPool2d(kernel_size=4, stride=4, padding=0)\n","                (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","              )\n","              (convs): ModuleList(\n","                (0-2): 3 x Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              )\n","              (dynas): ModuleList(\n","                (0-2): 3 x dynamic_filter(\n","                  (conv): Conv2d(128, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (act): Tanh()\n","                  (pad): ReflectionPad2d((1, 1, 1, 1))\n","                  (ap): AdaptiveAvgPool2d(output_size=(1, 1))\n","                  (gap): AdaptiveAvgPool2d(output_size=1)\n","                )\n","              )\n","              (relu): GELU(approximate='none')\n","              (conv_sum): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            )\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (feat_extract): ModuleList(\n","    (0): BasicConv(\n","      (main): Sequential(\n","        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GELU(approximate='none')\n","      )\n","    )\n","    (1): BasicConv(\n","      (main): Sequential(\n","        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (1): GELU(approximate='none')\n","      )\n","    )\n","    (2): BasicConv(\n","      (main): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (1): GELU(approximate='none')\n","      )\n","    )\n","    (3): BasicConv(\n","      (main): Sequential(\n","        (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","        (1): GELU(approximate='none')\n","      )\n","    )\n","    (4): BasicConv(\n","      (main): Sequential(\n","        (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","        (1): GELU(approximate='none')\n","      )\n","    )\n","    (5): BasicConv(\n","      (main): Sequential(\n","        (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","  )\n","  (Decoder): ModuleList(\n","    (0): DBlock(\n","      (layers): Sequential(\n","        (0): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (1): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (2): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (3): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): DeepPoolLayer(\n","              (pools): ModuleList(\n","                (0): AvgPool2d(kernel_size=8, stride=8, padding=0)\n","                (1): AvgPool2d(kernel_size=4, stride=4, padding=0)\n","                (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","              )\n","              (convs): ModuleList(\n","                (0-2): 3 x Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              )\n","              (dynas): ModuleList(\n","                (0-2): 3 x dynamic_filter(\n","                  (conv): Conv2d(128, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (act): Tanh()\n","                  (pad): ReflectionPad2d((1, 1, 1, 1))\n","                  (ap): AdaptiveAvgPool2d(output_size=(1, 1))\n","                  (gap): AdaptiveAvgPool2d(output_size=1)\n","                )\n","              )\n","              (relu): GELU(approximate='none')\n","              (conv_sum): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            )\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (1): DBlock(\n","      (layers): Sequential(\n","        (0): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (1): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (2): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (3): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): DeepPoolLayer(\n","              (pools): ModuleList(\n","                (0): AvgPool2d(kernel_size=8, stride=8, padding=0)\n","                (1): AvgPool2d(kernel_size=4, stride=4, padding=0)\n","                (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","              )\n","              (convs): ModuleList(\n","                (0-2): 3 x Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              )\n","              (dynas): ModuleList(\n","                (0-2): 3 x dynamic_filter(\n","                  (conv): Conv2d(64, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (act): Tanh()\n","                  (pad): ReflectionPad2d((1, 1, 1, 1))\n","                  (ap): AdaptiveAvgPool2d(output_size=(1, 1))\n","                  (gap): AdaptiveAvgPool2d(output_size=1)\n","                )\n","              )\n","              (relu): GELU(approximate='none')\n","              (conv_sum): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            )\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (2): DBlock(\n","      (layers): Sequential(\n","        (0): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (1): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (2): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): Identity()\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","        (3): ResBlock(\n","          (main): Sequential(\n","            (0): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (1): GELU(approximate='none')\n","              )\n","            )\n","            (1): DeepPoolLayer(\n","              (pools): ModuleList(\n","                (0): AvgPool2d(kernel_size=8, stride=8, padding=0)\n","                (1): AvgPool2d(kernel_size=4, stride=4, padding=0)\n","                (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","              )\n","              (convs): ModuleList(\n","                (0-2): 3 x Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              )\n","              (dynas): ModuleList(\n","                (0-2): 3 x dynamic_filter(\n","                  (conv): Conv2d(32, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (act): Tanh()\n","                  (pad): ReflectionPad2d((1, 1, 1, 1))\n","                  (ap): AdaptiveAvgPool2d(output_size=(1, 1))\n","                  (gap): AdaptiveAvgPool2d(output_size=1)\n","                )\n","              )\n","              (relu): GELU(approximate='none')\n","              (conv_sum): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            )\n","            (2): BasicConv(\n","              (main): Sequential(\n","                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","              )\n","            )\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (Convs): ModuleList(\n","    (0): BasicConv(\n","      (main): Sequential(\n","        (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n","        (1): GELU(approximate='none')\n","      )\n","    )\n","    (1): BasicConv(\n","      (main): Sequential(\n","        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n","        (1): GELU(approximate='none')\n","      )\n","    )\n","  )\n","  (ConvsOut): ModuleList(\n","    (0): BasicConv(\n","      (main): Sequential(\n","        (0): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (1): BasicConv(\n","      (main): Sequential(\n","        (0): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","  )\n","  (FAM1): FAM(\n","    (merge): BasicConv(\n","      (main): Sequential(\n","        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","  )\n","  (SCM1): SCM(\n","    (main): Sequential(\n","      (0): BasicConv(\n","        (main): Sequential(\n","          (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): GELU(approximate='none')\n","        )\n","      )\n","      (1): BasicConv(\n","        (main): Sequential(\n","          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n","          (1): GELU(approximate='none')\n","        )\n","      )\n","      (2): BasicConv(\n","        (main): Sequential(\n","          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): GELU(approximate='none')\n","        )\n","      )\n","      (3): BasicConv(\n","        (main): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n","    )\n","  )\n","  (FAM2): FAM(\n","    (merge): BasicConv(\n","      (main): Sequential(\n","        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","  )\n","  (SCM2): SCM(\n","    (main): Sequential(\n","      (0): BasicConv(\n","        (main): Sequential(\n","          (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): GELU(approximate='none')\n","        )\n","      )\n","      (1): BasicConv(\n","        (main): Sequential(\n","          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n","          (1): GELU(approximate='none')\n","        )\n","      )\n","      (2): BasicConv(\n","        (main): Sequential(\n","          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): GELU(approximate='none')\n","        )\n","      )\n","      (3): BasicConv(\n","        (main): Sequential(\n","          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n","    )\n","  )\n",")\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["1 iter PSNR_dehazing: 15.18 ssim: 0.856310\n","1 iter PSNR: 15.18 time: 0.366361\n","2 iter PSNR_dehazing: 18.61 ssim: 0.926677\n","2 iter PSNR: 18.61 time: 0.022019\n","3 iter PSNR_dehazing: 16.26 ssim: 0.923427\n","3 iter PSNR: 16.26 time: 0.021960\n","4 iter PSNR_dehazing: 19.78 ssim: 0.948012\n","4 iter PSNR: 19.78 time: 0.021619\n","5 iter PSNR_dehazing: 18.39 ssim: 0.905726\n","5 iter PSNR: 18.39 time: 0.022430\n","6 iter PSNR_dehazing: 12.19 ssim: 0.760770\n","6 iter PSNR: 12.19 time: 0.022144\n","7 iter PSNR_dehazing: 13.30 ssim: 0.781013\n","7 iter PSNR: 13.30 time: 0.032222\n","8 iter PSNR_dehazing: 9.56 ssim: 0.615302\n","8 iter PSNR: 9.56 time: 0.022871\n","9 iter PSNR_dehazing: 11.21 ssim: 0.639268\n","9 iter PSNR: 11.21 time: 0.021224\n","10 iter PSNR_dehazing: 15.78 ssim: 0.891968\n","10 iter PSNR: 15.78 time: 0.021543\n","11 iter PSNR_dehazing: 20.69 ssim: 0.959315\n","11 iter PSNR: 20.69 time: 0.021493\n","12 iter PSNR_dehazing: 16.91 ssim: 0.838531\n","12 iter PSNR: 16.91 time: 0.023386\n","13 iter PSNR_dehazing: 13.50 ssim: 0.823123\n","13 iter PSNR: 13.50 time: 0.022761\n","14 iter PSNR_dehazing: 13.96 ssim: 0.799334\n","14 iter PSNR: 13.96 time: 0.022381\n","15 iter PSNR_dehazing: 20.07 ssim: 0.955834\n","15 iter PSNR: 20.07 time: 0.023146\n","16 iter PSNR_dehazing: 20.64 ssim: 0.960485\n","16 iter PSNR: 20.64 time: 0.023668\n","17 iter PSNR_dehazing: 13.66 ssim: 0.725143\n","17 iter PSNR: 13.66 time: 0.026497\n","18 iter PSNR_dehazing: 16.93 ssim: 0.688182\n","18 iter PSNR: 16.93 time: 0.027758\n","19 iter PSNR_dehazing: 12.54 ssim: 0.743128\n","19 iter PSNR: 12.54 time: 0.028233\n","20 iter PSNR_dehazing: 15.61 ssim: 0.838140\n","20 iter PSNR: 15.61 time: 0.022588\n","21 iter PSNR_dehazing: 16.91 ssim: 0.891932\n","21 iter PSNR: 16.91 time: 0.033505\n","22 iter PSNR_dehazing: 13.91 ssim: 0.803800\n","22 iter PSNR: 13.91 time: 0.022660\n","23 iter PSNR_dehazing: 19.40 ssim: 0.949098\n","23 iter PSNR: 19.40 time: 0.023449\n","24 iter PSNR_dehazing: 13.93 ssim: 0.807805\n","24 iter PSNR: 13.93 time: 0.021787\n","25 iter PSNR_dehazing: 18.32 ssim: 0.950757\n","25 iter PSNR: 18.32 time: 0.028416\n","26 iter PSNR_dehazing: 12.22 ssim: 0.713376\n","26 iter PSNR: 12.22 time: 0.022774\n","27 iter PSNR_dehazing: 17.00 ssim: 0.910006\n","27 iter PSNR: 17.00 time: 0.021834\n","28 iter PSNR_dehazing: 13.66 ssim: 0.810554\n","28 iter PSNR: 13.66 time: 0.033019\n","29 iter PSNR_dehazing: 15.59 ssim: 0.896061\n","29 iter PSNR: 15.59 time: 0.021505\n","30 iter PSNR_dehazing: 12.45 ssim: 0.742763\n","30 iter PSNR: 12.45 time: 0.021877\n","31 iter PSNR_dehazing: 16.70 ssim: 0.860755\n","31 iter PSNR: 16.70 time: 0.021585\n","32 iter PSNR_dehazing: 12.55 ssim: 0.815152\n","32 iter PSNR: 12.55 time: 0.021806\n","33 iter PSNR_dehazing: 17.36 ssim: 0.917394\n","33 iter PSNR: 17.36 time: 0.025304\n","34 iter PSNR_dehazing: 13.15 ssim: 0.728602\n","34 iter PSNR: 13.15 time: 0.021492\n","35 iter PSNR_dehazing: 14.67 ssim: 0.793810\n","35 iter PSNR: 14.67 time: 0.021314\n","36 iter PSNR_dehazing: 11.93 ssim: 0.754463\n","36 iter PSNR: 11.93 time: 0.021611\n","37 iter PSNR_dehazing: 13.07 ssim: 0.755669\n","37 iter PSNR: 13.07 time: 0.022357\n","38 iter PSNR_dehazing: 13.89 ssim: 0.780026\n","38 iter PSNR: 13.89 time: 0.021984\n","39 iter PSNR_dehazing: 15.94 ssim: 0.827474\n","39 iter PSNR: 15.94 time: 0.021335\n","40 iter PSNR_dehazing: 15.01 ssim: 0.800057\n","40 iter PSNR: 15.01 time: 0.021404\n","41 iter PSNR_dehazing: 20.38 ssim: 0.955000\n","41 iter PSNR: 20.38 time: 0.022779\n","42 iter PSNR_dehazing: 12.77 ssim: 0.720533\n","42 iter PSNR: 12.77 time: 0.022128\n","43 iter PSNR_dehazing: 15.57 ssim: 0.882806\n","43 iter PSNR: 15.57 time: 0.021555\n","44 iter PSNR_dehazing: 12.10 ssim: 0.735839\n","44 iter PSNR: 12.10 time: 0.021339\n","45 iter PSNR_dehazing: 13.40 ssim: 0.739689\n","45 iter PSNR: 13.40 time: 0.022982\n","46 iter PSNR_dehazing: 10.51 ssim: 0.704747\n","46 iter PSNR: 10.51 time: 0.021520\n","47 iter PSNR_dehazing: 12.15 ssim: 0.732934\n","47 iter PSNR: 12.15 time: 0.021504\n","48 iter PSNR_dehazing: 12.14 ssim: 0.748767\n","48 iter PSNR: 12.14 time: 0.021334\n","49 iter PSNR_dehazing: 13.55 ssim: 0.781041\n","49 iter PSNR: 13.55 time: 0.023088\n","50 iter PSNR_dehazing: 19.47 ssim: 0.924105\n","50 iter PSNR: 19.47 time: 0.023973\n","51 iter PSNR_dehazing: 12.72 ssim: 0.784663\n","51 iter PSNR: 12.72 time: 0.021830\n","52 iter PSNR_dehazing: 13.23 ssim: 0.789189\n","52 iter PSNR: 13.23 time: 0.020900\n","53 iter PSNR_dehazing: 15.55 ssim: 0.882141\n","53 iter PSNR: 15.55 time: 0.022592\n","54 iter PSNR_dehazing: 15.05 ssim: 0.818784\n","54 iter PSNR: 15.05 time: 0.020892\n","55 iter PSNR_dehazing: 13.25 ssim: 0.789027\n","55 iter PSNR: 13.25 time: 0.020840\n","56 iter PSNR_dehazing: 14.83 ssim: 0.682141\n","56 iter PSNR: 14.83 time: 0.021298\n","57 iter PSNR_dehazing: 12.77 ssim: 0.695209\n","57 iter PSNR: 12.77 time: 0.021142\n","58 iter PSNR_dehazing: 14.08 ssim: 0.792417\n","58 iter PSNR: 14.08 time: 0.023764\n","59 iter PSNR_dehazing: 12.54 ssim: 0.731176\n","59 iter PSNR: 12.54 time: 0.021553\n","60 iter PSNR_dehazing: 11.35 ssim: 0.730152\n","60 iter PSNR: 11.35 time: 0.020830\n","61 iter PSNR_dehazing: 13.94 ssim: 0.729105\n","61 iter PSNR: 13.94 time: 0.021488\n","62 iter PSNR_dehazing: 14.79 ssim: 0.822066\n","62 iter PSNR: 14.79 time: 0.022175\n","63 iter PSNR_dehazing: 16.90 ssim: 0.908961\n","63 iter PSNR: 16.90 time: 0.021427\n","64 iter PSNR_dehazing: 14.63 ssim: 0.864179\n","64 iter PSNR: 14.63 time: 0.021293\n","65 iter PSNR_dehazing: 16.01 ssim: 0.885467\n","65 iter PSNR: 16.01 time: 0.021252\n","66 iter PSNR_dehazing: 12.64 ssim: 0.741424\n","66 iter PSNR: 12.64 time: 0.023424\n","67 iter PSNR_dehazing: 16.16 ssim: 0.825130\n","67 iter PSNR: 16.16 time: 0.021331\n","68 iter PSNR_dehazing: 12.60 ssim: 0.699007\n","68 iter PSNR: 12.60 time: 0.020938\n","69 iter PSNR_dehazing: 13.91 ssim: 0.767120\n","69 iter PSNR: 13.91 time: 0.020926\n","70 iter PSNR_dehazing: 13.03 ssim: 0.798697\n","70 iter PSNR: 13.03 time: 0.022533\n","71 iter PSNR_dehazing: 14.94 ssim: 0.794281\n","71 iter PSNR: 14.94 time: 0.021420\n","72 iter PSNR_dehazing: 12.44 ssim: 0.785098\n","72 iter PSNR: 12.44 time: 0.021453\n","73 iter PSNR_dehazing: 15.65 ssim: 0.885176\n","73 iter PSNR: 15.65 time: 0.021502\n","74 iter PSNR_dehazing: 14.21 ssim: 0.844629\n","74 iter PSNR: 14.21 time: 0.022397\n","75 iter PSNR_dehazing: 13.29 ssim: 0.784220\n","75 iter PSNR: 13.29 time: 0.021761\n","76 iter PSNR_dehazing: 20.03 ssim: 0.961267\n","76 iter PSNR: 20.03 time: 0.021302\n","77 iter PSNR_dehazing: 13.27 ssim: 0.806485\n","77 iter PSNR: 13.27 time: 0.021560\n","78 iter PSNR_dehazing: 16.30 ssim: 0.866339\n","78 iter PSNR: 16.30 time: 0.026205\n","79 iter PSNR_dehazing: 14.97 ssim: 0.817088\n","79 iter PSNR: 14.97 time: 0.021455\n","80 iter PSNR_dehazing: 19.12 ssim: 0.957374\n","80 iter PSNR: 19.12 time: 0.021762\n","81 iter PSNR_dehazing: 11.66 ssim: 0.720106\n","81 iter PSNR: 11.66 time: 0.023464\n","82 iter PSNR_dehazing: 12.01 ssim: 0.735884\n","82 iter PSNR: 12.01 time: 0.023048\n","83 iter PSNR_dehazing: 15.45 ssim: 0.871110\n","83 iter PSNR: 15.45 time: 0.021245\n","84 iter PSNR_dehazing: 18.50 ssim: 0.884121\n","84 iter PSNR: 18.50 time: 0.022393\n","85 iter PSNR_dehazing: 16.20 ssim: 0.866116\n","85 iter PSNR: 16.20 time: 0.021558\n","86 iter PSNR_dehazing: 14.44 ssim: 0.763258\n","86 iter PSNR: 14.44 time: 0.022800\n","87 iter PSNR_dehazing: 11.71 ssim: 0.775809\n","87 iter PSNR: 11.71 time: 0.021427\n","88 iter PSNR_dehazing: 18.43 ssim: 0.943095\n","88 iter PSNR: 18.43 time: 0.021263\n","89 iter PSNR_dehazing: 17.87 ssim: 0.944529\n","89 iter PSNR: 17.87 time: 0.021476\n","90 iter PSNR_dehazing: 12.68 ssim: 0.698925\n","90 iter PSNR: 12.68 time: 0.023631\n","91 iter PSNR_dehazing: 9.10 ssim: 0.612427\n","91 iter PSNR: 9.10 time: 0.021556\n","92 iter PSNR_dehazing: 17.06 ssim: 0.897947\n","92 iter PSNR: 17.06 time: 0.021682\n","93 iter PSNR_dehazing: 16.74 ssim: 0.885048\n","93 iter PSNR: 16.74 time: 0.021319\n","94 iter PSNR_dehazing: 12.03 ssim: 0.649187\n","94 iter PSNR: 12.03 time: 0.022406\n","95 iter PSNR_dehazing: 16.04 ssim: 0.905920\n","95 iter PSNR: 16.04 time: 0.021073\n","96 iter PSNR_dehazing: 15.50 ssim: 0.681679\n","96 iter PSNR: 15.50 time: 0.021395\n","97 iter PSNR_dehazing: 14.90 ssim: 0.785617\n","97 iter PSNR: 14.90 time: 0.024705\n","98 iter PSNR_dehazing: 13.28 ssim: 0.832895\n","98 iter PSNR: 13.28 time: 0.022878\n","99 iter PSNR_dehazing: 16.66 ssim: 0.923171\n","99 iter PSNR: 16.66 time: 0.021523\n","100 iter PSNR_dehazing: 11.30 ssim: 0.723065\n","100 iter PSNR: 11.30 time: 0.021360\n","101 iter PSNR_dehazing: 15.05 ssim: 0.871137\n","101 iter PSNR: 15.05 time: 0.021232\n","102 iter PSNR_dehazing: 18.53 ssim: 0.913876\n","102 iter PSNR: 18.53 time: 0.022466\n","103 iter PSNR_dehazing: 13.86 ssim: 0.837085\n","103 iter PSNR: 13.86 time: 0.022266\n","104 iter PSNR_dehazing: 14.54 ssim: 0.725647\n","104 iter PSNR: 14.54 time: 0.021382\n","105 iter PSNR_dehazing: 17.77 ssim: 0.910143\n","105 iter PSNR: 17.77 time: 0.021411\n","106 iter PSNR_dehazing: 13.63 ssim: 0.787158\n","106 iter PSNR: 13.63 time: 0.027612\n","107 iter PSNR_dehazing: 13.95 ssim: 0.833679\n","107 iter PSNR: 13.95 time: 0.021545\n","108 iter PSNR_dehazing: 13.24 ssim: 0.759902\n","108 iter PSNR: 13.24 time: 0.021794\n","109 iter PSNR_dehazing: 14.64 ssim: 0.768331\n","109 iter PSNR: 14.64 time: 0.021098\n","110 iter PSNR_dehazing: 15.72 ssim: 0.894227\n","110 iter PSNR: 15.72 time: 0.021842\n","111 iter PSNR_dehazing: 9.75 ssim: 0.714161\n","111 iter PSNR: 9.75 time: 0.021277\n","112 iter PSNR_dehazing: 12.52 ssim: 0.738919\n","112 iter PSNR: 12.52 time: 0.021290\n","113 iter PSNR_dehazing: 14.81 ssim: 0.848038\n","113 iter PSNR: 14.81 time: 0.021033\n","114 iter PSNR_dehazing: 12.78 ssim: 0.777385\n","114 iter PSNR: 12.78 time: 0.022742\n","115 iter PSNR_dehazing: 17.27 ssim: 0.884685\n","115 iter PSNR: 17.27 time: 0.021365\n","116 iter PSNR_dehazing: 14.88 ssim: 0.868937\n","116 iter PSNR: 14.88 time: 0.020957\n","117 iter PSNR_dehazing: 14.81 ssim: 0.878124\n","117 iter PSNR: 14.81 time: 0.021024\n","118 iter PSNR_dehazing: 12.25 ssim: 0.760681\n","118 iter PSNR: 12.25 time: 0.021061\n","119 iter PSNR_dehazing: 15.74 ssim: 0.887913\n","119 iter PSNR: 15.74 time: 0.020910\n","120 iter PSNR_dehazing: 8.50 ssim: 0.595354\n","120 iter PSNR: 8.50 time: 0.021072\n","121 iter PSNR_dehazing: 15.72 ssim: 0.855327\n","121 iter PSNR: 15.72 time: 0.021277\n","122 iter PSNR_dehazing: 12.34 ssim: 0.710037\n","122 iter PSNR: 12.34 time: 0.021138\n","123 iter PSNR_dehazing: 10.19 ssim: 0.630956\n","123 iter PSNR: 10.19 time: 0.021369\n","124 iter PSNR_dehazing: 13.94 ssim: 0.705703\n","124 iter PSNR: 13.94 time: 0.021528\n","125 iter PSNR_dehazing: 16.54 ssim: 0.843400\n","125 iter PSNR: 16.54 time: 0.021120\n","126 iter PSNR_dehazing: 18.71 ssim: 0.939682\n","126 iter PSNR: 18.71 time: 0.020961\n","127 iter PSNR_dehazing: 19.64 ssim: 0.941404\n","127 iter PSNR: 19.64 time: 0.021020\n","128 iter PSNR_dehazing: 15.33 ssim: 0.879667\n","128 iter PSNR: 15.33 time: 0.021111\n","129 iter PSNR_dehazing: 18.89 ssim: 0.887320\n","129 iter PSNR: 18.89 time: 0.021268\n","130 iter PSNR_dehazing: 12.01 ssim: 0.732041\n","130 iter PSNR: 12.01 time: 0.021089\n","131 iter PSNR_dehazing: 14.60 ssim: 0.877262\n","131 iter PSNR: 14.60 time: 0.021128\n","132 iter PSNR_dehazing: 16.92 ssim: 0.899542\n","132 iter PSNR: 16.92 time: 0.021109\n","133 iter PSNR_dehazing: 10.20 ssim: 0.619016\n","133 iter PSNR: 10.20 time: 0.020652\n","134 iter PSNR_dehazing: 13.93 ssim: 0.878367\n","134 iter PSNR: 13.93 time: 0.020752\n","135 iter PSNR_dehazing: 19.67 ssim: 0.960281\n","135 iter PSNR: 19.67 time: 0.021003\n","136 iter PSNR_dehazing: 16.10 ssim: 0.916263\n","136 iter PSNR: 16.10 time: 0.020691\n","137 iter PSNR_dehazing: 18.72 ssim: 0.941748\n","137 iter PSNR: 18.72 time: 0.021109\n","138 iter PSNR_dehazing: 10.45 ssim: 0.695504\n","138 iter PSNR: 10.45 time: 0.021122\n","139 iter PSNR_dehazing: 10.18 ssim: 0.634565\n","139 iter PSNR: 10.18 time: 0.021255\n","140 iter PSNR_dehazing: 11.71 ssim: 0.762679\n","140 iter PSNR: 11.71 time: 0.021493\n","141 iter PSNR_dehazing: 14.90 ssim: 0.826456\n","141 iter PSNR: 14.90 time: 0.020908\n","142 iter PSNR_dehazing: 11.91 ssim: 0.727827\n","142 iter PSNR: 11.91 time: 0.020881\n","143 iter PSNR_dehazing: 13.12 ssim: 0.741347\n","143 iter PSNR: 13.12 time: 0.021266\n","144 iter PSNR_dehazing: 10.66 ssim: 0.620461\n","144 iter PSNR: 10.66 time: 0.021870\n","145 iter PSNR_dehazing: 14.40 ssim: 0.729739\n","145 iter PSNR: 14.40 time: 0.021435\n","146 iter PSNR_dehazing: 18.47 ssim: 0.911829\n","146 iter PSNR: 18.47 time: 0.020804\n","147 iter PSNR_dehazing: 18.88 ssim: 0.942690\n","147 iter PSNR: 18.88 time: 0.020960\n","148 iter PSNR_dehazing: 11.77 ssim: 0.651518\n","148 iter PSNR: 11.77 time: 0.020762\n","149 iter PSNR_dehazing: 11.23 ssim: 0.715180\n","149 iter PSNR: 11.23 time: 0.024407\n","150 iter PSNR_dehazing: 16.08 ssim: 0.858935\n","150 iter PSNR: 16.08 time: 0.020936\n","151 iter PSNR_dehazing: 13.85 ssim: 0.757886\n","151 iter PSNR: 13.85 time: 0.021289\n","152 iter PSNR_dehazing: 13.23 ssim: 0.793650\n","152 iter PSNR: 13.23 time: 0.021154\n","153 iter PSNR_dehazing: 16.11 ssim: 0.857573\n","153 iter PSNR: 16.11 time: 0.022316\n","154 iter PSNR_dehazing: 14.27 ssim: 0.768988\n","154 iter PSNR: 14.27 time: 0.020915\n","155 iter PSNR_dehazing: 12.75 ssim: 0.687081\n","155 iter PSNR: 12.75 time: 0.021037\n","156 iter PSNR_dehazing: 15.89 ssim: 0.907241\n","156 iter PSNR: 15.89 time: 0.021485\n","157 iter PSNR_dehazing: 13.63 ssim: 0.846673\n","157 iter PSNR: 13.63 time: 0.022623\n","158 iter PSNR_dehazing: 11.50 ssim: 0.699914\n","158 iter PSNR: 11.50 time: 0.021790\n","159 iter PSNR_dehazing: 18.72 ssim: 0.941997\n","159 iter PSNR: 18.72 time: 0.021565\n","160 iter PSNR_dehazing: 10.58 ssim: 0.616803\n","160 iter PSNR: 10.58 time: 0.021672\n","161 iter PSNR_dehazing: 15.25 ssim: 0.811255\n","161 iter PSNR: 15.25 time: 0.021812\n","162 iter PSNR_dehazing: 9.09 ssim: 0.569041\n","162 iter PSNR: 9.09 time: 0.021012\n","163 iter PSNR_dehazing: 16.34 ssim: 0.882999\n","163 iter PSNR: 16.34 time: 0.021649\n","164 iter PSNR_dehazing: 12.61 ssim: 0.733049\n","164 iter PSNR: 12.61 time: 0.021780\n","165 iter PSNR_dehazing: 12.77 ssim: 0.734038\n","165 iter PSNR: 12.77 time: 0.021189\n","166 iter PSNR_dehazing: 12.58 ssim: 0.750413\n","166 iter PSNR: 12.58 time: 0.026217\n","167 iter PSNR_dehazing: 11.69 ssim: 0.777501\n","167 iter PSNR: 11.69 time: 0.023124\n","168 iter PSNR_dehazing: 11.50 ssim: 0.693684\n","168 iter PSNR: 11.50 time: 0.020953\n","169 iter PSNR_dehazing: 15.21 ssim: 0.739397\n","169 iter PSNR: 15.21 time: 0.021161\n","170 iter PSNR_dehazing: 15.59 ssim: 0.810355\n","170 iter PSNR: 15.59 time: 0.021040\n","171 iter PSNR_dehazing: 19.86 ssim: 0.952480\n","171 iter PSNR: 19.86 time: 0.021117\n","172 iter PSNR_dehazing: 11.02 ssim: 0.708408\n","172 iter PSNR: 11.02 time: 0.021506\n","173 iter PSNR_dehazing: 12.78 ssim: 0.834871\n","173 iter PSNR: 12.78 time: 0.021087\n","174 iter PSNR_dehazing: 16.26 ssim: 0.869605\n","174 iter PSNR: 16.26 time: 0.020857\n","175 iter PSNR_dehazing: 17.25 ssim: 0.920600\n","175 iter PSNR: 17.25 time: 0.020658\n","176 iter PSNR_dehazing: 18.47 ssim: 0.898288\n","176 iter PSNR: 18.47 time: 0.020634\n","177 iter PSNR_dehazing: 12.64 ssim: 0.730769\n","177 iter PSNR: 12.64 time: 0.020893\n","178 iter PSNR_dehazing: 15.30 ssim: 0.893911\n","178 iter PSNR: 15.30 time: 0.020977\n","179 iter PSNR_dehazing: 10.54 ssim: 0.673779\n","179 iter PSNR: 10.54 time: 0.020958\n","180 iter PSNR_dehazing: 16.99 ssim: 0.750575\n","180 iter PSNR: 16.99 time: 0.021072\n","181 iter PSNR_dehazing: 14.83 ssim: 0.878658\n","181 iter PSNR: 14.83 time: 0.020968\n","182 iter PSNR_dehazing: 10.64 ssim: 0.683643\n","182 iter PSNR: 10.64 time: 0.021945\n","183 iter PSNR_dehazing: 12.79 ssim: 0.801264\n","183 iter PSNR: 12.79 time: 0.020868\n","184 iter PSNR_dehazing: 18.87 ssim: 0.942697\n","184 iter PSNR: 18.87 time: 0.021302\n","185 iter PSNR_dehazing: 13.18 ssim: 0.691378\n","185 iter PSNR: 13.18 time: 0.020991\n","186 iter PSNR_dehazing: 14.16 ssim: 0.763959\n","186 iter PSNR: 14.16 time: 0.021868\n","187 iter PSNR_dehazing: 11.91 ssim: 0.775154\n","187 iter PSNR: 11.91 time: 0.020767\n","188 iter PSNR_dehazing: 11.59 ssim: 0.602627\n","188 iter PSNR: 11.59 time: 0.020619\n","189 iter PSNR_dehazing: 17.13 ssim: 0.902812\n","189 iter PSNR: 17.13 time: 0.020924\n","190 iter PSNR_dehazing: 9.78 ssim: 0.665926\n","190 iter PSNR: 9.78 time: 0.020929\n","191 iter PSNR_dehazing: 15.26 ssim: 0.909498\n","191 iter PSNR: 15.26 time: 0.020910\n","192 iter PSNR_dehazing: 17.34 ssim: 0.897199\n","192 iter PSNR: 17.34 time: 0.020779\n","193 iter PSNR_dehazing: 16.58 ssim: 0.814558\n","193 iter PSNR: 16.58 time: 0.020965\n","194 iter PSNR_dehazing: 15.23 ssim: 0.849976\n","194 iter PSNR: 15.23 time: 0.020603\n","195 iter PSNR_dehazing: 12.98 ssim: 0.781694\n","195 iter PSNR: 12.98 time: 0.020616\n","196 iter PSNR_dehazing: 12.74 ssim: 0.798236\n","196 iter PSNR: 12.74 time: 0.020899\n","197 iter PSNR_dehazing: 14.86 ssim: 0.782787\n","197 iter PSNR: 14.86 time: 0.020794\n","198 iter PSNR_dehazing: 15.16 ssim: 0.843292\n","198 iter PSNR: 15.16 time: 0.020520\n","199 iter PSNR_dehazing: 20.24 ssim: 0.955163\n","199 iter PSNR: 20.24 time: 0.024283\n","200 iter PSNR_dehazing: 15.09 ssim: 0.808278\n","200 iter PSNR: 15.09 time: 0.020615\n","201 iter PSNR_dehazing: 11.99 ssim: 0.771318\n","201 iter PSNR: 11.99 time: 0.020643\n","202 iter PSNR_dehazing: 14.30 ssim: 0.670010\n","202 iter PSNR: 14.30 time: 0.020687\n","203 iter PSNR_dehazing: 17.73 ssim: 0.817531\n","203 iter PSNR: 17.73 time: 0.020708\n","204 iter PSNR_dehazing: 11.14 ssim: 0.647180\n","204 iter PSNR: 11.14 time: 0.021351\n","205 iter PSNR_dehazing: 10.96 ssim: 0.660532\n","205 iter PSNR: 10.96 time: 0.021011\n","206 iter PSNR_dehazing: 12.91 ssim: 0.746644\n","206 iter PSNR: 12.91 time: 0.020851\n","207 iter PSNR_dehazing: 11.39 ssim: 0.744549\n","207 iter PSNR: 11.39 time: 0.021799\n","208 iter PSNR_dehazing: 18.59 ssim: 0.875301\n","208 iter PSNR: 18.59 time: 0.020610\n","209 iter PSNR_dehazing: 13.37 ssim: 0.776057\n","209 iter PSNR: 13.37 time: 0.020986\n","210 iter PSNR_dehazing: 11.65 ssim: 0.607859\n","210 iter PSNR: 11.65 time: 0.020975\n","211 iter PSNR_dehazing: 18.87 ssim: 0.845190\n","211 iter PSNR: 18.87 time: 0.023359\n","212 iter PSNR_dehazing: 13.06 ssim: 0.778333\n","212 iter PSNR: 13.06 time: 0.020859\n","213 iter PSNR_dehazing: 19.72 ssim: 0.902730\n","213 iter PSNR: 19.72 time: 0.020946\n","214 iter PSNR_dehazing: 14.07 ssim: 0.695911\n","214 iter PSNR: 14.07 time: 0.020910\n","215 iter PSNR_dehazing: 15.52 ssim: 0.848793\n","215 iter PSNR: 15.52 time: 0.021226\n","216 iter PSNR_dehazing: 13.77 ssim: 0.861475\n","216 iter PSNR: 13.77 time: 0.020810\n","217 iter PSNR_dehazing: 17.61 ssim: 0.914352\n","217 iter PSNR: 17.61 time: 0.020736\n","218 iter PSNR_dehazing: 13.87 ssim: 0.829269\n","218 iter PSNR: 13.87 time: 0.020752\n","219 iter PSNR_dehazing: 12.79 ssim: 0.770115\n","219 iter PSNR: 12.79 time: 0.020946\n","220 iter PSNR_dehazing: 14.09 ssim: 0.867753\n","220 iter PSNR: 14.09 time: 0.020894\n","221 iter PSNR_dehazing: 12.29 ssim: 0.730577\n","221 iter PSNR: 12.29 time: 0.021031\n","222 iter PSNR_dehazing: 14.27 ssim: 0.736829\n","222 iter PSNR: 14.27 time: 0.021364\n","223 iter PSNR_dehazing: 18.71 ssim: 0.929107\n","223 iter PSNR: 18.71 time: 0.020681\n","224 iter PSNR_dehazing: 16.36 ssim: 0.908021\n","224 iter PSNR: 16.36 time: 0.020674\n","225 iter PSNR_dehazing: 15.29 ssim: 0.895787\n","225 iter PSNR: 15.29 time: 0.021341\n","226 iter PSNR_dehazing: 10.08 ssim: 0.622409\n","226 iter PSNR: 10.08 time: 0.021677\n","227 iter PSNR_dehazing: 13.57 ssim: 0.765400\n","227 iter PSNR: 13.57 time: 0.020824\n","228 iter PSNR_dehazing: 15.96 ssim: 0.823199\n","228 iter PSNR: 15.96 time: 0.020811\n","229 iter PSNR_dehazing: 16.11 ssim: 0.883785\n","229 iter PSNR: 16.11 time: 0.020624\n","230 iter PSNR_dehazing: 14.75 ssim: 0.863664\n","230 iter PSNR: 14.75 time: 0.020731\n","231 iter PSNR_dehazing: 16.01 ssim: 0.894934\n","231 iter PSNR: 16.01 time: 0.021252\n","232 iter PSNR_dehazing: 16.98 ssim: 0.910828\n","232 iter PSNR: 16.98 time: 0.020859\n","233 iter PSNR_dehazing: 15.59 ssim: 0.863135\n","233 iter PSNR: 15.59 time: 0.021734\n","234 iter PSNR_dehazing: 14.84 ssim: 0.804516\n","234 iter PSNR: 14.84 time: 0.021159\n","235 iter PSNR_dehazing: 11.82 ssim: 0.710762\n","235 iter PSNR: 11.82 time: 0.020896\n","236 iter PSNR_dehazing: 20.27 ssim: 0.960073\n","236 iter PSNR: 20.27 time: 0.021093\n","237 iter PSNR_dehazing: 12.36 ssim: 0.740848\n","237 iter PSNR: 12.36 time: 0.021224\n","238 iter PSNR_dehazing: 16.26 ssim: 0.838496\n","238 iter PSNR: 16.26 time: 0.022668\n","239 iter PSNR_dehazing: 9.63 ssim: 0.636819\n","239 iter PSNR: 9.63 time: 0.021091\n","240 iter PSNR_dehazing: 17.64 ssim: 0.883446\n","240 iter PSNR: 17.64 time: 0.021044\n","241 iter PSNR_dehazing: 14.77 ssim: 0.866476\n","241 iter PSNR: 14.77 time: 0.021232\n","242 iter PSNR_dehazing: 12.24 ssim: 0.796100\n","242 iter PSNR: 12.24 time: 0.020905\n","243 iter PSNR_dehazing: 10.57 ssim: 0.746330\n","243 iter PSNR: 10.57 time: 0.020959\n","244 iter PSNR_dehazing: 16.79 ssim: 0.846056\n","244 iter PSNR: 16.79 time: 0.020795\n","245 iter PSNR_dehazing: 12.93 ssim: 0.816028\n","245 iter PSNR: 12.93 time: 0.021442\n","246 iter PSNR_dehazing: 14.98 ssim: 0.816317\n","246 iter PSNR: 14.98 time: 0.021224\n","247 iter PSNR_dehazing: 15.76 ssim: 0.882191\n","247 iter PSNR: 15.76 time: 0.021814\n","248 iter PSNR_dehazing: 17.76 ssim: 0.897499\n","248 iter PSNR: 17.76 time: 0.021205\n","249 iter PSNR_dehazing: 12.07 ssim: 0.835845\n","249 iter PSNR: 12.07 time: 0.021299\n","250 iter PSNR_dehazing: 11.36 ssim: 0.728765\n","250 iter PSNR: 11.36 time: 0.021136\n","251 iter PSNR_dehazing: 15.61 ssim: 0.870585\n","251 iter PSNR: 15.61 time: 0.022995\n","252 iter PSNR_dehazing: 16.95 ssim: 0.862816\n","252 iter PSNR: 16.95 time: 0.022811\n","253 iter PSNR_dehazing: 10.63 ssim: 0.711371\n","253 iter PSNR: 10.63 time: 0.022918\n","254 iter PSNR_dehazing: 14.42 ssim: 0.826583\n","254 iter PSNR: 14.42 time: 0.021164\n","255 iter PSNR_dehazing: 15.82 ssim: 0.878903\n","255 iter PSNR: 15.82 time: 0.021382\n","256 iter PSNR_dehazing: 12.04 ssim: 0.787758\n","256 iter PSNR: 12.04 time: 0.021639\n","257 iter PSNR_dehazing: 16.58 ssim: 0.905370\n","257 iter PSNR: 16.58 time: 0.020931\n","258 iter PSNR_dehazing: 12.93 ssim: 0.736557\n","258 iter PSNR: 12.93 time: 0.020897\n","259 iter PSNR_dehazing: 17.95 ssim: 0.916115\n","259 iter PSNR: 17.95 time: 0.021478\n","260 iter PSNR_dehazing: 14.25 ssim: 0.817229\n","260 iter PSNR: 14.25 time: 0.022663\n","261 iter PSNR_dehazing: 13.38 ssim: 0.789129\n","261 iter PSNR: 13.38 time: 0.022418\n","262 iter PSNR_dehazing: 18.38 ssim: 0.938886\n","262 iter PSNR: 18.38 time: 0.021276\n","263 iter PSNR_dehazing: 16.27 ssim: 0.866390\n","263 iter PSNR: 16.27 time: 0.022624\n","264 iter PSNR_dehazing: 16.94 ssim: 0.802950\n","264 iter PSNR: 16.94 time: 0.022197\n","265 iter PSNR_dehazing: 11.54 ssim: 0.710462\n","265 iter PSNR: 11.54 time: 0.020866\n","266 iter PSNR_dehazing: 13.04 ssim: 0.774747\n","266 iter PSNR: 13.04 time: 0.021036\n","267 iter PSNR_dehazing: 16.09 ssim: 0.796621\n","267 iter PSNR: 16.09 time: 0.021277\n","268 iter PSNR_dehazing: 10.59 ssim: 0.656901\n","268 iter PSNR: 10.59 time: 0.021699\n","269 iter PSNR_dehazing: 11.35 ssim: 0.600484\n","269 iter PSNR: 11.35 time: 0.021306\n","270 iter PSNR_dehazing: 13.86 ssim: 0.791135\n","270 iter PSNR: 13.86 time: 0.021954\n","271 iter PSNR_dehazing: 16.57 ssim: 0.894657\n","271 iter PSNR: 16.57 time: 0.021668\n","272 iter PSNR_dehazing: 16.86 ssim: 0.918299\n","272 iter PSNR: 16.86 time: 0.020743\n","273 iter PSNR_dehazing: 14.49 ssim: 0.837001\n","273 iter PSNR: 14.49 time: 0.020852\n","274 iter PSNR_dehazing: 15.21 ssim: 0.857210\n","274 iter PSNR: 15.21 time: 0.021870\n","275 iter PSNR_dehazing: 13.73 ssim: 0.792975\n","275 iter PSNR: 13.73 time: 0.022122\n","276 iter PSNR_dehazing: 17.96 ssim: 0.915584\n","276 iter PSNR: 17.96 time: 0.022114\n","277 iter PSNR_dehazing: 13.71 ssim: 0.843478\n","277 iter PSNR: 13.71 time: 0.021245\n","278 iter PSNR_dehazing: 12.80 ssim: 0.795266\n","278 iter PSNR: 12.80 time: 0.021117\n","279 iter PSNR_dehazing: 14.21 ssim: 0.811897\n","279 iter PSNR: 14.21 time: 0.021699\n","280 iter PSNR_dehazing: 9.83 ssim: 0.666291\n","280 iter PSNR: 9.83 time: 0.021326\n","281 iter PSNR_dehazing: 13.48 ssim: 0.753067\n","281 iter PSNR: 13.48 time: 0.021635\n","282 iter PSNR_dehazing: 10.75 ssim: 0.775642\n","282 iter PSNR: 10.75 time: 0.021410\n","283 iter PSNR_dehazing: 21.44 ssim: 0.978456\n","283 iter PSNR: 21.44 time: 0.021650\n","284 iter PSNR_dehazing: 17.04 ssim: 0.931434\n","284 iter PSNR: 17.04 time: 0.030499\n","285 iter PSNR_dehazing: 17.19 ssim: 0.893772\n","285 iter PSNR: 17.19 time: 0.021416\n","286 iter PSNR_dehazing: 13.01 ssim: 0.778639\n","286 iter PSNR: 13.01 time: 0.021484\n","287 iter PSNR_dehazing: 14.67 ssim: 0.785119\n","287 iter PSNR: 14.67 time: 0.020964\n","288 iter PSNR_dehazing: 11.03 ssim: 0.717432\n","288 iter PSNR: 11.03 time: 0.021924\n","289 iter PSNR_dehazing: 17.18 ssim: 0.916278\n","289 iter PSNR: 17.18 time: 0.021135\n","290 iter PSNR_dehazing: 16.19 ssim: 0.737126\n","290 iter PSNR: 16.19 time: 0.022851\n","291 iter PSNR_dehazing: 15.41 ssim: 0.826024\n","291 iter PSNR: 15.41 time: 0.021215\n","292 iter PSNR_dehazing: 12.77 ssim: 0.646577\n","292 iter PSNR: 12.77 time: 0.022791\n","293 iter PSNR_dehazing: 18.05 ssim: 0.915178\n","293 iter PSNR: 18.05 time: 0.021022\n","294 iter PSNR_dehazing: 14.82 ssim: 0.773553\n","294 iter PSNR: 14.82 time: 0.021134\n","295 iter PSNR_dehazing: 12.86 ssim: 0.815150\n","295 iter PSNR: 12.86 time: 0.020567\n","296 iter PSNR_dehazing: 17.35 ssim: 0.882633\n","296 iter PSNR: 17.35 time: 0.021934\n","297 iter PSNR_dehazing: 18.15 ssim: 0.872925\n","297 iter PSNR: 18.15 time: 0.020520\n","298 iter PSNR_dehazing: 16.81 ssim: 0.893175\n","298 iter PSNR: 16.81 time: 0.020687\n","299 iter PSNR_dehazing: 17.42 ssim: 0.884210\n","299 iter PSNR: 17.42 time: 0.021236\n","300 iter PSNR_dehazing: 14.05 ssim: 0.784138\n","300 iter PSNR: 14.05 time: 0.020949\n","301 iter PSNR_dehazing: 14.46 ssim: 0.758981\n","301 iter PSNR: 14.46 time: 0.021162\n","302 iter PSNR_dehazing: 12.89 ssim: 0.720814\n","302 iter PSNR: 12.89 time: 0.020985\n","303 iter PSNR_dehazing: 16.11 ssim: 0.899769\n","303 iter PSNR: 16.11 time: 0.021263\n","304 iter PSNR_dehazing: 15.99 ssim: 0.868233\n","304 iter PSNR: 15.99 time: 0.021319\n","305 iter PSNR_dehazing: 14.33 ssim: 0.835740\n","305 iter PSNR: 14.33 time: 0.022947\n","306 iter PSNR_dehazing: 14.07 ssim: 0.786326\n","306 iter PSNR: 14.07 time: 0.021114\n","307 iter PSNR_dehazing: 17.24 ssim: 0.831201\n","307 iter PSNR: 17.24 time: 0.020972\n","308 iter PSNR_dehazing: 14.21 ssim: 0.714738\n","308 iter PSNR: 14.21 time: 0.020790\n","309 iter PSNR_dehazing: 12.60 ssim: 0.728361\n","309 iter PSNR: 12.60 time: 0.021427\n","310 iter PSNR_dehazing: 9.69 ssim: 0.608290\n","310 iter PSNR: 9.69 time: 0.021308\n","311 iter PSNR_dehazing: 17.63 ssim: 0.876144\n","311 iter PSNR: 17.63 time: 0.020973\n","312 iter PSNR_dehazing: 12.38 ssim: 0.771916\n","312 iter PSNR: 12.38 time: 0.020795\n","313 iter PSNR_dehazing: 19.55 ssim: 0.819155\n","313 iter PSNR: 19.55 time: 0.020707\n","314 iter PSNR_dehazing: 13.97 ssim: 0.809821\n","314 iter PSNR: 13.97 time: 0.020689\n","315 iter PSNR_dehazing: 12.50 ssim: 0.652185\n","315 iter PSNR: 12.50 time: 0.021208\n","316 iter PSNR_dehazing: 15.70 ssim: 0.828322\n","316 iter PSNR: 15.70 time: 0.021347\n","317 iter PSNR_dehazing: 17.48 ssim: 0.899038\n","317 iter PSNR: 17.48 time: 0.021568\n","318 iter PSNR_dehazing: 12.53 ssim: 0.827402\n","318 iter PSNR: 12.53 time: 0.021438\n","319 iter PSNR_dehazing: 13.99 ssim: 0.869596\n","319 iter PSNR: 13.99 time: 0.021644\n","320 iter PSNR_dehazing: 15.73 ssim: 0.893031\n","320 iter PSNR: 15.73 time: 0.021581\n","321 iter PSNR_dehazing: 16.63 ssim: 0.914422\n","321 iter PSNR: 16.63 time: 0.021631\n","322 iter PSNR_dehazing: 18.04 ssim: 0.892248\n","322 iter PSNR: 18.04 time: 0.021238\n","323 iter PSNR_dehazing: 13.22 ssim: 0.785590\n","323 iter PSNR: 13.22 time: 0.021525\n","324 iter PSNR_dehazing: 14.13 ssim: 0.773885\n","324 iter PSNR: 14.13 time: 0.021451\n","325 iter PSNR_dehazing: 13.39 ssim: 0.775196\n","325 iter PSNR: 13.39 time: 0.022856\n","326 iter PSNR_dehazing: 16.41 ssim: 0.915001\n","326 iter PSNR: 16.41 time: 0.022232\n","327 iter PSNR_dehazing: 13.02 ssim: 0.879633\n","327 iter PSNR: 13.02 time: 0.021679\n","328 iter PSNR_dehazing: 12.23 ssim: 0.785665\n","328 iter PSNR: 12.23 time: 0.022016\n","329 iter PSNR_dehazing: 13.89 ssim: 0.856958\n","329 iter PSNR: 13.89 time: 0.021781\n","330 iter PSNR_dehazing: 12.66 ssim: 0.819143\n","330 iter PSNR: 12.66 time: 0.022096\n","331 iter PSNR_dehazing: 11.09 ssim: 0.723200\n","331 iter PSNR: 11.09 time: 0.021307\n","332 iter PSNR_dehazing: 13.02 ssim: 0.715470\n","332 iter PSNR: 13.02 time: 0.021600\n","333 iter PSNR_dehazing: 13.77 ssim: 0.818739\n","333 iter PSNR: 13.77 time: 0.021256\n","334 iter PSNR_dehazing: 16.43 ssim: 0.897384\n","334 iter PSNR: 16.43 time: 0.021527\n","335 iter PSNR_dehazing: 13.58 ssim: 0.881871\n","335 iter PSNR: 13.58 time: 0.021549\n","336 iter PSNR_dehazing: 15.82 ssim: 0.812921\n","336 iter PSNR: 15.82 time: 0.021173\n","337 iter PSNR_dehazing: 14.23 ssim: 0.738474\n","337 iter PSNR: 14.23 time: 0.021153\n","338 iter PSNR_dehazing: 14.21 ssim: 0.816021\n","338 iter PSNR: 14.21 time: 0.021084\n","339 iter PSNR_dehazing: 18.44 ssim: 0.910375\n","339 iter PSNR: 18.44 time: 0.021253\n","340 iter PSNR_dehazing: 12.08 ssim: 0.732355\n","340 iter PSNR: 12.08 time: 0.021118\n","341 iter PSNR_dehazing: 14.10 ssim: 0.815795\n","341 iter PSNR: 14.10 time: 0.023478\n","342 iter PSNR_dehazing: 13.97 ssim: 0.834083\n","342 iter PSNR: 13.97 time: 0.022296\n","343 iter PSNR_dehazing: 12.71 ssim: 0.746577\n","343 iter PSNR: 12.71 time: 0.021717\n","344 iter PSNR_dehazing: 17.13 ssim: 0.925524\n","344 iter PSNR: 17.13 time: 0.022023\n","345 iter PSNR_dehazing: 19.76 ssim: 0.973873\n","345 iter PSNR: 19.76 time: 0.021698\n","346 iter PSNR_dehazing: 15.87 ssim: 0.897935\n","346 iter PSNR: 15.87 time: 0.022274\n","347 iter PSNR_dehazing: 16.11 ssim: 0.806999\n","347 iter PSNR: 16.11 time: 0.021943\n","348 iter PSNR_dehazing: 12.34 ssim: 0.791615\n","348 iter PSNR: 12.34 time: 0.021656\n","349 iter PSNR_dehazing: 13.26 ssim: 0.811393\n","349 iter PSNR: 13.26 time: 0.020920\n","350 iter PSNR_dehazing: 18.90 ssim: 0.950110\n","350 iter PSNR: 18.90 time: 0.023105\n","351 iter PSNR_dehazing: 14.32 ssim: 0.889421\n","351 iter PSNR: 14.32 time: 0.021674\n","352 iter PSNR_dehazing: 11.67 ssim: 0.791997\n","352 iter PSNR: 11.67 time: 0.022415\n","353 iter PSNR_dehazing: 16.21 ssim: 0.850533\n","353 iter PSNR: 16.21 time: 0.021441\n","354 iter PSNR_dehazing: 13.82 ssim: 0.744349\n","354 iter PSNR: 13.82 time: 0.020882\n","355 iter PSNR_dehazing: 10.62 ssim: 0.620949\n","355 iter PSNR: 10.62 time: 0.021150\n","356 iter PSNR_dehazing: 9.73 ssim: 0.610811\n","356 iter PSNR: 9.73 time: 0.020694\n","357 iter PSNR_dehazing: 16.57 ssim: 0.817273\n","357 iter PSNR: 16.57 time: 0.020865\n","358 iter PSNR_dehazing: 14.61 ssim: 0.814606\n","358 iter PSNR: 14.61 time: 0.020534\n","359 iter PSNR_dehazing: 13.88 ssim: 0.850133\n","359 iter PSNR: 13.88 time: 0.023333\n","360 iter PSNR_dehazing: 12.34 ssim: 0.733366\n","360 iter PSNR: 12.34 time: 0.021503\n","361 iter PSNR_dehazing: 12.91 ssim: 0.818773\n","361 iter PSNR: 12.91 time: 0.022369\n","362 iter PSNR_dehazing: 10.42 ssim: 0.694583\n","362 iter PSNR: 10.42 time: 0.021201\n","363 iter PSNR_dehazing: 14.02 ssim: 0.819781\n","363 iter PSNR: 14.02 time: 0.020905\n","364 iter PSNR_dehazing: 12.39 ssim: 0.763633\n","364 iter PSNR: 12.39 time: 0.020689\n","365 iter PSNR_dehazing: 18.58 ssim: 0.869283\n","365 iter PSNR: 18.58 time: 0.021172\n","366 iter PSNR_dehazing: 20.46 ssim: 0.946541\n","366 iter PSNR: 20.46 time: 0.021566\n","367 iter PSNR_dehazing: 19.13 ssim: 0.935407\n","367 iter PSNR: 19.13 time: 0.021602\n","368 iter PSNR_dehazing: 9.93 ssim: 0.633325\n","368 iter PSNR: 9.93 time: 0.021584\n","369 iter PSNR_dehazing: 15.31 ssim: 0.774007\n","369 iter PSNR: 15.31 time: 0.021420\n","370 iter PSNR_dehazing: 14.11 ssim: 0.842107\n","370 iter PSNR: 14.11 time: 0.021993\n","371 iter PSNR_dehazing: 12.72 ssim: 0.680027\n","371 iter PSNR: 12.72 time: 0.022870\n","372 iter PSNR_dehazing: 11.47 ssim: 0.713960\n","372 iter PSNR: 11.47 time: 0.021551\n","373 iter PSNR_dehazing: 17.62 ssim: 0.914814\n","373 iter PSNR: 17.62 time: 0.021534\n","374 iter PSNR_dehazing: 15.57 ssim: 0.851829\n","374 iter PSNR: 15.57 time: 0.021309\n","375 iter PSNR_dehazing: 18.03 ssim: 0.880547\n","375 iter PSNR: 18.03 time: 0.021157\n","376 iter PSNR_dehazing: 12.26 ssim: 0.679583\n","376 iter PSNR: 12.26 time: 0.021305\n","377 iter PSNR_dehazing: 9.00 ssim: 0.607602\n","377 iter PSNR: 9.00 time: 0.022860\n","378 iter PSNR_dehazing: 16.84 ssim: 0.904870\n","378 iter PSNR: 16.84 time: 0.021137\n","379 iter PSNR_dehazing: 18.06 ssim: 0.877939\n","379 iter PSNR: 18.06 time: 0.021397\n","380 iter PSNR_dehazing: 10.43 ssim: 0.697111\n","380 iter PSNR: 10.43 time: 0.021606\n","381 iter PSNR_dehazing: 13.09 ssim: 0.732460\n","381 iter PSNR: 13.09 time: 0.021999\n","382 iter PSNR_dehazing: 16.17 ssim: 0.844420\n","382 iter PSNR: 16.17 time: 0.021571\n","383 iter PSNR_dehazing: 19.90 ssim: 0.868642\n","383 iter PSNR: 19.90 time: 0.021646\n","384 iter PSNR_dehazing: 18.06 ssim: 0.836888\n","384 iter PSNR: 18.06 time: 0.021766\n","385 iter PSNR_dehazing: 13.15 ssim: 0.785137\n","385 iter PSNR: 13.15 time: 0.022653\n","386 iter PSNR_dehazing: 11.93 ssim: 0.682358\n","386 iter PSNR: 11.93 time: 0.021474\n","387 iter PSNR_dehazing: 15.43 ssim: 0.861266\n","387 iter PSNR: 15.43 time: 0.021171\n","388 iter PSNR_dehazing: 14.47 ssim: 0.805492\n","388 iter PSNR: 14.47 time: 0.021431\n","389 iter PSNR_dehazing: 15.08 ssim: 0.821060\n","389 iter PSNR: 15.08 time: 0.021115\n","390 iter PSNR_dehazing: 15.61 ssim: 0.818699\n","390 iter PSNR: 15.61 time: 0.021656\n","391 iter PSNR_dehazing: 13.78 ssim: 0.849504\n","391 iter PSNR: 13.78 time: 0.021746\n","392 iter PSNR_dehazing: 17.56 ssim: 0.913135\n","392 iter PSNR: 17.56 time: 0.022573\n","393 iter PSNR_dehazing: 15.11 ssim: 0.892248\n","393 iter PSNR: 15.11 time: 0.021444\n","394 iter PSNR_dehazing: 11.28 ssim: 0.740907\n","394 iter PSNR: 11.28 time: 0.021771\n","395 iter PSNR_dehazing: 17.21 ssim: 0.897730\n","395 iter PSNR: 17.21 time: 0.021056\n","396 iter PSNR_dehazing: 13.62 ssim: 0.796883\n","396 iter PSNR: 13.62 time: 0.021135\n","397 iter PSNR_dehazing: 18.53 ssim: 0.905684\n","397 iter PSNR: 18.53 time: 0.022187\n","398 iter PSNR_dehazing: 11.49 ssim: 0.647429\n","398 iter PSNR: 11.49 time: 0.021228\n","399 iter PSNR_dehazing: 13.44 ssim: 0.836587\n","399 iter PSNR: 13.44 time: 0.021304\n","400 iter PSNR_dehazing: 16.53 ssim: 0.915273\n","400 iter PSNR: 16.53 time: 0.021100\n","401 iter PSNR_dehazing: 9.28 ssim: 0.633106\n","401 iter PSNR: 9.28 time: 0.020710\n","402 iter PSNR_dehazing: 11.34 ssim: 0.726993\n","402 iter PSNR: 11.34 time: 0.020723\n","403 iter PSNR_dehazing: 16.51 ssim: 0.846003\n","403 iter PSNR: 16.51 time: 0.022518\n","404 iter PSNR_dehazing: 19.54 ssim: 0.936664\n","404 iter PSNR: 19.54 time: 0.020574\n","405 iter PSNR_dehazing: 15.46 ssim: 0.859312\n","405 iter PSNR: 15.46 time: 0.021054\n","406 iter PSNR_dehazing: 16.41 ssim: 0.736134\n","406 iter PSNR: 16.41 time: 0.020561\n","407 iter PSNR_dehazing: 20.26 ssim: 0.937846\n","407 iter PSNR: 20.26 time: 0.020715\n","408 iter PSNR_dehazing: 20.03 ssim: 0.951289\n","408 iter PSNR: 20.03 time: 0.021465\n","409 iter PSNR_dehazing: 15.70 ssim: 0.806328\n","409 iter PSNR: 15.70 time: 0.020787\n","410 iter PSNR_dehazing: 15.90 ssim: 0.785129\n","410 iter PSNR: 15.90 time: 0.020890\n","411 iter PSNR_dehazing: 19.11 ssim: 0.925147\n","411 iter PSNR: 19.11 time: 0.021190\n","412 iter PSNR_dehazing: 15.74 ssim: 0.796311\n","412 iter PSNR: 15.74 time: 0.021251\n","413 iter PSNR_dehazing: 16.30 ssim: 0.861316\n","413 iter PSNR: 16.30 time: 0.020801\n","414 iter PSNR_dehazing: 18.19 ssim: 0.912319\n","414 iter PSNR: 18.19 time: 0.020958\n","415 iter PSNR_dehazing: 15.46 ssim: 0.737608\n","415 iter PSNR: 15.46 time: 0.021032\n","416 iter PSNR_dehazing: 17.59 ssim: 0.878584\n","416 iter PSNR: 17.59 time: 0.022635\n","417 iter PSNR_dehazing: 15.23 ssim: 0.786650\n","417 iter PSNR: 15.23 time: 0.022611\n","418 iter PSNR_dehazing: 14.73 ssim: 0.838305\n","418 iter PSNR: 14.73 time: 0.020942\n","419 iter PSNR_dehazing: 13.43 ssim: 0.776834\n","419 iter PSNR: 13.43 time: 0.021175\n","420 iter PSNR_dehazing: 15.91 ssim: 0.813415\n","420 iter PSNR: 15.91 time: 0.021014\n","421 iter PSNR_dehazing: 16.01 ssim: 0.734262\n","421 iter PSNR: 16.01 time: 0.021063\n","422 iter PSNR_dehazing: 20.26 ssim: 0.952449\n","422 iter PSNR: 20.26 time: 0.022168\n","423 iter PSNR_dehazing: 19.61 ssim: 0.944410\n","423 iter PSNR: 19.61 time: 0.022406\n","424 iter PSNR_dehazing: 13.84 ssim: 0.776494\n","424 iter PSNR: 13.84 time: 0.021014\n","425 iter PSNR_dehazing: 15.70 ssim: 0.816730\n","425 iter PSNR: 15.70 time: 0.021280\n","426 iter PSNR_dehazing: 18.10 ssim: 0.945861\n","426 iter PSNR: 18.10 time: 0.024175\n","427 iter PSNR_dehazing: 13.60 ssim: 0.773465\n","427 iter PSNR: 13.60 time: 0.021884\n","428 iter PSNR_dehazing: 15.66 ssim: 0.809539\n","428 iter PSNR: 15.66 time: 0.021293\n","429 iter PSNR_dehazing: 15.11 ssim: 0.805398\n","429 iter PSNR: 15.11 time: 0.021570\n","430 iter PSNR_dehazing: 16.82 ssim: 0.898869\n","430 iter PSNR: 16.82 time: 0.022280\n","431 iter PSNR_dehazing: 12.73 ssim: 0.743424\n","431 iter PSNR: 12.73 time: 0.020960\n","432 iter PSNR_dehazing: 16.58 ssim: 0.803567\n","432 iter PSNR: 16.58 time: 0.021403\n","433 iter PSNR_dehazing: 13.15 ssim: 0.835884\n","433 iter PSNR: 13.15 time: 0.021333\n","434 iter PSNR_dehazing: 16.45 ssim: 0.834647\n","434 iter PSNR: 16.45 time: 0.021030\n","435 iter PSNR_dehazing: 17.02 ssim: 0.854608\n","435 iter PSNR: 17.02 time: 0.021508\n","436 iter PSNR_dehazing: 15.36 ssim: 0.849279\n","436 iter PSNR: 15.36 time: 0.021076\n","437 iter PSNR_dehazing: 14.19 ssim: 0.747963\n","437 iter PSNR: 14.19 time: 0.021105\n","438 iter PSNR_dehazing: 17.14 ssim: 0.847008\n","438 iter PSNR: 17.14 time: 0.021198\n","439 iter PSNR_dehazing: 13.78 ssim: 0.764094\n","439 iter PSNR: 13.78 time: 0.021325\n","440 iter PSNR_dehazing: 13.83 ssim: 0.770778\n","440 iter PSNR: 13.83 time: 0.021648\n","441 iter PSNR_dehazing: 16.27 ssim: 0.883276\n","441 iter PSNR: 16.27 time: 0.021620\n","442 iter PSNR_dehazing: 18.48 ssim: 0.867708\n","442 iter PSNR: 18.48 time: 0.021768\n","443 iter PSNR_dehazing: 18.35 ssim: 0.865320\n","443 iter PSNR: 18.35 time: 0.021498\n","444 iter PSNR_dehazing: 14.86 ssim: 0.804979\n","444 iter PSNR: 14.86 time: 0.021558\n","445 iter PSNR_dehazing: 13.67 ssim: 0.769145\n","445 iter PSNR: 13.67 time: 0.021260\n","446 iter PSNR_dehazing: 17.33 ssim: 0.900593\n","446 iter PSNR: 17.33 time: 0.021753\n","447 iter PSNR_dehazing: 16.54 ssim: 0.793949\n","447 iter PSNR: 16.54 time: 0.021149\n","448 iter PSNR_dehazing: 19.72 ssim: 0.874736\n","448 iter PSNR: 19.72 time: 0.021229\n","449 iter PSNR_dehazing: 13.28 ssim: 0.757384\n","449 iter PSNR: 13.28 time: 0.022032\n","450 iter PSNR_dehazing: 10.84 ssim: 0.603312\n","450 iter PSNR: 10.84 time: 0.021150\n","451 iter PSNR_dehazing: 15.21 ssim: 0.783210\n","451 iter PSNR: 15.21 time: 0.021191\n","452 iter PSNR_dehazing: 17.66 ssim: 0.924188\n","452 iter PSNR: 17.66 time: 0.021185\n","453 iter PSNR_dehazing: 19.09 ssim: 0.880398\n","453 iter PSNR: 19.09 time: 0.021419\n","454 iter PSNR_dehazing: 17.71 ssim: 0.818840\n","454 iter PSNR: 17.71 time: 0.021040\n","455 iter PSNR_dehazing: 16.09 ssim: 0.822974\n","455 iter PSNR: 16.09 time: 0.021160\n","456 iter PSNR_dehazing: 12.54 ssim: 0.748753\n","456 iter PSNR: 12.54 time: 0.021346\n","457 iter PSNR_dehazing: 15.74 ssim: 0.845076\n","457 iter PSNR: 15.74 time: 0.020999\n","458 iter PSNR_dehazing: 18.94 ssim: 0.925620\n","458 iter PSNR: 18.94 time: 0.022561\n","459 iter PSNR_dehazing: 10.62 ssim: 0.616512\n","459 iter PSNR: 10.62 time: 0.021616\n","460 iter PSNR_dehazing: 18.62 ssim: 0.923754\n","460 iter PSNR: 18.62 time: 0.021276\n","461 iter PSNR_dehazing: 15.18 ssim: 0.880773\n","461 iter PSNR: 15.18 time: 0.021510\n","462 iter PSNR_dehazing: 17.35 ssim: 0.903149\n","462 iter PSNR: 17.35 time: 0.022575\n","463 iter PSNR_dehazing: 16.33 ssim: 0.876167\n","463 iter PSNR: 16.33 time: 0.021781\n","464 iter PSNR_dehazing: 15.97 ssim: 0.836911\n","464 iter PSNR: 15.97 time: 0.021550\n","465 iter PSNR_dehazing: 14.27 ssim: 0.849242\n","465 iter PSNR: 14.27 time: 0.021141\n","466 iter PSNR_dehazing: 17.35 ssim: 0.880220\n","466 iter PSNR: 17.35 time: 0.021418\n","467 iter PSNR_dehazing: 17.60 ssim: 0.924648\n","467 iter PSNR: 17.60 time: 0.021919\n","468 iter PSNR_dehazing: 19.73 ssim: 0.951016\n","468 iter PSNR: 19.73 time: 0.021223\n","469 iter PSNR_dehazing: 19.37 ssim: 0.946280\n","469 iter PSNR: 19.37 time: 0.021054\n","470 iter PSNR_dehazing: 14.78 ssim: 0.830480\n","470 iter PSNR: 14.78 time: 0.021438\n","471 iter PSNR_dehazing: 14.57 ssim: 0.810219\n","471 iter PSNR: 14.57 time: 0.021247\n","472 iter PSNR_dehazing: 17.42 ssim: 0.886530\n","472 iter PSNR: 17.42 time: 0.021159\n","473 iter PSNR_dehazing: 15.74 ssim: 0.872618\n","473 iter PSNR: 15.74 time: 0.021097\n","474 iter PSNR_dehazing: 17.79 ssim: 0.935802\n","474 iter PSNR: 17.79 time: 0.020973\n","475 iter PSNR_dehazing: 14.74 ssim: 0.881186\n","475 iter PSNR: 14.74 time: 0.021293\n","476 iter PSNR_dehazing: 19.66 ssim: 0.937084\n","476 iter PSNR: 19.66 time: 0.021185\n","477 iter PSNR_dehazing: 15.93 ssim: 0.893080\n","477 iter PSNR: 15.93 time: 0.021396\n","478 iter PSNR_dehazing: 15.80 ssim: 0.896321\n","478 iter PSNR: 15.80 time: 0.021389\n","479 iter PSNR_dehazing: 16.49 ssim: 0.881145\n","479 iter PSNR: 16.49 time: 0.020978\n","480 iter PSNR_dehazing: 15.19 ssim: 0.855963\n","480 iter PSNR: 15.19 time: 0.020970\n","481 iter PSNR_dehazing: 15.01 ssim: 0.830893\n","481 iter PSNR: 15.01 time: 0.021131\n","482 iter PSNR_dehazing: 14.04 ssim: 0.808120\n","482 iter PSNR: 14.04 time: 0.022049\n","483 iter PSNR_dehazing: 15.79 ssim: 0.858538\n","483 iter PSNR: 15.79 time: 0.021518\n","484 iter PSNR_dehazing: 15.28 ssim: 0.814866\n","484 iter PSNR: 15.28 time: 0.022114\n","485 iter PSNR_dehazing: 22.82 ssim: 0.979079\n","485 iter PSNR: 22.82 time: 0.021282\n","486 iter PSNR_dehazing: 12.38 ssim: 0.708379\n","486 iter PSNR: 12.38 time: 0.022564\n","487 iter PSNR_dehazing: 18.60 ssim: 0.910278\n","487 iter PSNR: 18.60 time: 0.021141\n","488 iter PSNR_dehazing: 12.43 ssim: 0.763754\n","488 iter PSNR: 12.43 time: 0.023350\n","489 iter PSNR_dehazing: 17.94 ssim: 0.890745\n","489 iter PSNR: 17.94 time: 0.021290\n","490 iter PSNR_dehazing: 16.03 ssim: 0.845414\n","490 iter PSNR: 16.03 time: 0.021111\n","491 iter PSNR_dehazing: 14.29 ssim: 0.741841\n","491 iter PSNR: 14.29 time: 0.021436\n","492 iter PSNR_dehazing: 15.63 ssim: 0.867912\n","492 iter PSNR: 15.63 time: 0.021004\n","493 iter PSNR_dehazing: 17.78 ssim: 0.898697\n","493 iter PSNR: 17.78 time: 0.021364\n","494 iter PSNR_dehazing: 16.13 ssim: 0.763840\n","494 iter PSNR: 16.13 time: 0.020845\n","495 iter PSNR_dehazing: 16.48 ssim: 0.838475\n","495 iter PSNR: 16.48 time: 0.021182\n","496 iter PSNR_dehazing: 16.75 ssim: 0.852535\n","496 iter PSNR: 16.75 time: 0.021769\n","497 iter PSNR_dehazing: 15.24 ssim: 0.831464\n","497 iter PSNR: 15.24 time: 0.020853\n","498 iter PSNR_dehazing: 17.14 ssim: 0.822750\n","498 iter PSNR: 17.14 time: 0.021187\n","499 iter PSNR_dehazing: 15.21 ssim: 0.811198\n","499 iter PSNR: 15.21 time: 0.021316\n","500 iter PSNR_dehazing: 15.03 ssim: 0.814499\n","500 iter PSNR: 15.03 time: 0.021600\n","==========================================================\n","The average PSNR is 14.91 dB\n","The average SSIM is 0.81506 dB\n","Average time: 0.022370\n"]}],"source":["import os\n","import torch\n","import torchvision.transforms as transforms\n","# from torch.backends import cudnn # Uncomment if you need it\n","\n","class Args:\n","    model_name = 'IRNeXt'\n","    mode = 'test'\n","    data_dir = path\n","\n","    # Train\n","    batch_size = 8\n","    learning_rate = 1e-4\n","    weight_decay = 0\n","    num_epoch = 300\n","    print_freq = 100\n","    num_worker = 8\n","    save_freq = 10\n","    valid_freq = 10\n","    momentum_value = 0.4\n","\n","    resume = ''\n","    #/content/drive/MyDrive/MYresults2/IRNeXt/ITS/model.pkl\n","    # Test\n","    test_model = '/content/drive/MyDrive/OUTMYresults0_4/IRNeXt/ITS/Final.pkl'\n","    save_image = False\n","\n","    # Directories (set these as per your requirement)\n","    model_save_dir = os.path.join('/content/drive/MyDrive/OUTMYresults0_4/', 'IRNeXt', 'ITS/')\n","    result_dir = os.path.join('/content/drive/MyDrive/OUTMYresults0_4/', model_name, 'test')\n","\n","def main(args):\n","    # CUDNN\n","    # cudnn.benchmark = True # Uncomment if you need it\n","\n","    if not os.path.exists('/content/drive/MyDrive/OUTMYresults0_4/'):\n","        os.makedirs(args.model_save_dir)\n","    if not os.path.exists('/content/drive/MyDrive/OUTMYresults0_4/' + args.model_name + '/'):\n","        os.makedirs('/content/drive/MyDrive/OUTMYresults0_4/' + args.model_name + '/')\n","    if not os.path.exists(args.model_save_dir):\n","        os.makedirs(args.model_save_dir)\n","    if not os.path.exists(args.result_dir):\n","        os.makedirs(args.result_dir)\n","\n","    model = build_net()  # Make sure to define build_net or import it if it's defined elsewhere\n","    print(model)\n","\n","    if torch.cuda.is_available():\n","        model.cuda()\n","    if args.mode == 'train':\n","        _train(model, args)  # Make sure to define _train or import it if it's defined elsewhere\n","\n","    elif args.mode == 'test':\n","        _eval(model, args)   # Make sure to define _eval or import it if it's defined elsewhere\n","\n","# Replace parser.parse_args() with an instance of the Args class\n","args = Args()\n","if not os.path.exists(args.model_save_dir):\n","    os.makedirs(args.model_save_dir)\n","# Copying files (make sure these paths are correct)\n","command = 'cp ' + 'models/layers.py ' + args.model_save_dir\n","os.system(command)\n","command = 'cp ' + 'models/IRNeXt.py ' + args.model_save_dir\n","os.system(command)\n","command = 'cp ' + 'train.py ' + args.model_save_dir\n","os.system(command)\n","command = 'cp ' + 'main.py ' + args.model_save_dir\n","os.system(command)\n","print(args)\n","main(args)\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["_h78ryOOBvc2","Ik-THSPrBomo","FOmdMPEPBcqU","xuIwak2DBSmK","kiBJcTJbCOcj","muFr7px4CKua","IZ_iPV8xBg0K","0-FmaLbRBrf6","1n87ZbXeBapS"],"machine_shape":"hm","name":"","provenance":[{"file_id":"1dhpT0CcBYzS-PHZ64omWNUVKQtgmFyi_","timestamp":1700676994740},{"file_id":"1aFlFtmeHn8lS0p1lmVoYERWxJ3JYsDN2","timestamp":1700675653023},{"file_id":"1ulIbpjF3ez1j1gbzAuV9QDJYExBLRm4J","timestamp":1700675320197},{"file_id":"1LUJvAyH3mKvy4uCJm47ja45bIV8Ztd9i","timestamp":1700659563884}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}