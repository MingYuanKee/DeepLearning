{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9AuBstb-epVb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from collections import Counter\n",
        "torch.cuda.empty_cache()\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
        "from google.colab import files\n",
        "from scipy.stats import beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_AD4e02nEbQ"
      },
      "source": [
        "The CIFAR-100 training set you get from torchvision contains 50,000 images. Randomly divide\n",
        "this dataset into a new training set and a validation set, containing 40,000 and 10,000 data points\n",
        "respectively. Use random seed 0 for the partitioning. Show the following in your report.\n",
        "1. The line(s) of code you use to partition the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQA1YRIsjT91",
        "outputId": "a9e16567-3c62-4b52-de0e-0b8fe2903977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:01<00:00, 101735833.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "# Load the CIFAR-100 dataset from torchvision\n",
        "cifar100Data = datasets.CIFAR100(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Get class names\n",
        "class_names = cifar100Data.classes\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "trainSize = int((4/5) * len(cifar100Data))\n",
        "validationSize = len(cifar100Data) - trainSize\n",
        "seed = 0\n",
        "trainData, testData = random_split(cifar100Data, [trainSize, validationSize], generator=torch.Generator().manual_seed(seed))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. The proportion of each class in the new training set."
      ],
      "metadata": {
        "id": "AlAX363N8LjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the labels and class names of the training data\n",
        "trainLabels = [label for _, label in trainData]\n",
        "\n",
        "# Count the occurrences of each label\n",
        "labelCounts = Counter(trainLabels)\n",
        "\n",
        "# Calculate the proportion of each class\n",
        "classProportions = {label: count / len(trainLabels) for label, count in labelCounts.items()}\n",
        "\n",
        "# Sort the class proportions by label in ascending order\n",
        "classProportions_sorted = dict(sorted(classProportions.items()))\n",
        "\n",
        "# Print the sorted class proportions with class names\n",
        "print(\"Class Proportions in the training data (Sorted):\")\n",
        "for label, proportion in classProportions_sorted.items():\n",
        "    class_name = class_names[label]\n",
        "    print(f\"{class_name}: {proportion*100:.4f}%\")\n"
      ],
      "metadata": {
        "id": "OEv10W_G8CAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILSYBW-UnQSY"
      },
      "source": [
        "Compute the mean and standard deviation for each color channel on the training set. Report these\n",
        "numbers and use them in your preprocessing pipeline to whiten / normalize the data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate mean and standard deviation for each color channel\n",
        "data = next(iter(DataLoader(trainData, batch_size=len(trainData), shuffle=False)))\n",
        "mean = torch.mean(data[0], dim=(0, 2, 3))\n",
        "std = torch.std(data[0], dim=(0, 2, 3))\n",
        "\n",
        "# Print the computed mean and standard deviation\n",
        "print(\"Mean for each color channel:\", mean)\n",
        "print(\"Standard deviation for each color channel:\", std)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZAZgaRueBT3",
        "outputId": "a143c49c-ba8e-450a-e601-eb4be2426683"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean for each color channel: tensor([0.5068, 0.4861, 0.4403])\n",
            "Standard deviation for each color channel: tensor([0.2671, 0.2563, 0.2759])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKrhCf_lvmZk"
      },
      "source": [
        "3. We will first investigate the initial learning rate. Run three experiments with the learning rate set to 0.5, 0.05, and 0.01 respectively. The batch size should be set to 128. Train the networks for 15 epochs under each setting."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    # fix random seeds\n",
        "    torch.manual_seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "\n",
        "    # train val test\n",
        "    # AI6103 students: You need to create the dataloaders youself\n",
        "    train_loader, valid_loader = get_train_valid_loader(args.batch_size,args.seed)\n",
        "    if args.test:\n",
        "        test_loader = get_test_loader(args.batch_size)\n",
        "\n",
        "    # model\n",
        "    model = MobileNet(100)\n",
        "    print(model)\n",
        "    model.cuda()\n",
        "\n",
        "    # criterion\n",
        "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.wd)\n",
        "    if args.lr_scheduler:\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs)\n",
        "    else:\n",
        "        scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1.0, total_iters=args.epochs)\n",
        "\n",
        "    stat_training_loss = []\n",
        "    stat_val_loss = []\n",
        "    stat_training_acc = []\n",
        "    stat_val_acc = []\n",
        "    for epoch in range(args.epochs):\n",
        "        training_loss = 0\n",
        "        training_acc = 0\n",
        "        training_samples = 0\n",
        "        val_loss = 0\n",
        "        val_acc = 0\n",
        "        val_samples = 0\n",
        "        # training\n",
        "        model.train()\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            batch_size = imgs.shape[0]\n",
        "            optimizer.zero_grad()\n",
        "            logits = model.forward(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            _, top_class = logits.topk(1, dim=1)\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            training_acc += torch.sum(equals.type(torch.FloatTensor)).item()\n",
        "            training_loss += batch_size * loss.item()\n",
        "            training_samples += batch_size\n",
        "        # validation\n",
        "        model.eval()\n",
        "        for val_imgs, val_labels in valid_loader:\n",
        "            batch_size = val_imgs.shape[0]\n",
        "            val_logits = model.forward(val_imgs.cuda())\n",
        "            loss = criterion(val_logits, val_labels.cuda())\n",
        "            _, top_class = val_logits.topk(1, dim=1)\n",
        "            equals = top_class == val_labels.cuda().view(*top_class.shape)\n",
        "            val_acc += torch.sum(equals.type(torch.FloatTensor)).item()\n",
        "            val_loss += batch_size * loss.item()\n",
        "            val_samples += batch_size\n",
        "        assert val_samples == 10000\n",
        "        # update stats\n",
        "        stat_training_loss.append(training_loss/training_samples)\n",
        "        stat_val_loss.append(val_loss/val_samples)\n",
        "        stat_training_acc.append(training_acc/training_samples)\n",
        "        stat_val_acc.append(val_acc/val_samples)\n",
        "        # print\n",
        "        print(f\"Epoch {(epoch+1):d}/{args.epochs:d}.. Learning rate: {scheduler.get_lr()[0]:.4f}.. Train loss: {(training_loss/training_samples):.4f}.. Train acc: {(training_acc/training_samples):.4f}.. Val loss: {(val_loss/val_samples):.4f}.. Val acc: {(val_acc/val_samples):.4f}\")\n",
        "        # lr scheduler\n",
        "        scheduler.step()\n",
        "    # plot\n",
        "    plot_loss_acc(stat_training_loss, stat_val_loss, stat_training_acc, stat_val_acc, args.fig_name)\n",
        "    # test\n",
        "    if args.test:\n",
        "        test_loss = 0\n",
        "        test_acc = 0\n",
        "        test_samples = 0\n",
        "        for test_imgs, test_labels in test_loader:\n",
        "            batch_size = test_imgs.shape[0]\n",
        "            test_logits = model.forward(test_imgs.cuda())\n",
        "            test_loss = criterion(test_logits, test_labels.cuda())\n",
        "            _, top_class = test_logits.topk(1, dim=1)\n",
        "            equals = top_class == test_labels.cuda().view(*top_class.shape)\n",
        "            test_acc += torch.sum(equals.type(torch.FloatTensor)).item()\n",
        "            test_loss += batch_size * test_loss.item()\n",
        "            test_samples += batch_size\n",
        "        assert test_samples == 10000\n",
        "        print('Test loss: ', test_loss/test_samples)\n",
        "        print('Test acc: ', test_acc/test_samples)\n",
        "\n",
        "\n",
        "def get_test_loader(batch_size):\n",
        "    # Define transformations\n",
        "    transformTest = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ])\n",
        "    testDataTransformed = datasets.CIFAR100(\n",
        "        root='./data', train=False, download=True, transform=transformTest\n",
        "    )\n",
        "    testData = DataLoader(testDataTransformed, batch_size=batch_size,shuffle=False)\n",
        "    return testData\n",
        "\n",
        "def get_train_valid_loader(batch_size,seed):\n",
        "    # Define transformations\n",
        "    transformation = transforms.Compose([\n",
        "        transforms.Pad(4),\n",
        "        transforms.RandomCrop(32),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ])\n",
        "\n",
        "    # Apply transformations to training data\n",
        "    trainDataTransformed = datasets.CIFAR100(\n",
        "        root='./data', train=True, download=True, transform=transformation\n",
        "    )\n",
        "    TrainData, ValidationData = random_split(trainDataTransformed, [trainSize, validationSize], generator=torch.Generator().manual_seed(seed))\n",
        "    trainingData = DataLoader(TrainData, batch_size=batch_size,shuffle=True)\n",
        "    validationData  = DataLoader(ValidationData, batch_size=batch_size,shuffle=True)\n",
        "    return trainingData,validationData"
      ],
      "metadata": {
        "id": "vvEsCL9ywOax"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FV_iuQIsdq8m"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    '''Depthwise conv + Pointwise conv'''\n",
        "    def __init__(self, in_planes, out_planes, stride=1):\n",
        "        super(Block, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=in_planes, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        return out\n",
        "\n",
        "\n",
        "class MobileNet(nn.Module):\n",
        "    # (128,2) means conv planes=128, conv stride=2, by default conv stride=1\n",
        "    cfg = [64, (128,2), 128, (256,2), 256, (512,2), 512, 512, 512, 512, 512, (1024,2), 1024]\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(MobileNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.layers = self._make_layers(in_planes=32)\n",
        "        self.linear = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def _make_layers(self, in_planes):\n",
        "        layers = []\n",
        "        for x in self.cfg:\n",
        "            out_planes = x if isinstance(x, int) else x[0]\n",
        "            stride = 1 if isinstance(x, int) else x[1]\n",
        "            layers.append(Block(in_planes, out_planes, stride))\n",
        "            in_planes = out_planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layers(out)\n",
        "        out = F.avg_pool2d(out, 2)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "218t1srZd1hB"
      },
      "outputs": [],
      "source": [
        "def plot_loss_acc(train_loss, val_loss, train_acc, val_acc, fig_name):\n",
        "    x = np.arange(len(train_loss))\n",
        "    max_loss = max(max(train_loss), max(val_loss))\n",
        "\n",
        "    fig, ax1 = plt.subplots()\n",
        "    ax1.set_xlabel('epoch')\n",
        "    ax1.set_ylabel('loss')\n",
        "    ax1.set_ylim([0,max_loss+1])\n",
        "    lns1 = ax1.plot(x, train_loss, 'yo-', label='train_loss')\n",
        "    lns2 = ax1.plot(x, val_loss, 'go-', label='val_loss')\n",
        "    # ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.set_ylabel('accuracy')\n",
        "    ax2.set_ylim([0,1])\n",
        "    lns3 = ax2.plot(x, train_acc, 'bo-', label='train_acc')\n",
        "    lns4 = ax2.plot(x, val_acc, 'ro-', label='val_acc')\n",
        "    # ax2.tick_params(axis='y', labelcolor='tab:red')\n",
        "\n",
        "    lns = lns1+lns2+lns3+lns4\n",
        "    labs = [l.get_label() for l in lns]\n",
        "    ax2.legend(lns, labs, loc=0)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.title(fig_name)\n",
        "\n",
        "    plt.savefig(f'{fig_name}.png')\n",
        "    files.download(f'{fig_name}.png')\n",
        "    # Directory where you want to save the .npz file\n",
        "    directory = './content'\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    np.savez(os.path.join(directory, f'{fig_name}.npz'), train_loss=train_loss, val_loss=val_loss, train_acc=train_acc, val_acc=val_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    def __init__(self,batchSize, learningRate,epoch,weightDecay,learningRateScheduler,figureName):\n",
        "        # Define the default values for your arguments\n",
        "        self.batch_size = batchSize\n",
        "        self.lr = learningRate\n",
        "        self.epochs = epoch\n",
        "        self.wd=weightDecay\n",
        "        self.lr_scheduler=learningRateScheduler\n",
        "        self.fig_name=figureName\n",
        "        self.test=True\n",
        "        self.seed=0"
      ],
      "metadata": {
        "id": "hmdUp0MhsK97"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Reinitialize the model and optimizer for each learning rate\n",
        "learning_rates = [0.5, 0.05, 0.01]\n",
        "epochQ3 = 15\n",
        "for lr in learning_rates:\n",
        "    main(Args(128,lr,epochQ3,0,False,f\"Accuracy and Loss (Learning Rate = {lr})\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R4UkNT5F68MK",
        "outputId": "98f0407b-9c00-41a9-8729-b8090c4af9bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "MobileNet(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layers): Sequential(\n",
            "    (0): Block(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (6): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (7): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (8): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (9): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (10): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (11): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (12): Block(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=1024, out_features=100, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:486: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15.. Learning rate: 0.5000.. Train loss: 4.6016.. Train acc: 0.0156.. Val loss: 4.4590.. Val acc: 0.0275\n",
            "Epoch 2/15.. Learning rate: 0.5000.. Train loss: 4.2118.. Train acc: 0.0469.. Val loss: 4.1078.. Val acc: 0.0588\n",
            "Epoch 3/15.. Learning rate: 0.5000.. Train loss: 3.9210.. Train acc: 0.0793.. Val loss: 3.7865.. Val acc: 0.0979\n",
            "Epoch 4/15.. Learning rate: 0.5000.. Train loss: 3.6525.. Train acc: 0.1195.. Val loss: 3.5923.. Val acc: 0.1327\n",
            "Epoch 5/15.. Learning rate: 0.5000.. Train loss: 3.4069.. Train acc: 0.1603.. Val loss: 3.5537.. Val acc: 0.1495\n",
            "Epoch 6/15.. Learning rate: 0.5000.. Train loss: 3.1838.. Train acc: 0.2004.. Val loss: 3.3122.. Val acc: 0.1901\n",
            "Epoch 7/15.. Learning rate: 0.5000.. Train loss: 2.9909.. Train acc: 0.2349.. Val loss: 3.2752.. Val acc: 0.1912\n",
            "Epoch 8/15.. Learning rate: 0.5000.. Train loss: 2.8171.. Train acc: 0.2698.. Val loss: 2.9834.. Val acc: 0.2589\n",
            "Epoch 9/15.. Learning rate: 0.5000.. Train loss: 2.6770.. Train acc: 0.2992.. Val loss: 2.8971.. Val acc: 0.2689\n",
            "Epoch 10/15.. Learning rate: 0.5000.. Train loss: 2.5430.. Train acc: 0.3293.. Val loss: 2.6722.. Val acc: 0.3136\n",
            "Epoch 11/15.. Learning rate: 0.5000.. Train loss: 2.4279.. Train acc: 0.3545.. Val loss: 2.6588.. Val acc: 0.3177\n",
            "Epoch 12/15.. Learning rate: 0.5000.. Train loss: 2.3441.. Train acc: 0.3682.. Val loss: 2.6762.. Val acc: 0.3208\n",
            "Epoch 13/15.. Learning rate: 0.5000.. Train loss: 2.2414.. Train acc: 0.3937.. Val loss: 2.3941.. Val acc: 0.3690\n",
            "Epoch 14/15.. Learning rate: 0.5000.. Train loss: 2.1731.. Train acc: 0.4105.. Val loss: 2.3568.. Val acc: 0.3790\n",
            "Epoch 15/15.. Learning rate: 0.5000.. Train loss: 2.0919.. Train acc: 0.4289.. Val loss: 2.4005.. Val acc: 0.3751\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8bd88ff1-76c4-4349-a4c6-901040a6aaa1\", \"Accuracy and Loss (Learning Rate = 0.5).png\", 45844)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss:  tensor(0.0040, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Test acc:  0.3992\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "MobileNet(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layers): Sequential(\n",
            "    (0): Block(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (6): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (7): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (8): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (9): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (10): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (11): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (12): Block(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=1024, out_features=100, bias=True)\n",
            ")\n",
            "Epoch 1/15.. Learning rate: 0.0500.. Train loss: 4.1953.. Train acc: 0.0614.. Val loss: 3.9311.. Val acc: 0.0898\n",
            "Epoch 2/15.. Learning rate: 0.0500.. Train loss: 3.6726.. Train acc: 0.1258.. Val loss: 3.5276.. Val acc: 0.1504\n",
            "Epoch 3/15.. Learning rate: 0.0500.. Train loss: 3.3727.. Train acc: 0.1783.. Val loss: 3.3220.. Val acc: 0.1871\n",
            "Epoch 4/15.. Learning rate: 0.0500.. Train loss: 3.1293.. Train acc: 0.2248.. Val loss: 3.0187.. Val acc: 0.2423\n",
            "Epoch 5/15.. Learning rate: 0.0500.. Train loss: 2.9283.. Train acc: 0.2584.. Val loss: 2.8833.. Val acc: 0.2757\n",
            "Epoch 6/15.. Learning rate: 0.0500.. Train loss: 2.7428.. Train acc: 0.2946.. Val loss: 2.7538.. Val acc: 0.3036\n",
            "Epoch 7/15.. Learning rate: 0.0500.. Train loss: 2.5710.. Train acc: 0.3286.. Val loss: 2.6697.. Val acc: 0.3108\n",
            "Epoch 8/15.. Learning rate: 0.0500.. Train loss: 2.4344.. Train acc: 0.3560.. Val loss: 2.6083.. Val acc: 0.3322\n",
            "Epoch 9/15.. Learning rate: 0.0500.. Train loss: 2.2868.. Train acc: 0.3874.. Val loss: 2.4311.. Val acc: 0.3626\n",
            "Epoch 10/15.. Learning rate: 0.0500.. Train loss: 2.1759.. Train acc: 0.4105.. Val loss: 2.3684.. Val acc: 0.3838\n",
            "Epoch 11/15.. Learning rate: 0.0500.. Train loss: 2.0758.. Train acc: 0.4347.. Val loss: 2.4083.. Val acc: 0.3800\n",
            "Epoch 12/15.. Learning rate: 0.0500.. Train loss: 1.9837.. Train acc: 0.4528.. Val loss: 2.2711.. Val acc: 0.4022\n",
            "Epoch 13/15.. Learning rate: 0.0500.. Train loss: 1.8926.. Train acc: 0.4752.. Val loss: 2.1973.. Val acc: 0.4259\n",
            "Epoch 14/15.. Learning rate: 0.0500.. Train loss: 1.8208.. Train acc: 0.4923.. Val loss: 2.1446.. Val acc: 0.4374\n",
            "Epoch 15/15.. Learning rate: 0.0500.. Train loss: 1.7398.. Train acc: 0.5097.. Val loss: 2.1113.. Val acc: 0.4465\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7a61581b-693b-465b-af1a-8dfae1600d4c\", \"Accuracy and Loss (Learning Rate = 0.05).png\", 45413)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss:  tensor(0.0032, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Test acc:  0.4648\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "MobileNet(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layers): Sequential(\n",
            "    (0): Block(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (6): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (7): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (8): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (9): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (10): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (11): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (12): Block(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=1024, out_features=100, bias=True)\n",
            ")\n",
            "Epoch 1/15.. Learning rate: 0.0100.. Train loss: 4.3634.. Train acc: 0.0404.. Val loss: 4.0265.. Val acc: 0.0765\n",
            "Epoch 2/15.. Learning rate: 0.0100.. Train loss: 3.8370.. Train acc: 0.1038.. Val loss: 3.6757.. Val acc: 0.1229\n",
            "Epoch 3/15.. Learning rate: 0.0100.. Train loss: 3.5426.. Train acc: 0.1501.. Val loss: 3.4451.. Val acc: 0.1672\n",
            "Epoch 4/15.. Learning rate: 0.0100.. Train loss: 3.3239.. Train acc: 0.1905.. Val loss: 3.2357.. Val acc: 0.2082\n",
            "Epoch 5/15.. Learning rate: 0.0100.. Train loss: 3.1464.. Train acc: 0.2192.. Val loss: 3.1552.. Val acc: 0.2254\n",
            "Epoch 6/15.. Learning rate: 0.0100.. Train loss: 3.0028.. Train acc: 0.2468.. Val loss: 3.0071.. Val acc: 0.2555\n",
            "Epoch 7/15.. Learning rate: 0.0100.. Train loss: 2.8581.. Train acc: 0.2767.. Val loss: 2.8983.. Val acc: 0.2667\n",
            "Epoch 8/15.. Learning rate: 0.0100.. Train loss: 2.7363.. Train acc: 0.3007.. Val loss: 2.8570.. Val acc: 0.2881\n",
            "Epoch 9/15.. Learning rate: 0.0100.. Train loss: 2.6252.. Train acc: 0.3212.. Val loss: 2.7254.. Val acc: 0.3076\n",
            "Epoch 10/15.. Learning rate: 0.0100.. Train loss: 2.5123.. Train acc: 0.3454.. Val loss: 2.6609.. Val acc: 0.3245\n",
            "Epoch 11/15.. Learning rate: 0.0100.. Train loss: 2.4126.. Train acc: 0.3654.. Val loss: 2.6246.. Val acc: 0.3278\n",
            "Epoch 12/15.. Learning rate: 0.0100.. Train loss: 2.3126.. Train acc: 0.3837.. Val loss: 2.5318.. Val acc: 0.3531\n",
            "Epoch 13/15.. Learning rate: 0.0100.. Train loss: 2.2118.. Train acc: 0.4078.. Val loss: 2.4641.. Val acc: 0.3624\n",
            "Epoch 14/15.. Learning rate: 0.0100.. Train loss: 2.1255.. Train acc: 0.4242.. Val loss: 2.3980.. Val acc: 0.3816\n",
            "Epoch 15/15.. Learning rate: 0.0100.. Train loss: 2.0283.. Train acc: 0.4477.. Val loss: 2.3860.. Val acc: 0.3810\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_dad39615-acc7-4765-bc43-0a79935e8076\", \"Accuracy and Loss (Learning Rate = 0.01).png\", 43458)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss:  tensor(0.0035, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Test acc:  0.4053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Next, we gradually decrease the learning rate. One effective learning rate schedule is cosine annealing. Describe this particular schedule intuitively and with one or more mathematical equations (5%). Use the best learning rate identified earlier as the initial learning rate and keep all other settings and hyperparameters unchanged. Conduct experiments under two settings: (1) train for 300 epochs with the learning rate held constant, and (2) train for 300 epochs with cosine annealing, which decreases the initial learning rate to zero over the entirety of the training session."
      ],
      "metadata": {
        "id": "diJkBdmMreYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochQ4 = 300\n",
        "#Cosine Scheduler\n",
        "main(Args(128,0.05,epochQ4,0,True,\"Accuracy and Loss with Cosine Annealing learning rate schedule\"))\n",
        "#No Scheduler\n",
        "main(Args(128,0.05,epochQ4,0,False,\"Accuracy and Loss without learning rate schedule\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KOC0PMz4rg2c",
        "outputId": "09acae9d-26df-4ec3-a588-bdb952774610"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "MobileNet(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layers): Sequential(\n",
            "    (0): Block(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (6): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (7): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (8): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (9): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (10): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (11): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (12): Block(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=1024, out_features=100, bias=True)\n",
            ")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:809: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300.. Learning rate: 0.0500.. Train loss: 4.1953.. Train acc: 0.0614.. Val loss: 3.9311.. Val acc: 0.0898\n",
            "Epoch 2/300.. Learning rate: 0.0500.. Train loss: 3.6738.. Train acc: 0.1269.. Val loss: 3.5207.. Val acc: 0.1525\n",
            "Epoch 3/300.. Learning rate: 0.0500.. Train loss: 3.3706.. Train acc: 0.1792.. Val loss: 3.3240.. Val acc: 0.1863\n",
            "Epoch 4/300.. Learning rate: 0.0500.. Train loss: 3.1469.. Train acc: 0.2220.. Val loss: 3.0946.. Val acc: 0.2336\n",
            "Epoch 5/300.. Learning rate: 0.0500.. Train loss: 2.9350.. Train acc: 0.2587.. Val loss: 2.9913.. Val acc: 0.2672\n",
            "Epoch 6/300.. Learning rate: 0.0500.. Train loss: 2.7561.. Train acc: 0.2941.. Val loss: 2.7416.. Val acc: 0.2997\n",
            "Epoch 7/300.. Learning rate: 0.0499.. Train loss: 2.5856.. Train acc: 0.3256.. Val loss: 2.6451.. Val acc: 0.3149\n",
            "Epoch 8/300.. Learning rate: 0.0499.. Train loss: 2.4397.. Train acc: 0.3523.. Val loss: 2.5311.. Val acc: 0.3456\n",
            "Epoch 9/300.. Learning rate: 0.0499.. Train loss: 2.3150.. Train acc: 0.3799.. Val loss: 2.4845.. Val acc: 0.3493\n",
            "Epoch 10/300.. Learning rate: 0.0499.. Train loss: 2.2071.. Train acc: 0.4034.. Val loss: 2.3039.. Val acc: 0.3895\n",
            "Epoch 11/300.. Learning rate: 0.0498.. Train loss: 2.0810.. Train acc: 0.4332.. Val loss: 2.3445.. Val acc: 0.3869\n",
            "Epoch 12/300.. Learning rate: 0.0498.. Train loss: 1.9867.. Train acc: 0.4528.. Val loss: 2.2416.. Val acc: 0.4042\n",
            "Epoch 13/300.. Learning rate: 0.0498.. Train loss: 1.8895.. Train acc: 0.4749.. Val loss: 2.2226.. Val acc: 0.4324\n",
            "Epoch 14/300.. Learning rate: 0.0497.. Train loss: 1.8216.. Train acc: 0.4913.. Val loss: 2.1173.. Val acc: 0.4335\n",
            "Epoch 15/300.. Learning rate: 0.0497.. Train loss: 1.7377.. Train acc: 0.5107.. Val loss: 2.1230.. Val acc: 0.4385\n",
            "Epoch 16/300.. Learning rate: 0.0497.. Train loss: 1.6732.. Train acc: 0.5278.. Val loss: 2.1407.. Val acc: 0.4440\n",
            "Epoch 17/300.. Learning rate: 0.0496.. Train loss: 1.6047.. Train acc: 0.5431.. Val loss: 2.0565.. Val acc: 0.4647\n",
            "Epoch 18/300.. Learning rate: 0.0496.. Train loss: 1.5346.. Train acc: 0.5604.. Val loss: 2.1448.. Val acc: 0.4573\n",
            "Epoch 19/300.. Learning rate: 0.0495.. Train loss: 1.4792.. Train acc: 0.5710.. Val loss: 1.9880.. Val acc: 0.4847\n",
            "Epoch 20/300.. Learning rate: 0.0495.. Train loss: 1.4168.. Train acc: 0.5889.. Val loss: 1.9956.. Val acc: 0.4883\n",
            "Epoch 21/300.. Learning rate: 0.0494.. Train loss: 1.3582.. Train acc: 0.6024.. Val loss: 1.9528.. Val acc: 0.4865\n",
            "Epoch 22/300.. Learning rate: 0.0493.. Train loss: 1.3125.. Train acc: 0.6106.. Val loss: 1.9677.. Val acc: 0.4911\n",
            "Epoch 23/300.. Learning rate: 0.0493.. Train loss: 1.2508.. Train acc: 0.6286.. Val loss: 1.9414.. Val acc: 0.4963\n",
            "Epoch 24/300.. Learning rate: 0.0492.. Train loss: 1.1926.. Train acc: 0.6416.. Val loss: 2.0063.. Val acc: 0.4944\n",
            "Epoch 25/300.. Learning rate: 0.0492.. Train loss: 1.1434.. Train acc: 0.6552.. Val loss: 1.9458.. Val acc: 0.5086\n",
            "Epoch 26/300.. Learning rate: 0.0491.. Train loss: 1.0944.. Train acc: 0.6679.. Val loss: 2.0017.. Val acc: 0.4997\n",
            "Epoch 27/300.. Learning rate: 0.0490.. Train loss: 1.0465.. Train acc: 0.6804.. Val loss: 2.0560.. Val acc: 0.4941\n",
            "Epoch 28/300.. Learning rate: 0.0489.. Train loss: 0.9946.. Train acc: 0.6921.. Val loss: 2.1204.. Val acc: 0.4924\n",
            "Epoch 29/300.. Learning rate: 0.0489.. Train loss: 0.9503.. Train acc: 0.7042.. Val loss: 2.0982.. Val acc: 0.4966\n",
            "Epoch 30/300.. Learning rate: 0.0488.. Train loss: 0.9046.. Train acc: 0.7182.. Val loss: 2.0611.. Val acc: 0.5057\n",
            "Epoch 31/300.. Learning rate: 0.0487.. Train loss: 0.8579.. Train acc: 0.7329.. Val loss: 2.0514.. Val acc: 0.5112\n",
            "Epoch 32/300.. Learning rate: 0.0486.. Train loss: 0.8113.. Train acc: 0.7465.. Val loss: 2.1850.. Val acc: 0.5014\n",
            "Epoch 33/300.. Learning rate: 0.0485.. Train loss: 0.7657.. Train acc: 0.7579.. Val loss: 2.1725.. Val acc: 0.5111\n",
            "Epoch 34/300.. Learning rate: 0.0484.. Train loss: 0.7235.. Train acc: 0.7677.. Val loss: 2.1148.. Val acc: 0.5159\n",
            "Epoch 35/300.. Learning rate: 0.0483.. Train loss: 0.6649.. Train acc: 0.7872.. Val loss: 2.2736.. Val acc: 0.5028\n",
            "Epoch 36/300.. Learning rate: 0.0482.. Train loss: 0.6384.. Train acc: 0.7944.. Val loss: 2.2067.. Val acc: 0.5149\n",
            "Epoch 37/300.. Learning rate: 0.0481.. Train loss: 0.6015.. Train acc: 0.8047.. Val loss: 2.2352.. Val acc: 0.5089\n",
            "Epoch 38/300.. Learning rate: 0.0480.. Train loss: 0.5549.. Train acc: 0.8226.. Val loss: 2.2623.. Val acc: 0.5178\n",
            "Epoch 39/300.. Learning rate: 0.0479.. Train loss: 0.5194.. Train acc: 0.8314.. Val loss: 2.2709.. Val acc: 0.5174\n",
            "Epoch 40/300.. Learning rate: 0.0478.. Train loss: 0.4909.. Train acc: 0.8402.. Val loss: 2.2944.. Val acc: 0.5175\n",
            "Epoch 41/300.. Learning rate: 0.0477.. Train loss: 0.4676.. Train acc: 0.8482.. Val loss: 2.3788.. Val acc: 0.5108\n",
            "Epoch 42/300.. Learning rate: 0.0476.. Train loss: 0.4377.. Train acc: 0.8588.. Val loss: 2.4209.. Val acc: 0.5100\n",
            "Epoch 43/300.. Learning rate: 0.0475.. Train loss: 0.3957.. Train acc: 0.8712.. Val loss: 2.4447.. Val acc: 0.5159\n",
            "Epoch 44/300.. Learning rate: 0.0474.. Train loss: 0.3684.. Train acc: 0.8797.. Val loss: 2.4278.. Val acc: 0.5202\n",
            "Epoch 45/300.. Learning rate: 0.0473.. Train loss: 0.3457.. Train acc: 0.8876.. Val loss: 2.4849.. Val acc: 0.5184\n",
            "Epoch 46/300.. Learning rate: 0.0472.. Train loss: 0.3328.. Train acc: 0.8916.. Val loss: 2.5026.. Val acc: 0.5225\n",
            "Epoch 47/300.. Learning rate: 0.0470.. Train loss: 0.3089.. Train acc: 0.8992.. Val loss: 2.5168.. Val acc: 0.5220\n",
            "Epoch 48/300.. Learning rate: 0.0469.. Train loss: 0.2893.. Train acc: 0.9045.. Val loss: 2.5318.. Val acc: 0.5234\n",
            "Epoch 49/300.. Learning rate: 0.0468.. Train loss: 0.2784.. Train acc: 0.9095.. Val loss: 2.5431.. Val acc: 0.5294\n",
            "Epoch 50/300.. Learning rate: 0.0467.. Train loss: 0.2644.. Train acc: 0.9139.. Val loss: 2.5607.. Val acc: 0.5271\n",
            "Epoch 51/300.. Learning rate: 0.0465.. Train loss: 0.2443.. Train acc: 0.9199.. Val loss: 2.5924.. Val acc: 0.5313\n",
            "Epoch 52/300.. Learning rate: 0.0464.. Train loss: 0.2326.. Train acc: 0.9242.. Val loss: 2.6378.. Val acc: 0.5263\n",
            "Epoch 53/300.. Learning rate: 0.0463.. Train loss: 0.2106.. Train acc: 0.9312.. Val loss: 2.6801.. Val acc: 0.5306\n",
            "Epoch 54/300.. Learning rate: 0.0461.. Train loss: 0.2057.. Train acc: 0.9338.. Val loss: 2.6833.. Val acc: 0.5268\n",
            "Epoch 55/300.. Learning rate: 0.0460.. Train loss: 0.1940.. Train acc: 0.9358.. Val loss: 2.7212.. Val acc: 0.5226\n",
            "Epoch 56/300.. Learning rate: 0.0458.. Train loss: 0.1846.. Train acc: 0.9388.. Val loss: 2.8002.. Val acc: 0.5195\n",
            "Epoch 57/300.. Learning rate: 0.0457.. Train loss: 0.1753.. Train acc: 0.9426.. Val loss: 2.6824.. Val acc: 0.5373\n",
            "Epoch 58/300.. Learning rate: 0.0455.. Train loss: 0.1630.. Train acc: 0.9466.. Val loss: 2.7609.. Val acc: 0.5354\n",
            "Epoch 59/300.. Learning rate: 0.0454.. Train loss: 0.1521.. Train acc: 0.9504.. Val loss: 2.7312.. Val acc: 0.5405\n",
            "Epoch 60/300.. Learning rate: 0.0452.. Train loss: 0.1527.. Train acc: 0.9503.. Val loss: 2.7765.. Val acc: 0.5320\n",
            "Epoch 61/300.. Learning rate: 0.0451.. Train loss: 0.1393.. Train acc: 0.9558.. Val loss: 2.8518.. Val acc: 0.5312\n",
            "Epoch 62/300.. Learning rate: 0.0449.. Train loss: 0.1355.. Train acc: 0.9571.. Val loss: 2.7953.. Val acc: 0.5412\n",
            "Epoch 63/300.. Learning rate: 0.0448.. Train loss: 0.1256.. Train acc: 0.9584.. Val loss: 2.8247.. Val acc: 0.5312\n",
            "Epoch 64/300.. Learning rate: 0.0446.. Train loss: 0.1182.. Train acc: 0.9610.. Val loss: 2.8733.. Val acc: 0.5396\n",
            "Epoch 65/300.. Learning rate: 0.0444.. Train loss: 0.1058.. Train acc: 0.9662.. Val loss: 2.8536.. Val acc: 0.5370\n",
            "Epoch 66/300.. Learning rate: 0.0443.. Train loss: 0.1046.. Train acc: 0.9651.. Val loss: 2.8518.. Val acc: 0.5363\n",
            "Epoch 67/300.. Learning rate: 0.0441.. Train loss: 0.1051.. Train acc: 0.9667.. Val loss: 2.9086.. Val acc: 0.5331\n",
            "Epoch 68/300.. Learning rate: 0.0439.. Train loss: 0.1013.. Train acc: 0.9676.. Val loss: 2.8948.. Val acc: 0.5311\n",
            "Epoch 69/300.. Learning rate: 0.0438.. Train loss: 0.0980.. Train acc: 0.9684.. Val loss: 2.9667.. Val acc: 0.5369\n",
            "Epoch 70/300.. Learning rate: 0.0436.. Train loss: 0.0937.. Train acc: 0.9697.. Val loss: 2.9065.. Val acc: 0.5375\n",
            "Epoch 71/300.. Learning rate: 0.0434.. Train loss: 0.0862.. Train acc: 0.9728.. Val loss: 2.9644.. Val acc: 0.5381\n",
            "Epoch 72/300.. Learning rate: 0.0432.. Train loss: 0.0853.. Train acc: 0.9738.. Val loss: 2.9369.. Val acc: 0.5456\n",
            "Epoch 73/300.. Learning rate: 0.0430.. Train loss: 0.0796.. Train acc: 0.9755.. Val loss: 2.9380.. Val acc: 0.5447\n",
            "Epoch 74/300.. Learning rate: 0.0429.. Train loss: 0.0755.. Train acc: 0.9755.. Val loss: 2.9943.. Val acc: 0.5386\n",
            "Epoch 75/300.. Learning rate: 0.0427.. Train loss: 0.0722.. Train acc: 0.9768.. Val loss: 2.9867.. Val acc: 0.5440\n",
            "Epoch 76/300.. Learning rate: 0.0425.. Train loss: 0.0658.. Train acc: 0.9791.. Val loss: 3.0141.. Val acc: 0.5392\n",
            "Epoch 77/300.. Learning rate: 0.0423.. Train loss: 0.0668.. Train acc: 0.9794.. Val loss: 3.0519.. Val acc: 0.5366\n",
            "Epoch 78/300.. Learning rate: 0.0421.. Train loss: 0.0656.. Train acc: 0.9799.. Val loss: 3.0103.. Val acc: 0.5382\n",
            "Epoch 79/300.. Learning rate: 0.0419.. Train loss: 0.0595.. Train acc: 0.9816.. Val loss: 3.0413.. Val acc: 0.5407\n",
            "Epoch 80/300.. Learning rate: 0.0417.. Train loss: 0.0607.. Train acc: 0.9812.. Val loss: 2.9833.. Val acc: 0.5427\n",
            "Epoch 81/300.. Learning rate: 0.0415.. Train loss: 0.0516.. Train acc: 0.9843.. Val loss: 3.0250.. Val acc: 0.5409\n",
            "Epoch 82/300.. Learning rate: 0.0413.. Train loss: 0.0569.. Train acc: 0.9814.. Val loss: 3.0514.. Val acc: 0.5396\n",
            "Epoch 83/300.. Learning rate: 0.0411.. Train loss: 0.0499.. Train acc: 0.9848.. Val loss: 3.0120.. Val acc: 0.5426\n",
            "Epoch 84/300.. Learning rate: 0.0409.. Train loss: 0.0455.. Train acc: 0.9856.. Val loss: 3.0288.. Val acc: 0.5435\n",
            "Epoch 85/300.. Learning rate: 0.0407.. Train loss: 0.0467.. Train acc: 0.9856.. Val loss: 3.0844.. Val acc: 0.5399\n",
            "Epoch 86/300.. Learning rate: 0.0405.. Train loss: 0.0409.. Train acc: 0.9872.. Val loss: 3.0380.. Val acc: 0.5487\n",
            "Epoch 87/300.. Learning rate: 0.0403.. Train loss: 0.0394.. Train acc: 0.9880.. Val loss: 3.0775.. Val acc: 0.5467\n",
            "Epoch 88/300.. Learning rate: 0.0401.. Train loss: 0.0382.. Train acc: 0.9891.. Val loss: 3.0725.. Val acc: 0.5450\n",
            "Epoch 89/300.. Learning rate: 0.0399.. Train loss: 0.0351.. Train acc: 0.9891.. Val loss: 3.0800.. Val acc: 0.5446\n",
            "Epoch 90/300.. Learning rate: 0.0397.. Train loss: 0.0366.. Train acc: 0.9894.. Val loss: 3.1308.. Val acc: 0.5443\n",
            "Epoch 91/300.. Learning rate: 0.0395.. Train loss: 0.0341.. Train acc: 0.9898.. Val loss: 3.1357.. Val acc: 0.5466\n",
            "Epoch 92/300.. Learning rate: 0.0393.. Train loss: 0.0299.. Train acc: 0.9912.. Val loss: 3.0621.. Val acc: 0.5492\n",
            "Epoch 93/300.. Learning rate: 0.0391.. Train loss: 0.0331.. Train acc: 0.9898.. Val loss: 3.1144.. Val acc: 0.5478\n",
            "Epoch 94/300.. Learning rate: 0.0388.. Train loss: 0.0328.. Train acc: 0.9905.. Val loss: 3.2110.. Val acc: 0.5384\n",
            "Epoch 95/300.. Learning rate: 0.0386.. Train loss: 0.0288.. Train acc: 0.9916.. Val loss: 3.0725.. Val acc: 0.5523\n",
            "Epoch 96/300.. Learning rate: 0.0384.. Train loss: 0.0278.. Train acc: 0.9920.. Val loss: 3.0868.. Val acc: 0.5499\n",
            "Epoch 97/300.. Learning rate: 0.0382.. Train loss: 0.0308.. Train acc: 0.9905.. Val loss: 3.1699.. Val acc: 0.5444\n",
            "Epoch 98/300.. Learning rate: 0.0380.. Train loss: 0.0291.. Train acc: 0.9909.. Val loss: 3.1582.. Val acc: 0.5482\n",
            "Epoch 99/300.. Learning rate: 0.0377.. Train loss: 0.0265.. Train acc: 0.9919.. Val loss: 3.1468.. Val acc: 0.5501\n",
            "Epoch 100/300.. Learning rate: 0.0375.. Train loss: 0.0232.. Train acc: 0.9932.. Val loss: 3.1685.. Val acc: 0.5524\n",
            "Epoch 101/300.. Learning rate: 0.0373.. Train loss: 0.0218.. Train acc: 0.9938.. Val loss: 3.1734.. Val acc: 0.5521\n",
            "Epoch 102/300.. Learning rate: 0.0370.. Train loss: 0.0225.. Train acc: 0.9933.. Val loss: 3.1724.. Val acc: 0.5465\n",
            "Epoch 103/300.. Learning rate: 0.0368.. Train loss: 0.0237.. Train acc: 0.9931.. Val loss: 3.2040.. Val acc: 0.5516\n",
            "Epoch 104/300.. Learning rate: 0.0366.. Train loss: 0.0192.. Train acc: 0.9944.. Val loss: 3.1516.. Val acc: 0.5546\n",
            "Epoch 105/300.. Learning rate: 0.0364.. Train loss: 0.0180.. Train acc: 0.9950.. Val loss: 3.1851.. Val acc: 0.5477\n",
            "Epoch 106/300.. Learning rate: 0.0361.. Train loss: 0.0199.. Train acc: 0.9943.. Val loss: 3.1952.. Val acc: 0.5490\n",
            "Epoch 107/300.. Learning rate: 0.0359.. Train loss: 0.0200.. Train acc: 0.9943.. Val loss: 3.1944.. Val acc: 0.5589\n",
            "Epoch 108/300.. Learning rate: 0.0356.. Train loss: 0.0167.. Train acc: 0.9952.. Val loss: 3.2218.. Val acc: 0.5524\n",
            "Epoch 109/300.. Learning rate: 0.0354.. Train loss: 0.0156.. Train acc: 0.9958.. Val loss: 3.1887.. Val acc: 0.5544\n",
            "Epoch 110/300.. Learning rate: 0.0352.. Train loss: 0.0147.. Train acc: 0.9960.. Val loss: 3.1841.. Val acc: 0.5575\n",
            "Epoch 111/300.. Learning rate: 0.0349.. Train loss: 0.0141.. Train acc: 0.9961.. Val loss: 3.2235.. Val acc: 0.5547\n",
            "Epoch 112/300.. Learning rate: 0.0347.. Train loss: 0.0138.. Train acc: 0.9961.. Val loss: 3.2293.. Val acc: 0.5486\n",
            "Epoch 113/300.. Learning rate: 0.0344.. Train loss: 0.0134.. Train acc: 0.9960.. Val loss: 3.2252.. Val acc: 0.5550\n",
            "Epoch 114/300.. Learning rate: 0.0342.. Train loss: 0.0126.. Train acc: 0.9964.. Val loss: 3.2683.. Val acc: 0.5515\n",
            "Epoch 115/300.. Learning rate: 0.0340.. Train loss: 0.0132.. Train acc: 0.9962.. Val loss: 3.2424.. Val acc: 0.5478\n",
            "Epoch 116/300.. Learning rate: 0.0337.. Train loss: 0.0123.. Train acc: 0.9966.. Val loss: 3.2720.. Val acc: 0.5494\n",
            "Epoch 117/300.. Learning rate: 0.0335.. Train loss: 0.0134.. Train acc: 0.9962.. Val loss: 3.2054.. Val acc: 0.5553\n",
            "Epoch 118/300.. Learning rate: 0.0332.. Train loss: 0.0111.. Train acc: 0.9970.. Val loss: 3.2178.. Val acc: 0.5546\n",
            "Epoch 119/300.. Learning rate: 0.0330.. Train loss: 0.0112.. Train acc: 0.9969.. Val loss: 3.2120.. Val acc: 0.5578\n",
            "Epoch 120/300.. Learning rate: 0.0327.. Train loss: 0.0114.. Train acc: 0.9970.. Val loss: 3.1982.. Val acc: 0.5566\n",
            "Epoch 121/300.. Learning rate: 0.0325.. Train loss: 0.0105.. Train acc: 0.9971.. Val loss: 3.2376.. Val acc: 0.5550\n",
            "Epoch 122/300.. Learning rate: 0.0322.. Train loss: 0.0092.. Train acc: 0.9976.. Val loss: 3.2535.. Val acc: 0.5554\n",
            "Epoch 123/300.. Learning rate: 0.0320.. Train loss: 0.0095.. Train acc: 0.9974.. Val loss: 3.1931.. Val acc: 0.5625\n",
            "Epoch 124/300.. Learning rate: 0.0317.. Train loss: 0.0101.. Train acc: 0.9970.. Val loss: 3.3123.. Val acc: 0.5533\n",
            "Epoch 125/300.. Learning rate: 0.0315.. Train loss: 0.0084.. Train acc: 0.9978.. Val loss: 3.2400.. Val acc: 0.5559\n",
            "Epoch 126/300.. Learning rate: 0.0312.. Train loss: 0.0077.. Train acc: 0.9979.. Val loss: 3.2640.. Val acc: 0.5606\n",
            "Epoch 127/300.. Learning rate: 0.0310.. Train loss: 0.0085.. Train acc: 0.9974.. Val loss: 3.2895.. Val acc: 0.5552\n",
            "Epoch 128/300.. Learning rate: 0.0307.. Train loss: 0.0070.. Train acc: 0.9985.. Val loss: 3.2610.. Val acc: 0.5598\n",
            "Epoch 129/300.. Learning rate: 0.0305.. Train loss: 0.0083.. Train acc: 0.9975.. Val loss: 3.2758.. Val acc: 0.5555\n",
            "Epoch 130/300.. Learning rate: 0.0302.. Train loss: 0.0089.. Train acc: 0.9975.. Val loss: 3.2518.. Val acc: 0.5586\n",
            "Epoch 131/300.. Learning rate: 0.0299.. Train loss: 0.0084.. Train acc: 0.9976.. Val loss: 3.2805.. Val acc: 0.5592\n",
            "Epoch 132/300.. Learning rate: 0.0297.. Train loss: 0.0079.. Train acc: 0.9980.. Val loss: 3.2700.. Val acc: 0.5559\n",
            "Epoch 133/300.. Learning rate: 0.0294.. Train loss: 0.0077.. Train acc: 0.9979.. Val loss: 3.2882.. Val acc: 0.5561\n",
            "Epoch 134/300.. Learning rate: 0.0292.. Train loss: 0.0072.. Train acc: 0.9979.. Val loss: 3.2797.. Val acc: 0.5632\n",
            "Epoch 135/300.. Learning rate: 0.0289.. Train loss: 0.0063.. Train acc: 0.9984.. Val loss: 3.3103.. Val acc: 0.5581\n",
            "Epoch 136/300.. Learning rate: 0.0287.. Train loss: 0.0056.. Train acc: 0.9987.. Val loss: 3.2760.. Val acc: 0.5600\n",
            "Epoch 137/300.. Learning rate: 0.0284.. Train loss: 0.0065.. Train acc: 0.9982.. Val loss: 3.2872.. Val acc: 0.5546\n",
            "Epoch 138/300.. Learning rate: 0.0281.. Train loss: 0.0059.. Train acc: 0.9984.. Val loss: 3.2669.. Val acc: 0.5574\n",
            "Epoch 139/300.. Learning rate: 0.0279.. Train loss: 0.0063.. Train acc: 0.9983.. Val loss: 3.2808.. Val acc: 0.5521\n",
            "Epoch 140/300.. Learning rate: 0.0276.. Train loss: 0.0053.. Train acc: 0.9987.. Val loss: 3.3035.. Val acc: 0.5555\n",
            "Epoch 141/300.. Learning rate: 0.0274.. Train loss: 0.0052.. Train acc: 0.9986.. Val loss: 3.3087.. Val acc: 0.5564\n",
            "Epoch 142/300.. Learning rate: 0.0271.. Train loss: 0.0068.. Train acc: 0.9979.. Val loss: 3.3137.. Val acc: 0.5534\n",
            "Epoch 143/300.. Learning rate: 0.0268.. Train loss: 0.0057.. Train acc: 0.9984.. Val loss: 3.2674.. Val acc: 0.5605\n",
            "Epoch 144/300.. Learning rate: 0.0266.. Train loss: 0.0054.. Train acc: 0.9986.. Val loss: 3.2752.. Val acc: 0.5568\n",
            "Epoch 145/300.. Learning rate: 0.0263.. Train loss: 0.0049.. Train acc: 0.9989.. Val loss: 3.3047.. Val acc: 0.5591\n",
            "Epoch 146/300.. Learning rate: 0.0260.. Train loss: 0.0050.. Train acc: 0.9986.. Val loss: 3.2868.. Val acc: 0.5599\n",
            "Epoch 147/300.. Learning rate: 0.0258.. Train loss: 0.0056.. Train acc: 0.9983.. Val loss: 3.3078.. Val acc: 0.5596\n",
            "Epoch 148/300.. Learning rate: 0.0255.. Train loss: 0.0053.. Train acc: 0.9986.. Val loss: 3.2924.. Val acc: 0.5597\n",
            "Epoch 149/300.. Learning rate: 0.0253.. Train loss: 0.0045.. Train acc: 0.9988.. Val loss: 3.3035.. Val acc: 0.5595\n",
            "Epoch 150/300.. Learning rate: 0.0250.. Train loss: 0.0037.. Train acc: 0.9993.. Val loss: 3.3331.. Val acc: 0.5573\n",
            "Epoch 151/300.. Learning rate: 0.0247.. Train loss: 0.0043.. Train acc: 0.9990.. Val loss: 3.2951.. Val acc: 0.5612\n",
            "Epoch 152/300.. Learning rate: 0.0245.. Train loss: 0.0040.. Train acc: 0.9990.. Val loss: 3.2931.. Val acc: 0.5579\n",
            "Epoch 153/300.. Learning rate: 0.0242.. Train loss: 0.0038.. Train acc: 0.9990.. Val loss: 3.2905.. Val acc: 0.5621\n",
            "Epoch 154/300.. Learning rate: 0.0240.. Train loss: 0.0037.. Train acc: 0.9992.. Val loss: 3.3387.. Val acc: 0.5605\n",
            "Epoch 155/300.. Learning rate: 0.0237.. Train loss: 0.0041.. Train acc: 0.9989.. Val loss: 3.2985.. Val acc: 0.5624\n",
            "Epoch 156/300.. Learning rate: 0.0234.. Train loss: 0.0041.. Train acc: 0.9990.. Val loss: 3.2738.. Val acc: 0.5621\n",
            "Epoch 157/300.. Learning rate: 0.0232.. Train loss: 0.0042.. Train acc: 0.9988.. Val loss: 3.2776.. Val acc: 0.5616\n",
            "Epoch 158/300.. Learning rate: 0.0229.. Train loss: 0.0036.. Train acc: 0.9991.. Val loss: 3.2911.. Val acc: 0.5665\n",
            "Epoch 159/300.. Learning rate: 0.0227.. Train loss: 0.0034.. Train acc: 0.9992.. Val loss: 3.3026.. Val acc: 0.5635\n",
            "Epoch 160/300.. Learning rate: 0.0224.. Train loss: 0.0038.. Train acc: 0.9990.. Val loss: 3.3017.. Val acc: 0.5621\n",
            "Epoch 161/300.. Learning rate: 0.0221.. Train loss: 0.0028.. Train acc: 0.9994.. Val loss: 3.3264.. Val acc: 0.5629\n",
            "Epoch 162/300.. Learning rate: 0.0219.. Train loss: 0.0032.. Train acc: 0.9991.. Val loss: 3.3180.. Val acc: 0.5650\n",
            "Epoch 163/300.. Learning rate: 0.0216.. Train loss: 0.0032.. Train acc: 0.9992.. Val loss: 3.2781.. Val acc: 0.5653\n",
            "Epoch 164/300.. Learning rate: 0.0214.. Train loss: 0.0031.. Train acc: 0.9992.. Val loss: 3.3066.. Val acc: 0.5592\n",
            "Epoch 165/300.. Learning rate: 0.0211.. Train loss: 0.0034.. Train acc: 0.9991.. Val loss: 3.3174.. Val acc: 0.5628\n",
            "Epoch 166/300.. Learning rate: 0.0208.. Train loss: 0.0031.. Train acc: 0.9991.. Val loss: 3.2799.. Val acc: 0.5638\n",
            "Epoch 167/300.. Learning rate: 0.0206.. Train loss: 0.0032.. Train acc: 0.9992.. Val loss: 3.3238.. Val acc: 0.5634\n",
            "Epoch 168/300.. Learning rate: 0.0203.. Train loss: 0.0032.. Train acc: 0.9993.. Val loss: 3.3218.. Val acc: 0.5600\n",
            "Epoch 169/300.. Learning rate: 0.0201.. Train loss: 0.0028.. Train acc: 0.9993.. Val loss: 3.3056.. Val acc: 0.5651\n",
            "Epoch 170/300.. Learning rate: 0.0198.. Train loss: 0.0027.. Train acc: 0.9993.. Val loss: 3.2770.. Val acc: 0.5670\n",
            "Epoch 171/300.. Learning rate: 0.0195.. Train loss: 0.0027.. Train acc: 0.9993.. Val loss: 3.3099.. Val acc: 0.5637\n",
            "Epoch 172/300.. Learning rate: 0.0193.. Train loss: 0.0028.. Train acc: 0.9992.. Val loss: 3.3313.. Val acc: 0.5641\n",
            "Epoch 173/300.. Learning rate: 0.0190.. Train loss: 0.0024.. Train acc: 0.9995.. Val loss: 3.3241.. Val acc: 0.5630\n",
            "Epoch 174/300.. Learning rate: 0.0188.. Train loss: 0.0029.. Train acc: 0.9992.. Val loss: 3.3201.. Val acc: 0.5673\n",
            "Epoch 175/300.. Learning rate: 0.0185.. Train loss: 0.0027.. Train acc: 0.9992.. Val loss: 3.3509.. Val acc: 0.5645\n",
            "Epoch 176/300.. Learning rate: 0.0183.. Train loss: 0.0026.. Train acc: 0.9993.. Val loss: 3.3003.. Val acc: 0.5635\n",
            "Epoch 177/300.. Learning rate: 0.0180.. Train loss: 0.0026.. Train acc: 0.9992.. Val loss: 3.3159.. Val acc: 0.5612\n",
            "Epoch 178/300.. Learning rate: 0.0178.. Train loss: 0.0028.. Train acc: 0.9993.. Val loss: 3.3094.. Val acc: 0.5653\n",
            "Epoch 179/300.. Learning rate: 0.0175.. Train loss: 0.0023.. Train acc: 0.9993.. Val loss: 3.3204.. Val acc: 0.5620\n",
            "Epoch 180/300.. Learning rate: 0.0173.. Train loss: 0.0025.. Train acc: 0.9994.. Val loss: 3.3336.. Val acc: 0.5639\n",
            "Epoch 181/300.. Learning rate: 0.0170.. Train loss: 0.0024.. Train acc: 0.9993.. Val loss: 3.3127.. Val acc: 0.5653\n",
            "Epoch 182/300.. Learning rate: 0.0168.. Train loss: 0.0024.. Train acc: 0.9993.. Val loss: 3.3541.. Val acc: 0.5604\n",
            "Epoch 183/300.. Learning rate: 0.0165.. Train loss: 0.0025.. Train acc: 0.9993.. Val loss: 3.3085.. Val acc: 0.5714\n",
            "Epoch 184/300.. Learning rate: 0.0163.. Train loss: 0.0022.. Train acc: 0.9995.. Val loss: 3.3392.. Val acc: 0.5683\n",
            "Epoch 185/300.. Learning rate: 0.0160.. Train loss: 0.0020.. Train acc: 0.9995.. Val loss: 3.3076.. Val acc: 0.5649\n",
            "Epoch 186/300.. Learning rate: 0.0158.. Train loss: 0.0020.. Train acc: 0.9995.. Val loss: 3.3281.. Val acc: 0.5624\n",
            "Epoch 187/300.. Learning rate: 0.0156.. Train loss: 0.0022.. Train acc: 0.9994.. Val loss: 3.3372.. Val acc: 0.5633\n",
            "Epoch 188/300.. Learning rate: 0.0153.. Train loss: 0.0022.. Train acc: 0.9994.. Val loss: 3.3156.. Val acc: 0.5643\n",
            "Epoch 189/300.. Learning rate: 0.0151.. Train loss: 0.0020.. Train acc: 0.9995.. Val loss: 3.3219.. Val acc: 0.5635\n",
            "Epoch 190/300.. Learning rate: 0.0148.. Train loss: 0.0023.. Train acc: 0.9994.. Val loss: 3.3162.. Val acc: 0.5697\n",
            "Epoch 191/300.. Learning rate: 0.0146.. Train loss: 0.0023.. Train acc: 0.9994.. Val loss: 3.2832.. Val acc: 0.5711\n",
            "Epoch 192/300.. Learning rate: 0.0144.. Train loss: 0.0019.. Train acc: 0.9997.. Val loss: 3.3134.. Val acc: 0.5654\n",
            "Epoch 193/300.. Learning rate: 0.0141.. Train loss: 0.0017.. Train acc: 0.9995.. Val loss: 3.3151.. Val acc: 0.5653\n",
            "Epoch 194/300.. Learning rate: 0.0139.. Train loss: 0.0019.. Train acc: 0.9996.. Val loss: 3.3208.. Val acc: 0.5687\n",
            "Epoch 195/300.. Learning rate: 0.0137.. Train loss: 0.0018.. Train acc: 0.9994.. Val loss: 3.3236.. Val acc: 0.5624\n",
            "Epoch 196/300.. Learning rate: 0.0134.. Train loss: 0.0021.. Train acc: 0.9994.. Val loss: 3.2938.. Val acc: 0.5673\n",
            "Epoch 197/300.. Learning rate: 0.0132.. Train loss: 0.0017.. Train acc: 0.9996.. Val loss: 3.3024.. Val acc: 0.5649\n",
            "Epoch 198/300.. Learning rate: 0.0130.. Train loss: 0.0020.. Train acc: 0.9994.. Val loss: 3.3292.. Val acc: 0.5651\n",
            "Epoch 199/300.. Learning rate: 0.0127.. Train loss: 0.0021.. Train acc: 0.9993.. Val loss: 3.3353.. Val acc: 0.5658\n",
            "Epoch 200/300.. Learning rate: 0.0125.. Train loss: 0.0019.. Train acc: 0.9995.. Val loss: 3.3177.. Val acc: 0.5622\n",
            "Epoch 201/300.. Learning rate: 0.0123.. Train loss: 0.0019.. Train acc: 0.9994.. Val loss: 3.3107.. Val acc: 0.5662\n",
            "Epoch 202/300.. Learning rate: 0.0121.. Train loss: 0.0017.. Train acc: 0.9996.. Val loss: 3.3179.. Val acc: 0.5682\n",
            "Epoch 203/300.. Learning rate: 0.0118.. Train loss: 0.0015.. Train acc: 0.9996.. Val loss: 3.3279.. Val acc: 0.5650\n",
            "Epoch 204/300.. Learning rate: 0.0116.. Train loss: 0.0017.. Train acc: 0.9996.. Val loss: 3.3826.. Val acc: 0.5620\n",
            "Epoch 205/300.. Learning rate: 0.0114.. Train loss: 0.0016.. Train acc: 0.9996.. Val loss: 3.3414.. Val acc: 0.5645\n",
            "Epoch 206/300.. Learning rate: 0.0112.. Train loss: 0.0016.. Train acc: 0.9996.. Val loss: 3.3370.. Val acc: 0.5664\n",
            "Epoch 207/300.. Learning rate: 0.0110.. Train loss: 0.0016.. Train acc: 0.9995.. Val loss: 3.3576.. Val acc: 0.5656\n",
            "Epoch 208/300.. Learning rate: 0.0107.. Train loss: 0.0015.. Train acc: 0.9997.. Val loss: 3.3297.. Val acc: 0.5681\n",
            "Epoch 209/300.. Learning rate: 0.0105.. Train loss: 0.0020.. Train acc: 0.9993.. Val loss: 3.3457.. Val acc: 0.5665\n",
            "Epoch 210/300.. Learning rate: 0.0103.. Train loss: 0.0017.. Train acc: 0.9995.. Val loss: 3.3154.. Val acc: 0.5698\n",
            "Epoch 211/300.. Learning rate: 0.0101.. Train loss: 0.0017.. Train acc: 0.9995.. Val loss: 3.3185.. Val acc: 0.5664\n",
            "Epoch 212/300.. Learning rate: 0.0099.. Train loss: 0.0015.. Train acc: 0.9996.. Val loss: 3.3398.. Val acc: 0.5694\n",
            "Epoch 213/300.. Learning rate: 0.0097.. Train loss: 0.0016.. Train acc: 0.9996.. Val loss: 3.3099.. Val acc: 0.5688\n",
            "Epoch 214/300.. Learning rate: 0.0095.. Train loss: 0.0017.. Train acc: 0.9994.. Val loss: 3.3137.. Val acc: 0.5656\n",
            "Epoch 215/300.. Learning rate: 0.0093.. Train loss: 0.0015.. Train acc: 0.9996.. Val loss: 3.3102.. Val acc: 0.5684\n",
            "Epoch 216/300.. Learning rate: 0.0091.. Train loss: 0.0017.. Train acc: 0.9996.. Val loss: 3.3231.. Val acc: 0.5653\n",
            "Epoch 217/300.. Learning rate: 0.0089.. Train loss: 0.0016.. Train acc: 0.9996.. Val loss: 3.3130.. Val acc: 0.5680\n",
            "Epoch 218/300.. Learning rate: 0.0087.. Train loss: 0.0013.. Train acc: 0.9997.. Val loss: 3.2909.. Val acc: 0.5672\n",
            "Epoch 219/300.. Learning rate: 0.0085.. Train loss: 0.0013.. Train acc: 0.9998.. Val loss: 3.3169.. Val acc: 0.5671\n",
            "Epoch 220/300.. Learning rate: 0.0083.. Train loss: 0.0013.. Train acc: 0.9998.. Val loss: 3.3336.. Val acc: 0.5701\n",
            "Epoch 221/300.. Learning rate: 0.0081.. Train loss: 0.0017.. Train acc: 0.9995.. Val loss: 3.3087.. Val acc: 0.5677\n",
            "Epoch 222/300.. Learning rate: 0.0079.. Train loss: 0.0016.. Train acc: 0.9997.. Val loss: 3.2968.. Val acc: 0.5690\n",
            "Epoch 223/300.. Learning rate: 0.0077.. Train loss: 0.0014.. Train acc: 0.9996.. Val loss: 3.3136.. Val acc: 0.5703\n",
            "Epoch 224/300.. Learning rate: 0.0075.. Train loss: 0.0015.. Train acc: 0.9996.. Val loss: 3.3319.. Val acc: 0.5711\n",
            "Epoch 225/300.. Learning rate: 0.0073.. Train loss: 0.0013.. Train acc: 0.9997.. Val loss: 3.3357.. Val acc: 0.5704\n",
            "Epoch 226/300.. Learning rate: 0.0071.. Train loss: 0.0015.. Train acc: 0.9995.. Val loss: 3.3162.. Val acc: 0.5692\n",
            "Epoch 227/300.. Learning rate: 0.0070.. Train loss: 0.0014.. Train acc: 0.9997.. Val loss: 3.3613.. Val acc: 0.5632\n",
            "Epoch 228/300.. Learning rate: 0.0068.. Train loss: 0.0013.. Train acc: 0.9996.. Val loss: 3.3060.. Val acc: 0.5655\n",
            "Epoch 229/300.. Learning rate: 0.0066.. Train loss: 0.0015.. Train acc: 0.9996.. Val loss: 3.3246.. Val acc: 0.5647\n",
            "Epoch 230/300.. Learning rate: 0.0064.. Train loss: 0.0013.. Train acc: 0.9997.. Val loss: 3.3383.. Val acc: 0.5666\n",
            "Epoch 231/300.. Learning rate: 0.0062.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.3206.. Val acc: 0.5703\n",
            "Epoch 232/300.. Learning rate: 0.0061.. Train loss: 0.0014.. Train acc: 0.9996.. Val loss: 3.3522.. Val acc: 0.5625\n",
            "Epoch 233/300.. Learning rate: 0.0059.. Train loss: 0.0012.. Train acc: 0.9998.. Val loss: 3.3174.. Val acc: 0.5695\n",
            "Epoch 234/300.. Learning rate: 0.0057.. Train loss: 0.0011.. Train acc: 0.9997.. Val loss: 3.3399.. Val acc: 0.5656\n",
            "Epoch 235/300.. Learning rate: 0.0056.. Train loss: 0.0015.. Train acc: 0.9997.. Val loss: 3.3032.. Val acc: 0.5658\n",
            "Epoch 236/300.. Learning rate: 0.0054.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.3320.. Val acc: 0.5716\n",
            "Epoch 237/300.. Learning rate: 0.0052.. Train loss: 0.0013.. Train acc: 0.9997.. Val loss: 3.3116.. Val acc: 0.5673\n",
            "Epoch 238/300.. Learning rate: 0.0051.. Train loss: 0.0013.. Train acc: 0.9996.. Val loss: 3.3425.. Val acc: 0.5679\n",
            "Epoch 239/300.. Learning rate: 0.0049.. Train loss: 0.0014.. Train acc: 0.9997.. Val loss: 3.3245.. Val acc: 0.5684\n",
            "Epoch 240/300.. Learning rate: 0.0048.. Train loss: 0.0014.. Train acc: 0.9996.. Val loss: 3.3190.. Val acc: 0.5693\n",
            "Epoch 241/300.. Learning rate: 0.0046.. Train loss: 0.0014.. Train acc: 0.9997.. Val loss: 3.2980.. Val acc: 0.5702\n",
            "Epoch 242/300.. Learning rate: 0.0045.. Train loss: 0.0011.. Train acc: 0.9996.. Val loss: 3.3205.. Val acc: 0.5659\n",
            "Epoch 243/300.. Learning rate: 0.0043.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.3783.. Val acc: 0.5658\n",
            "Epoch 244/300.. Learning rate: 0.0042.. Train loss: 0.0015.. Train acc: 0.9996.. Val loss: 3.3210.. Val acc: 0.5702\n",
            "Epoch 245/300.. Learning rate: 0.0040.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.3273.. Val acc: 0.5691\n",
            "Epoch 246/300.. Learning rate: 0.0039.. Train loss: 0.0013.. Train acc: 0.9996.. Val loss: 3.3137.. Val acc: 0.5659\n",
            "Epoch 247/300.. Learning rate: 0.0038.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.3234.. Val acc: 0.5695\n",
            "Epoch 248/300.. Learning rate: 0.0036.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.3438.. Val acc: 0.5672\n",
            "Epoch 249/300.. Learning rate: 0.0035.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.3448.. Val acc: 0.5662\n",
            "Epoch 250/300.. Learning rate: 0.0034.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.3151.. Val acc: 0.5666\n",
            "Epoch 251/300.. Learning rate: 0.0032.. Train loss: 0.0011.. Train acc: 0.9997.. Val loss: 3.3439.. Val acc: 0.5674\n",
            "Epoch 252/300.. Learning rate: 0.0031.. Train loss: 0.0011.. Train acc: 0.9998.. Val loss: 3.3060.. Val acc: 0.5725\n",
            "Epoch 253/300.. Learning rate: 0.0030.. Train loss: 0.0014.. Train acc: 0.9996.. Val loss: 3.3215.. Val acc: 0.5680\n",
            "Epoch 254/300.. Learning rate: 0.0028.. Train loss: 0.0010.. Train acc: 0.9998.. Val loss: 3.3497.. Val acc: 0.5670\n",
            "Epoch 255/300.. Learning rate: 0.0027.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.3589.. Val acc: 0.5676\n",
            "Epoch 256/300.. Learning rate: 0.0026.. Train loss: 0.0013.. Train acc: 0.9996.. Val loss: 3.3061.. Val acc: 0.5644\n",
            "Epoch 257/300.. Learning rate: 0.0025.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.3155.. Val acc: 0.5648\n",
            "Epoch 258/300.. Learning rate: 0.0024.. Train loss: 0.0013.. Train acc: 0.9996.. Val loss: 3.3094.. Val acc: 0.5677\n",
            "Epoch 259/300.. Learning rate: 0.0023.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.3298.. Val acc: 0.5686\n",
            "Epoch 260/300.. Learning rate: 0.0022.. Train loss: 0.0015.. Train acc: 0.9995.. Val loss: 3.3154.. Val acc: 0.5695\n",
            "Epoch 261/300.. Learning rate: 0.0021.. Train loss: 0.0013.. Train acc: 0.9996.. Val loss: 3.3254.. Val acc: 0.5685\n",
            "Epoch 262/300.. Learning rate: 0.0020.. Train loss: 0.0013.. Train acc: 0.9997.. Val loss: 3.3465.. Val acc: 0.5682\n",
            "Epoch 263/300.. Learning rate: 0.0019.. Train loss: 0.0012.. Train acc: 0.9998.. Val loss: 3.3354.. Val acc: 0.5662\n",
            "Epoch 264/300.. Learning rate: 0.0018.. Train loss: 0.0010.. Train acc: 0.9998.. Val loss: 3.3320.. Val acc: 0.5653\n",
            "Epoch 265/300.. Learning rate: 0.0017.. Train loss: 0.0013.. Train acc: 0.9996.. Val loss: 3.3367.. Val acc: 0.5734\n",
            "Epoch 266/300.. Learning rate: 0.0016.. Train loss: 0.0012.. Train acc: 0.9996.. Val loss: 3.3044.. Val acc: 0.5690\n",
            "Epoch 267/300.. Learning rate: 0.0015.. Train loss: 0.0012.. Train acc: 0.9998.. Val loss: 3.3222.. Val acc: 0.5685\n",
            "Epoch 268/300.. Learning rate: 0.0014.. Train loss: 0.0011.. Train acc: 0.9998.. Val loss: 3.3548.. Val acc: 0.5686\n",
            "Epoch 269/300.. Learning rate: 0.0013.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.3401.. Val acc: 0.5695\n",
            "Epoch 270/300.. Learning rate: 0.0012.. Train loss: 0.0011.. Train acc: 0.9998.. Val loss: 3.2791.. Val acc: 0.5691\n",
            "Epoch 271/300.. Learning rate: 0.0011.. Train loss: 0.0011.. Train acc: 0.9998.. Val loss: 3.2998.. Val acc: 0.5692\n",
            "Epoch 272/300.. Learning rate: 0.0011.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.3512.. Val acc: 0.5670\n",
            "Epoch 273/300.. Learning rate: 0.0010.. Train loss: 0.0011.. Train acc: 0.9997.. Val loss: 3.3123.. Val acc: 0.5673\n",
            "Epoch 274/300.. Learning rate: 0.0009.. Train loss: 0.0012.. Train acc: 0.9998.. Val loss: 3.3459.. Val acc: 0.5662\n",
            "Epoch 275/300.. Learning rate: 0.0009.. Train loss: 0.0011.. Train acc: 0.9998.. Val loss: 3.3239.. Val acc: 0.5658\n",
            "Epoch 276/300.. Learning rate: 0.0008.. Train loss: 0.0013.. Train acc: 0.9996.. Val loss: 3.3040.. Val acc: 0.5720\n",
            "Epoch 277/300.. Learning rate: 0.0007.. Train loss: 0.0011.. Train acc: 0.9997.. Val loss: 3.3251.. Val acc: 0.5680\n",
            "Epoch 278/300.. Learning rate: 0.0007.. Train loss: 0.0011.. Train acc: 0.9997.. Val loss: 3.3175.. Val acc: 0.5699\n",
            "Epoch 279/300.. Learning rate: 0.0006.. Train loss: 0.0010.. Train acc: 0.9998.. Val loss: 3.3089.. Val acc: 0.5699\n",
            "Epoch 280/300.. Learning rate: 0.0005.. Train loss: 0.0011.. Train acc: 0.9997.. Val loss: 3.2987.. Val acc: 0.5700\n",
            "Epoch 281/300.. Learning rate: 0.0005.. Train loss: 0.0012.. Train acc: 0.9998.. Val loss: 3.3305.. Val acc: 0.5678\n",
            "Epoch 282/300.. Learning rate: 0.0004.. Train loss: 0.0013.. Train acc: 0.9997.. Val loss: 3.3452.. Val acc: 0.5668\n",
            "Epoch 283/300.. Learning rate: 0.0004.. Train loss: 0.0011.. Train acc: 0.9998.. Val loss: 3.3043.. Val acc: 0.5721\n",
            "Epoch 284/300.. Learning rate: 0.0004.. Train loss: 0.0012.. Train acc: 0.9998.. Val loss: 3.3630.. Val acc: 0.5694\n",
            "Epoch 285/300.. Learning rate: 0.0003.. Train loss: 0.0010.. Train acc: 0.9997.. Val loss: 3.3222.. Val acc: 0.5680\n",
            "Epoch 286/300.. Learning rate: 0.0003.. Train loss: 0.0011.. Train acc: 0.9997.. Val loss: 3.3585.. Val acc: 0.5687\n",
            "Epoch 287/300.. Learning rate: 0.0002.. Train loss: 0.0011.. Train acc: 0.9998.. Val loss: 3.3007.. Val acc: 0.5728\n",
            "Epoch 288/300.. Learning rate: 0.0002.. Train loss: 0.0013.. Train acc: 0.9997.. Val loss: 3.3359.. Val acc: 0.5653\n",
            "Epoch 289/300.. Learning rate: 0.0002.. Train loss: 0.0012.. Train acc: 0.9998.. Val loss: 3.3214.. Val acc: 0.5661\n",
            "Epoch 290/300.. Learning rate: 0.0001.. Train loss: 0.0010.. Train acc: 0.9997.. Val loss: 3.3179.. Val acc: 0.5696\n",
            "Epoch 291/300.. Learning rate: 0.0001.. Train loss: 0.0012.. Train acc: 0.9998.. Val loss: 3.3322.. Val acc: 0.5708\n",
            "Epoch 292/300.. Learning rate: 0.0001.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.3507.. Val acc: 0.5686\n",
            "Epoch 293/300.. Learning rate: 0.0001.. Train loss: 0.0012.. Train acc: 0.9999.. Val loss: 3.3399.. Val acc: 0.5678\n",
            "Epoch 294/300.. Learning rate: 0.0001.. Train loss: 0.0013.. Train acc: 0.9996.. Val loss: 3.3202.. Val acc: 0.5662\n",
            "Epoch 295/300.. Learning rate: 0.0000.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.3010.. Val acc: 0.5746\n",
            "Epoch 296/300.. Learning rate: 0.0000.. Train loss: 0.0012.. Train acc: 0.9998.. Val loss: 3.3430.. Val acc: 0.5668\n",
            "Epoch 297/300.. Learning rate: 0.0000.. Train loss: 0.0012.. Train acc: 0.9997.. Val loss: 3.3140.. Val acc: 0.5727\n",
            "Epoch 298/300.. Learning rate: 0.0000.. Train loss: 0.0010.. Train acc: 0.9998.. Val loss: 3.3209.. Val acc: 0.5695\n",
            "Epoch 299/300.. Learning rate: 0.0000.. Train loss: 0.0011.. Train acc: 0.9997.. Val loss: 3.3323.. Val acc: 0.5697\n",
            "Epoch 300/300.. Learning rate: 0.0000.. Train loss: 0.0011.. Train acc: 0.9998.. Val loss: 3.3232.. Val acc: 0.5638\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a741f78b-403c-4a6b-976d-510c94b0861c\", \"Accuracy and Loss with Cosine Annealing learning rate schedule.png\", 43829)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss:  tensor(0.0047, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Test acc:  0.5686\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "MobileNet(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layers): Sequential(\n",
            "    (0): Block(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (6): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (7): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (8): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (9): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (10): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (11): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (12): Block(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=1024, out_features=100, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:486: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300.. Learning rate: 0.0500.. Train loss: 4.1953.. Train acc: 0.0614.. Val loss: 3.9311.. Val acc: 0.0898\n",
            "Epoch 2/300.. Learning rate: 0.0500.. Train loss: 3.6726.. Train acc: 0.1258.. Val loss: 3.5276.. Val acc: 0.1504\n",
            "Epoch 3/300.. Learning rate: 0.0500.. Train loss: 3.3727.. Train acc: 0.1783.. Val loss: 3.3220.. Val acc: 0.1871\n",
            "Epoch 4/300.. Learning rate: 0.0500.. Train loss: 3.1293.. Train acc: 0.2248.. Val loss: 3.0187.. Val acc: 0.2423\n",
            "Epoch 5/300.. Learning rate: 0.0500.. Train loss: 2.9283.. Train acc: 0.2584.. Val loss: 2.8833.. Val acc: 0.2757\n",
            "Epoch 6/300.. Learning rate: 0.0500.. Train loss: 2.7428.. Train acc: 0.2946.. Val loss: 2.7538.. Val acc: 0.3036\n",
            "Epoch 7/300.. Learning rate: 0.0500.. Train loss: 2.5710.. Train acc: 0.3286.. Val loss: 2.6697.. Val acc: 0.3108\n",
            "Epoch 8/300.. Learning rate: 0.0500.. Train loss: 2.4344.. Train acc: 0.3560.. Val loss: 2.6083.. Val acc: 0.3322\n",
            "Epoch 9/300.. Learning rate: 0.0500.. Train loss: 2.2868.. Train acc: 0.3874.. Val loss: 2.4311.. Val acc: 0.3626\n",
            "Epoch 10/300.. Learning rate: 0.0500.. Train loss: 2.1759.. Train acc: 0.4105.. Val loss: 2.3684.. Val acc: 0.3838\n",
            "Epoch 11/300.. Learning rate: 0.0500.. Train loss: 2.0758.. Train acc: 0.4347.. Val loss: 2.4083.. Val acc: 0.3800\n",
            "Epoch 12/300.. Learning rate: 0.0500.. Train loss: 1.9837.. Train acc: 0.4528.. Val loss: 2.2711.. Val acc: 0.4022\n",
            "Epoch 13/300.. Learning rate: 0.0500.. Train loss: 1.8926.. Train acc: 0.4752.. Val loss: 2.1973.. Val acc: 0.4259\n",
            "Epoch 14/300.. Learning rate: 0.0500.. Train loss: 1.8208.. Train acc: 0.4923.. Val loss: 2.1446.. Val acc: 0.4374\n",
            "Epoch 15/300.. Learning rate: 0.0500.. Train loss: 1.7398.. Train acc: 0.5097.. Val loss: 2.1113.. Val acc: 0.4465\n",
            "Epoch 16/300.. Learning rate: 0.0500.. Train loss: 1.6745.. Train acc: 0.5233.. Val loss: 2.0958.. Val acc: 0.4448\n",
            "Epoch 17/300.. Learning rate: 0.0500.. Train loss: 1.6036.. Train acc: 0.5432.. Val loss: 2.0515.. Val acc: 0.4637\n",
            "Epoch 18/300.. Learning rate: 0.0500.. Train loss: 1.5347.. Train acc: 0.5574.. Val loss: 2.0506.. Val acc: 0.4655\n",
            "Epoch 19/300.. Learning rate: 0.0500.. Train loss: 1.4754.. Train acc: 0.5750.. Val loss: 2.1003.. Val acc: 0.4636\n",
            "Epoch 20/300.. Learning rate: 0.0500.. Train loss: 1.4154.. Train acc: 0.5886.. Val loss: 2.0421.. Val acc: 0.4811\n",
            "Epoch 21/300.. Learning rate: 0.0500.. Train loss: 1.3598.. Train acc: 0.6037.. Val loss: 2.0405.. Val acc: 0.4791\n",
            "Epoch 22/300.. Learning rate: 0.0500.. Train loss: 1.3037.. Train acc: 0.6149.. Val loss: 1.9587.. Val acc: 0.4945\n",
            "Epoch 23/300.. Learning rate: 0.0500.. Train loss: 1.2527.. Train acc: 0.6294.. Val loss: 1.9593.. Val acc: 0.4974\n",
            "Epoch 24/300.. Learning rate: 0.0500.. Train loss: 1.2059.. Train acc: 0.6388.. Val loss: 1.9820.. Val acc: 0.4904\n",
            "Epoch 25/300.. Learning rate: 0.0500.. Train loss: 1.1543.. Train acc: 0.6531.. Val loss: 2.0244.. Val acc: 0.4944\n",
            "Epoch 26/300.. Learning rate: 0.0500.. Train loss: 1.0922.. Train acc: 0.6672.. Val loss: 2.0068.. Val acc: 0.5011\n",
            "Epoch 27/300.. Learning rate: 0.0500.. Train loss: 1.0486.. Train acc: 0.6804.. Val loss: 2.0572.. Val acc: 0.4974\n",
            "Epoch 28/300.. Learning rate: 0.0500.. Train loss: 1.0072.. Train acc: 0.6880.. Val loss: 2.0690.. Val acc: 0.4977\n",
            "Epoch 29/300.. Learning rate: 0.0500.. Train loss: 0.9578.. Train acc: 0.7050.. Val loss: 2.0849.. Val acc: 0.4973\n",
            "Epoch 30/300.. Learning rate: 0.0500.. Train loss: 0.9145.. Train acc: 0.7181.. Val loss: 2.0823.. Val acc: 0.5029\n",
            "Epoch 31/300.. Learning rate: 0.0500.. Train loss: 0.8573.. Train acc: 0.7337.. Val loss: 2.0803.. Val acc: 0.5105\n",
            "Epoch 32/300.. Learning rate: 0.0500.. Train loss: 0.8234.. Train acc: 0.7422.. Val loss: 2.1586.. Val acc: 0.5010\n",
            "Epoch 33/300.. Learning rate: 0.0500.. Train loss: 0.7780.. Train acc: 0.7528.. Val loss: 2.1560.. Val acc: 0.5091\n",
            "Epoch 34/300.. Learning rate: 0.0500.. Train loss: 0.7292.. Train acc: 0.7689.. Val loss: 2.2210.. Val acc: 0.4964\n",
            "Epoch 35/300.. Learning rate: 0.0500.. Train loss: 0.6765.. Train acc: 0.7880.. Val loss: 2.2237.. Val acc: 0.5036\n",
            "Epoch 36/300.. Learning rate: 0.0500.. Train loss: 0.6425.. Train acc: 0.7949.. Val loss: 2.2082.. Val acc: 0.5116\n",
            "Epoch 37/300.. Learning rate: 0.0500.. Train loss: 0.5967.. Train acc: 0.8083.. Val loss: 2.2189.. Val acc: 0.5169\n",
            "Epoch 38/300.. Learning rate: 0.0500.. Train loss: 0.5783.. Train acc: 0.8121.. Val loss: 2.2603.. Val acc: 0.5152\n",
            "Epoch 39/300.. Learning rate: 0.0500.. Train loss: 0.5402.. Train acc: 0.8246.. Val loss: 2.2835.. Val acc: 0.5186\n",
            "Epoch 40/300.. Learning rate: 0.0500.. Train loss: 0.4853.. Train acc: 0.8428.. Val loss: 2.3713.. Val acc: 0.5142\n",
            "Epoch 41/300.. Learning rate: 0.0500.. Train loss: 0.4806.. Train acc: 0.8434.. Val loss: 2.4198.. Val acc: 0.5037\n",
            "Epoch 42/300.. Learning rate: 0.0500.. Train loss: 0.4353.. Train acc: 0.8577.. Val loss: 2.3859.. Val acc: 0.5202\n",
            "Epoch 43/300.. Learning rate: 0.0500.. Train loss: 0.4123.. Train acc: 0.8643.. Val loss: 2.4588.. Val acc: 0.5091\n",
            "Epoch 44/300.. Learning rate: 0.0500.. Train loss: 0.3909.. Train acc: 0.8723.. Val loss: 2.5305.. Val acc: 0.5050\n",
            "Epoch 45/300.. Learning rate: 0.0500.. Train loss: 0.3760.. Train acc: 0.8759.. Val loss: 2.5293.. Val acc: 0.5166\n",
            "Epoch 46/300.. Learning rate: 0.0500.. Train loss: 0.3517.. Train acc: 0.8855.. Val loss: 2.5193.. Val acc: 0.5139\n",
            "Epoch 47/300.. Learning rate: 0.0500.. Train loss: 0.3373.. Train acc: 0.8891.. Val loss: 2.5090.. Val acc: 0.5212\n",
            "Epoch 48/300.. Learning rate: 0.0500.. Train loss: 0.3124.. Train acc: 0.8997.. Val loss: 2.5555.. Val acc: 0.5187\n",
            "Epoch 49/300.. Learning rate: 0.0500.. Train loss: 0.2950.. Train acc: 0.9032.. Val loss: 2.5335.. Val acc: 0.5272\n",
            "Epoch 50/300.. Learning rate: 0.0500.. Train loss: 0.2708.. Train acc: 0.9118.. Val loss: 2.5718.. Val acc: 0.5242\n",
            "Epoch 51/300.. Learning rate: 0.0500.. Train loss: 0.2560.. Train acc: 0.9159.. Val loss: 2.6037.. Val acc: 0.5278\n",
            "Epoch 52/300.. Learning rate: 0.0500.. Train loss: 0.2500.. Train acc: 0.9179.. Val loss: 2.6221.. Val acc: 0.5211\n",
            "Epoch 53/300.. Learning rate: 0.0500.. Train loss: 0.2490.. Train acc: 0.9182.. Val loss: 2.7205.. Val acc: 0.5206\n",
            "Epoch 54/300.. Learning rate: 0.0500.. Train loss: 0.2304.. Train acc: 0.9252.. Val loss: 2.6393.. Val acc: 0.5299\n",
            "Epoch 55/300.. Learning rate: 0.0500.. Train loss: 0.2131.. Train acc: 0.9300.. Val loss: 2.7096.. Val acc: 0.5258\n",
            "Epoch 56/300.. Learning rate: 0.0500.. Train loss: 0.2056.. Train acc: 0.9330.. Val loss: 2.7356.. Val acc: 0.5172\n",
            "Epoch 57/300.. Learning rate: 0.0500.. Train loss: 0.1825.. Train acc: 0.9407.. Val loss: 2.7226.. Val acc: 0.5245\n",
            "Epoch 58/300.. Learning rate: 0.0500.. Train loss: 0.1748.. Train acc: 0.9434.. Val loss: 2.7524.. Val acc: 0.5304\n",
            "Epoch 59/300.. Learning rate: 0.0500.. Train loss: 0.1686.. Train acc: 0.9444.. Val loss: 2.7891.. Val acc: 0.5265\n",
            "Epoch 60/300.. Learning rate: 0.0500.. Train loss: 0.1719.. Train acc: 0.9449.. Val loss: 2.7787.. Val acc: 0.5261\n",
            "Epoch 61/300.. Learning rate: 0.0500.. Train loss: 0.1522.. Train acc: 0.9516.. Val loss: 2.7860.. Val acc: 0.5311\n",
            "Epoch 62/300.. Learning rate: 0.0500.. Train loss: 0.1456.. Train acc: 0.9533.. Val loss: 2.7775.. Val acc: 0.5363\n",
            "Epoch 63/300.. Learning rate: 0.0500.. Train loss: 0.1403.. Train acc: 0.9553.. Val loss: 2.8053.. Val acc: 0.5325\n",
            "Epoch 64/300.. Learning rate: 0.0500.. Train loss: 0.1317.. Train acc: 0.9566.. Val loss: 2.8358.. Val acc: 0.5328\n",
            "Epoch 65/300.. Learning rate: 0.0500.. Train loss: 0.1258.. Train acc: 0.9597.. Val loss: 2.8894.. Val acc: 0.5339\n",
            "Epoch 66/300.. Learning rate: 0.0500.. Train loss: 0.1300.. Train acc: 0.9580.. Val loss: 2.9063.. Val acc: 0.5345\n",
            "Epoch 67/300.. Learning rate: 0.0500.. Train loss: 0.1186.. Train acc: 0.9614.. Val loss: 2.8936.. Val acc: 0.5359\n",
            "Epoch 68/300.. Learning rate: 0.0500.. Train loss: 0.1242.. Train acc: 0.9604.. Val loss: 2.8830.. Val acc: 0.5339\n",
            "Epoch 69/300.. Learning rate: 0.0500.. Train loss: 0.1225.. Train acc: 0.9599.. Val loss: 2.9055.. Val acc: 0.5387\n",
            "Epoch 70/300.. Learning rate: 0.0500.. Train loss: 0.1044.. Train acc: 0.9666.. Val loss: 2.9076.. Val acc: 0.5342\n",
            "Epoch 71/300.. Learning rate: 0.0500.. Train loss: 0.1045.. Train acc: 0.9660.. Val loss: 2.9683.. Val acc: 0.5316\n",
            "Epoch 72/300.. Learning rate: 0.0500.. Train loss: 0.0989.. Train acc: 0.9675.. Val loss: 2.9014.. Val acc: 0.5344\n",
            "Epoch 73/300.. Learning rate: 0.0500.. Train loss: 0.0967.. Train acc: 0.9691.. Val loss: 2.9228.. Val acc: 0.5390\n",
            "Epoch 74/300.. Learning rate: 0.0500.. Train loss: 0.0920.. Train acc: 0.9713.. Val loss: 2.9461.. Val acc: 0.5356\n",
            "Epoch 75/300.. Learning rate: 0.0500.. Train loss: 0.0897.. Train acc: 0.9715.. Val loss: 2.9763.. Val acc: 0.5372\n",
            "Epoch 76/300.. Learning rate: 0.0500.. Train loss: 0.0848.. Train acc: 0.9722.. Val loss: 2.9676.. Val acc: 0.5382\n",
            "Epoch 77/300.. Learning rate: 0.0500.. Train loss: 0.0851.. Train acc: 0.9717.. Val loss: 3.0427.. Val acc: 0.5297\n",
            "Epoch 78/300.. Learning rate: 0.0500.. Train loss: 0.0811.. Train acc: 0.9739.. Val loss: 3.0163.. Val acc: 0.5348\n",
            "Epoch 79/300.. Learning rate: 0.0500.. Train loss: 0.0805.. Train acc: 0.9736.. Val loss: 3.0169.. Val acc: 0.5362\n",
            "Epoch 80/300.. Learning rate: 0.0500.. Train loss: 0.0805.. Train acc: 0.9740.. Val loss: 3.0511.. Val acc: 0.5394\n",
            "Epoch 81/300.. Learning rate: 0.0500.. Train loss: 0.0762.. Train acc: 0.9754.. Val loss: 3.0598.. Val acc: 0.5334\n",
            "Epoch 82/300.. Learning rate: 0.0500.. Train loss: 0.0692.. Train acc: 0.9788.. Val loss: 3.0593.. Val acc: 0.5336\n",
            "Epoch 83/300.. Learning rate: 0.0500.. Train loss: 0.0717.. Train acc: 0.9766.. Val loss: 3.1342.. Val acc: 0.5376\n",
            "Epoch 84/300.. Learning rate: 0.0500.. Train loss: 0.0666.. Train acc: 0.9791.. Val loss: 3.0983.. Val acc: 0.5358\n",
            "Epoch 85/300.. Learning rate: 0.0500.. Train loss: 0.0691.. Train acc: 0.9777.. Val loss: 3.1312.. Val acc: 0.5419\n",
            "Epoch 86/300.. Learning rate: 0.0500.. Train loss: 0.0668.. Train acc: 0.9785.. Val loss: 3.1305.. Val acc: 0.5406\n",
            "Epoch 87/300.. Learning rate: 0.0500.. Train loss: 0.0637.. Train acc: 0.9797.. Val loss: 3.0849.. Val acc: 0.5402\n",
            "Epoch 88/300.. Learning rate: 0.0500.. Train loss: 0.0590.. Train acc: 0.9816.. Val loss: 3.0976.. Val acc: 0.5439\n",
            "Epoch 89/300.. Learning rate: 0.0500.. Train loss: 0.0578.. Train acc: 0.9814.. Val loss: 3.1142.. Val acc: 0.5349\n",
            "Epoch 90/300.. Learning rate: 0.0500.. Train loss: 0.0558.. Train acc: 0.9827.. Val loss: 3.1544.. Val acc: 0.5382\n",
            "Epoch 91/300.. Learning rate: 0.0500.. Train loss: 0.0597.. Train acc: 0.9814.. Val loss: 3.1022.. Val acc: 0.5431\n",
            "Epoch 92/300.. Learning rate: 0.0500.. Train loss: 0.0556.. Train acc: 0.9832.. Val loss: 3.1375.. Val acc: 0.5404\n",
            "Epoch 93/300.. Learning rate: 0.0500.. Train loss: 0.0532.. Train acc: 0.9840.. Val loss: 3.1264.. Val acc: 0.5491\n",
            "Epoch 94/300.. Learning rate: 0.0500.. Train loss: 0.0516.. Train acc: 0.9837.. Val loss: 3.2150.. Val acc: 0.5413\n",
            "Epoch 95/300.. Learning rate: 0.0500.. Train loss: 0.0498.. Train acc: 0.9842.. Val loss: 3.1058.. Val acc: 0.5444\n",
            "Epoch 96/300.. Learning rate: 0.0500.. Train loss: 0.0452.. Train acc: 0.9856.. Val loss: 3.2019.. Val acc: 0.5376\n",
            "Epoch 97/300.. Learning rate: 0.0500.. Train loss: 0.0516.. Train acc: 0.9840.. Val loss: 3.2588.. Val acc: 0.5343\n",
            "Epoch 98/300.. Learning rate: 0.0500.. Train loss: 0.0505.. Train acc: 0.9835.. Val loss: 3.2262.. Val acc: 0.5434\n",
            "Epoch 99/300.. Learning rate: 0.0500.. Train loss: 0.0417.. Train acc: 0.9869.. Val loss: 3.1867.. Val acc: 0.5432\n",
            "Epoch 100/300.. Learning rate: 0.0500.. Train loss: 0.0457.. Train acc: 0.9852.. Val loss: 3.2381.. Val acc: 0.5367\n",
            "Epoch 101/300.. Learning rate: 0.0500.. Train loss: 0.0381.. Train acc: 0.9886.. Val loss: 3.2266.. Val acc: 0.5439\n",
            "Epoch 102/300.. Learning rate: 0.0500.. Train loss: 0.0448.. Train acc: 0.9858.. Val loss: 3.2493.. Val acc: 0.5395\n",
            "Epoch 103/300.. Learning rate: 0.0500.. Train loss: 0.0442.. Train acc: 0.9857.. Val loss: 3.2967.. Val acc: 0.5405\n",
            "Epoch 104/300.. Learning rate: 0.0500.. Train loss: 0.0439.. Train acc: 0.9857.. Val loss: 3.2457.. Val acc: 0.5430\n",
            "Epoch 105/300.. Learning rate: 0.0500.. Train loss: 0.0449.. Train acc: 0.9859.. Val loss: 3.2491.. Val acc: 0.5365\n",
            "Epoch 106/300.. Learning rate: 0.0500.. Train loss: 0.0452.. Train acc: 0.9855.. Val loss: 3.2960.. Val acc: 0.5427\n",
            "Epoch 107/300.. Learning rate: 0.0500.. Train loss: 0.0446.. Train acc: 0.9855.. Val loss: 3.2550.. Val acc: 0.5414\n",
            "Epoch 108/300.. Learning rate: 0.0500.. Train loss: 0.0413.. Train acc: 0.9871.. Val loss: 3.2883.. Val acc: 0.5405\n",
            "Epoch 109/300.. Learning rate: 0.0500.. Train loss: 0.0402.. Train acc: 0.9874.. Val loss: 3.3289.. Val acc: 0.5412\n",
            "Epoch 110/300.. Learning rate: 0.0500.. Train loss: 0.0363.. Train acc: 0.9887.. Val loss: 3.3032.. Val acc: 0.5450\n",
            "Epoch 111/300.. Learning rate: 0.0500.. Train loss: 0.0374.. Train acc: 0.9886.. Val loss: 3.3152.. Val acc: 0.5448\n",
            "Epoch 112/300.. Learning rate: 0.0500.. Train loss: 0.0346.. Train acc: 0.9893.. Val loss: 3.2272.. Val acc: 0.5486\n",
            "Epoch 113/300.. Learning rate: 0.0500.. Train loss: 0.0317.. Train acc: 0.9900.. Val loss: 3.2723.. Val acc: 0.5448\n",
            "Epoch 114/300.. Learning rate: 0.0500.. Train loss: 0.0371.. Train acc: 0.9882.. Val loss: 3.3548.. Val acc: 0.5410\n",
            "Epoch 115/300.. Learning rate: 0.0500.. Train loss: 0.0401.. Train acc: 0.9871.. Val loss: 3.3104.. Val acc: 0.5432\n",
            "Epoch 116/300.. Learning rate: 0.0500.. Train loss: 0.0374.. Train acc: 0.9885.. Val loss: 3.3468.. Val acc: 0.5399\n",
            "Epoch 117/300.. Learning rate: 0.0500.. Train loss: 0.0370.. Train acc: 0.9883.. Val loss: 3.3492.. Val acc: 0.5457\n",
            "Epoch 118/300.. Learning rate: 0.0500.. Train loss: 0.0386.. Train acc: 0.9877.. Val loss: 3.3234.. Val acc: 0.5445\n",
            "Epoch 119/300.. Learning rate: 0.0500.. Train loss: 0.0343.. Train acc: 0.9896.. Val loss: 3.2891.. Val acc: 0.5488\n",
            "Epoch 120/300.. Learning rate: 0.0500.. Train loss: 0.0325.. Train acc: 0.9899.. Val loss: 3.3271.. Val acc: 0.5439\n",
            "Epoch 121/300.. Learning rate: 0.0500.. Train loss: 0.0300.. Train acc: 0.9906.. Val loss: 3.3561.. Val acc: 0.5450\n",
            "Epoch 122/300.. Learning rate: 0.0500.. Train loss: 0.0309.. Train acc: 0.9899.. Val loss: 3.3572.. Val acc: 0.5429\n",
            "Epoch 123/300.. Learning rate: 0.0500.. Train loss: 0.0286.. Train acc: 0.9908.. Val loss: 3.3457.. Val acc: 0.5467\n",
            "Epoch 124/300.. Learning rate: 0.0500.. Train loss: 0.0317.. Train acc: 0.9902.. Val loss: 3.3544.. Val acc: 0.5500\n",
            "Epoch 125/300.. Learning rate: 0.0500.. Train loss: 0.0315.. Train acc: 0.9902.. Val loss: 3.3513.. Val acc: 0.5395\n",
            "Epoch 126/300.. Learning rate: 0.0500.. Train loss: 0.0298.. Train acc: 0.9903.. Val loss: 3.3764.. Val acc: 0.5391\n",
            "Epoch 127/300.. Learning rate: 0.0500.. Train loss: 0.0296.. Train acc: 0.9908.. Val loss: 3.3860.. Val acc: 0.5439\n",
            "Epoch 128/300.. Learning rate: 0.0500.. Train loss: 0.0290.. Train acc: 0.9908.. Val loss: 3.3329.. Val acc: 0.5446\n",
            "Epoch 129/300.. Learning rate: 0.0500.. Train loss: 0.0294.. Train acc: 0.9910.. Val loss: 3.3936.. Val acc: 0.5449\n",
            "Epoch 130/300.. Learning rate: 0.0500.. Train loss: 0.0285.. Train acc: 0.9913.. Val loss: 3.3945.. Val acc: 0.5448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Weight Decay\n",
        "Add weight decay to the best learning rate you discovered, and the cosine learning rate schedule.\n",
        "Other configurations should remain identical to the previous experiment. Experiment with two different\n",
        "weight decay coefficients λ = 5 × 10−4 and 1 × 10−4, and illustrate their regularization effects using\n",
        "training-curve diagrams. Report the final losses and accuracy values for both the training set and the\n",
        "validation set. The network should be trained for 300 epochs."
      ],
      "metadata": {
        "id": "AbnzYPoevbnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochQ5 = 300\n",
        "#wd=0.0005, Cosine Annealing\n",
        "main(Args(128,0.05,epochQ5,0.0005,True,\"Accuracy and Loss with weight decay = 0.0005\"))\n",
        "\n",
        "#wd=0.0001, Cosine Annealing\n",
        "main(Args(128,0.05,epochQ5,0.0001,True,\"Accuracy and Loss with weight decay = 0.0001\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcnVJw4Mvj8I",
        "outputId": "8d82b4eb-c00c-4d77-a295-306daa429b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "MobileNet(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layers): Sequential(\n",
            "    (0): Block(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (6): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (7): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (8): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (9): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (10): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (11): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (12): Block(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=1024, out_features=100, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:809: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300.. Learning rate: 0.0500.. Train loss: 4.1763.. Train acc: 0.0612.. Val loss: 3.8822.. Val acc: 0.0880\n",
            "Epoch 2/300.. Learning rate: 0.0500.. Train loss: 3.6643.. Train acc: 0.1257.. Val loss: 3.5245.. Val acc: 0.1479\n",
            "Epoch 3/300.. Learning rate: 0.0500.. Train loss: 3.3642.. Train acc: 0.1752.. Val loss: 3.3454.. Val acc: 0.1820\n",
            "Epoch 4/300.. Learning rate: 0.0500.. Train loss: 3.1196.. Train acc: 0.2224.. Val loss: 3.0821.. Val acc: 0.2320\n",
            "Epoch 5/300.. Learning rate: 0.0500.. Train loss: 2.8845.. Train acc: 0.2624.. Val loss: 2.9797.. Val acc: 0.2566\n",
            "Epoch 6/300.. Learning rate: 0.0500.. Train loss: 2.6881.. Train acc: 0.3018.. Val loss: 2.6950.. Val acc: 0.3013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Data Augmentation\n",
        "With the best experimental setup you discovered so far, experiment with the mixup augmentation\n",
        "technique [3]. Set the hyperparameter α to 0.2. Draw the probability density function associated with\n",
        "the beta distribution parameterized by this α (5%). Train the network for 300 epochs."
      ],
      "metadata": {
        "id": "NIFIkWy80jbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mainMixUp(args):\n",
        "    # fix random seeds\n",
        "    torch.manual_seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "\n",
        "    # train val test\n",
        "    # AI6103 students: You need to create the dataloaders youself\n",
        "    train_loader, valid_loader = get_train_valid_loader(args.batch_size,args.seed)\n",
        "    if args.test:\n",
        "        test_loader = get_test_loader(args.batch_size)\n",
        "\n",
        "    # model\n",
        "    model = MobileNet(100)\n",
        "    print(model)\n",
        "    model.cuda()\n",
        "\n",
        "    # criterion\n",
        "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.wd)\n",
        "    if args.lr_scheduler:\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs)\n",
        "    else:\n",
        "        scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1.0, total_iters=args.epochs)\n",
        "\n",
        "    stat_training_loss = []\n",
        "    stat_val_loss = []\n",
        "    stat_training_acc = []\n",
        "    stat_val_acc = []\n",
        "    for epoch in range(args.epochs):\n",
        "        training_loss = 0\n",
        "        training_acc = 0\n",
        "        training_samples = 0\n",
        "        val_loss = 0\n",
        "        val_acc = 0\n",
        "        val_samples = 0\n",
        "        # training\n",
        "        model.train()\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "            batch_size = imgs.shape[0]\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            newImgs, ya, yb, lambdaMixUp = mixUpData(imgs, labels, 0.2)\n",
        "\n",
        "            logits = model.forward(newImgs)\n",
        "            loss = lambdaMixUp * criterion(logits, ya) + (1 - lambdaMixUp) * criterion(logits, yb)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            _, top_class = logits.topk(1, dim=1)\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            training_acc += torch.sum(equals.type(torch.FloatTensor)).item()\n",
        "            training_loss += batch_size * loss.item()\n",
        "            training_samples += batch_size\n",
        "        # validation\n",
        "        model.eval()\n",
        "        for val_imgs, val_labels in valid_loader:\n",
        "            batch_size = val_imgs.shape[0]\n",
        "            val_logits = model.forward(val_imgs.cuda())\n",
        "            loss = criterion(val_logits, val_labels.cuda())\n",
        "            _, top_class = val_logits.topk(1, dim=1)\n",
        "            equals = top_class == val_labels.cuda().view(*top_class.shape)\n",
        "            val_acc += torch.sum(equals.type(torch.FloatTensor)).item()\n",
        "            val_loss += batch_size * loss.item()\n",
        "            val_samples += batch_size\n",
        "        assert val_samples == 10000\n",
        "        # update stats\n",
        "        stat_training_loss.append(training_loss/training_samples)\n",
        "        stat_val_loss.append(val_loss/val_samples)\n",
        "        stat_training_acc.append(training_acc/training_samples)\n",
        "        stat_val_acc.append(val_acc/val_samples)\n",
        "        # print\n",
        "        print(f\"Epoch {(epoch+1):d}/{args.epochs:d}.. Learning rate: {scheduler.get_lr()[0]:.4f}.. Train loss: {(training_loss/training_samples):.4f}.. Train acc: {(training_acc/training_samples):.4f}.. Val loss: {(val_loss/val_samples):.4f}.. Val acc: {(val_acc/val_samples):.4f}\")\n",
        "        # lr scheduler\n",
        "        scheduler.step()\n",
        "    # plot\n",
        "    plot_loss_acc(stat_training_loss, stat_val_loss, stat_training_acc, stat_val_acc, args.fig_name)\n",
        "    # test\n",
        "    if args.test:\n",
        "        test_loss = 0\n",
        "        test_acc = 0\n",
        "        test_samples = 0\n",
        "        for test_imgs, test_labels in test_loader:\n",
        "            batch_size = test_imgs.shape[0]\n",
        "            test_logits = model.forward(test_imgs.cuda())\n",
        "            test_loss = criterion(test_logits, test_labels.cuda())\n",
        "            _, top_class = test_logits.topk(1, dim=1)\n",
        "            equals = top_class == test_labels.cuda().view(*top_class.shape)\n",
        "            test_acc += torch.sum(equals.type(torch.FloatTensor)).item()\n",
        "            test_loss += batch_size * test_loss.item()\n",
        "            test_samples += batch_size\n",
        "        assert test_samples == 10000\n",
        "        print('Test loss: ', test_loss/test_samples)\n",
        "        print('Test acc: ', test_acc/test_samples)\n",
        "\n",
        "def mixUpData(input, y, alpha):\n",
        "    if alpha > 0:\n",
        "        lambdaMixUp = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lambdaMixUp = 1\n",
        "\n",
        "    batch_size = input.size(0)\n",
        "    index = torch.randperm(batch_size).cuda()\n",
        "\n",
        "    newInput = lambdaMixUp * input + (1 - lambdaMixUp) * input[index, :]\n",
        "    ya, yb = y, y[index]\n",
        "    return newInput, ya,yb,lambdaMixUp\n"
      ],
      "metadata": {
        "id": "NUFg_Kzb_Tfs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochQ6=300\n",
        "mainMixUp(Args(128,0.05,epochQ6,0.0005,True,\"Accuracy and Loss with mix up of alpha = 0.2\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VXCmP2lH15Q9",
        "outputId": "554dc6a2-bfde-4e08-8c72-85e8105eeb3f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "MobileNet(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layers): Sequential(\n",
            "    (0): Block(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): Block(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): Block(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (6): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (7): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (8): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (9): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (10): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (11): Block(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (12): Block(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=1024, out_features=100, bias=True)\n",
            ")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:809: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300.. Learning rate: 0.0500.. Train loss: 4.2633.. Train acc: 0.0334.. Val loss: 3.8645.. Val acc: 0.0951\n",
            "Epoch 2/300.. Learning rate: 0.0500.. Train loss: 3.8562.. Train acc: 0.0663.. Val loss: 3.5733.. Val acc: 0.1376\n",
            "Epoch 3/300.. Learning rate: 0.0500.. Train loss: 3.6799.. Train acc: 0.0814.. Val loss: 3.4297.. Val acc: 0.1679\n",
            "Epoch 4/300.. Learning rate: 0.0500.. Train loss: 3.4898.. Train acc: 0.1051.. Val loss: 3.3132.. Val acc: 0.2016\n",
            "Epoch 5/300.. Learning rate: 0.0500.. Train loss: 3.3032.. Train acc: 0.1258.. Val loss: 3.0228.. Val acc: 0.2445\n",
            "Epoch 6/300.. Learning rate: 0.0500.. Train loss: 3.1771.. Train acc: 0.1452.. Val loss: 2.8765.. Val acc: 0.2754\n",
            "Epoch 7/300.. Learning rate: 0.0499.. Train loss: 2.9986.. Train acc: 0.1526.. Val loss: 2.7300.. Val acc: 0.2958\n",
            "Epoch 8/300.. Learning rate: 0.0499.. Train loss: 2.9118.. Train acc: 0.1642.. Val loss: 2.6717.. Val acc: 0.3218\n",
            "Epoch 9/300.. Learning rate: 0.0499.. Train loss: 2.7656.. Train acc: 0.1804.. Val loss: 2.6316.. Val acc: 0.3193\n",
            "Epoch 10/300.. Learning rate: 0.0499.. Train loss: 2.7316.. Train acc: 0.1831.. Val loss: 2.4967.. Val acc: 0.3501\n",
            "Epoch 11/300.. Learning rate: 0.0498.. Train loss: 2.5685.. Train acc: 0.2089.. Val loss: 2.4281.. Val acc: 0.3748\n",
            "Epoch 12/300.. Learning rate: 0.0498.. Train loss: 2.5457.. Train acc: 0.2141.. Val loss: 2.4870.. Val acc: 0.3628\n",
            "Epoch 13/300.. Learning rate: 0.0498.. Train loss: 2.5449.. Train acc: 0.2037.. Val loss: 2.2075.. Val acc: 0.4194\n",
            "Epoch 14/300.. Learning rate: 0.0497.. Train loss: 2.3963.. Train acc: 0.2426.. Val loss: 2.1217.. Val acc: 0.4315\n",
            "Epoch 15/300.. Learning rate: 0.0497.. Train loss: 2.3796.. Train acc: 0.2336.. Val loss: 2.2796.. Val acc: 0.4050\n",
            "Epoch 16/300.. Learning rate: 0.0497.. Train loss: 2.3584.. Train acc: 0.2382.. Val loss: 2.0540.. Val acc: 0.4537\n",
            "Epoch 17/300.. Learning rate: 0.0496.. Train loss: 2.3575.. Train acc: 0.2662.. Val loss: 2.0369.. Val acc: 0.4665\n",
            "Epoch 18/300.. Learning rate: 0.0496.. Train loss: 2.3156.. Train acc: 0.2521.. Val loss: 2.0453.. Val acc: 0.4565\n",
            "Epoch 19/300.. Learning rate: 0.0495.. Train loss: 2.2592.. Train acc: 0.2354.. Val loss: 2.0204.. Val acc: 0.4644\n",
            "Epoch 20/300.. Learning rate: 0.0495.. Train loss: 2.2966.. Train acc: 0.2475.. Val loss: 2.1678.. Val acc: 0.4226\n",
            "Epoch 21/300.. Learning rate: 0.0494.. Train loss: 2.2710.. Train acc: 0.2415.. Val loss: 2.0219.. Val acc: 0.4672\n",
            "Epoch 22/300.. Learning rate: 0.0493.. Train loss: 2.2251.. Train acc: 0.2586.. Val loss: 2.0371.. Val acc: 0.4610\n",
            "Epoch 23/300.. Learning rate: 0.0493.. Train loss: 2.2068.. Train acc: 0.2607.. Val loss: 2.0806.. Val acc: 0.4554\n",
            "Epoch 24/300.. Learning rate: 0.0492.. Train loss: 2.2024.. Train acc: 0.2544.. Val loss: 2.0205.. Val acc: 0.4703\n",
            "Epoch 25/300.. Learning rate: 0.0492.. Train loss: 2.1373.. Train acc: 0.2430.. Val loss: 1.9063.. Val acc: 0.5012\n",
            "Epoch 26/300.. Learning rate: 0.0491.. Train loss: 2.1262.. Train acc: 0.2646.. Val loss: 2.0160.. Val acc: 0.4698\n",
            "Epoch 27/300.. Learning rate: 0.0490.. Train loss: 2.1288.. Train acc: 0.2783.. Val loss: 1.9313.. Val acc: 0.4920\n",
            "Epoch 28/300.. Learning rate: 0.0489.. Train loss: 2.1574.. Train acc: 0.2726.. Val loss: 1.9244.. Val acc: 0.4816\n",
            "Epoch 29/300.. Learning rate: 0.0489.. Train loss: 2.1351.. Train acc: 0.2321.. Val loss: 1.9271.. Val acc: 0.4963\n",
            "Epoch 30/300.. Learning rate: 0.0488.. Train loss: 2.0669.. Train acc: 0.2897.. Val loss: 1.9243.. Val acc: 0.4950\n",
            "Epoch 31/300.. Learning rate: 0.0487.. Train loss: 2.0747.. Train acc: 0.2877.. Val loss: 1.9354.. Val acc: 0.4876\n",
            "Epoch 32/300.. Learning rate: 0.0486.. Train loss: 2.1269.. Train acc: 0.2741.. Val loss: 2.0532.. Val acc: 0.4689\n",
            "Epoch 33/300.. Learning rate: 0.0485.. Train loss: 2.1241.. Train acc: 0.2940.. Val loss: 1.8934.. Val acc: 0.4989\n",
            "Epoch 34/300.. Learning rate: 0.0484.. Train loss: 2.1080.. Train acc: 0.2681.. Val loss: 1.8419.. Val acc: 0.5106\n",
            "Epoch 35/300.. Learning rate: 0.0483.. Train loss: 2.0504.. Train acc: 0.3014.. Val loss: 1.9302.. Val acc: 0.4910\n",
            "Epoch 36/300.. Learning rate: 0.0482.. Train loss: 2.0636.. Train acc: 0.2890.. Val loss: 1.9452.. Val acc: 0.4870\n",
            "Epoch 37/300.. Learning rate: 0.0481.. Train loss: 2.0544.. Train acc: 0.2887.. Val loss: 1.9400.. Val acc: 0.4924\n",
            "Epoch 38/300.. Learning rate: 0.0480.. Train loss: 2.0936.. Train acc: 0.3099.. Val loss: 1.9503.. Val acc: 0.4948\n",
            "Epoch 39/300.. Learning rate: 0.0479.. Train loss: 2.0469.. Train acc: 0.2816.. Val loss: 2.0144.. Val acc: 0.4817\n",
            "Epoch 40/300.. Learning rate: 0.0478.. Train loss: 2.0072.. Train acc: 0.2983.. Val loss: 1.9737.. Val acc: 0.4858\n",
            "Epoch 41/300.. Learning rate: 0.0477.. Train loss: 2.0656.. Train acc: 0.2904.. Val loss: 1.8970.. Val acc: 0.5043\n",
            "Epoch 42/300.. Learning rate: 0.0476.. Train loss: 2.0267.. Train acc: 0.2748.. Val loss: 1.8175.. Val acc: 0.5138\n",
            "Epoch 43/300.. Learning rate: 0.0475.. Train loss: 2.0084.. Train acc: 0.2622.. Val loss: 1.8987.. Val acc: 0.5029\n",
            "Epoch 44/300.. Learning rate: 0.0474.. Train loss: 2.0770.. Train acc: 0.2840.. Val loss: 1.8996.. Val acc: 0.5050\n",
            "Epoch 45/300.. Learning rate: 0.0473.. Train loss: 1.9830.. Train acc: 0.3002.. Val loss: 1.8510.. Val acc: 0.5105\n",
            "Epoch 46/300.. Learning rate: 0.0472.. Train loss: 1.9747.. Train acc: 0.2847.. Val loss: 1.7746.. Val acc: 0.5302\n",
            "Epoch 47/300.. Learning rate: 0.0470.. Train loss: 1.9401.. Train acc: 0.2632.. Val loss: 1.8515.. Val acc: 0.5145\n",
            "Epoch 48/300.. Learning rate: 0.0469.. Train loss: 1.9530.. Train acc: 0.3070.. Val loss: 1.8711.. Val acc: 0.5100\n",
            "Epoch 49/300.. Learning rate: 0.0468.. Train loss: 2.0019.. Train acc: 0.3037.. Val loss: 1.9520.. Val acc: 0.4948\n",
            "Epoch 50/300.. Learning rate: 0.0467.. Train loss: 2.0034.. Train acc: 0.2869.. Val loss: 2.0245.. Val acc: 0.4747\n",
            "Epoch 51/300.. Learning rate: 0.0465.. Train loss: 1.9734.. Train acc: 0.3463.. Val loss: 1.8540.. Val acc: 0.5102\n",
            "Epoch 52/300.. Learning rate: 0.0464.. Train loss: 1.9614.. Train acc: 0.2687.. Val loss: 1.8631.. Val acc: 0.5159\n",
            "Epoch 53/300.. Learning rate: 0.0463.. Train loss: 2.0290.. Train acc: 0.2994.. Val loss: 1.9014.. Val acc: 0.5135\n",
            "Epoch 54/300.. Learning rate: 0.0461.. Train loss: 1.9123.. Train acc: 0.2857.. Val loss: 1.7588.. Val acc: 0.5351\n",
            "Epoch 55/300.. Learning rate: 0.0460.. Train loss: 1.8841.. Train acc: 0.2680.. Val loss: 1.8475.. Val acc: 0.5109\n",
            "Epoch 56/300.. Learning rate: 0.0458.. Train loss: 1.9717.. Train acc: 0.2780.. Val loss: 1.7543.. Val acc: 0.5365\n",
            "Epoch 57/300.. Learning rate: 0.0457.. Train loss: 1.9399.. Train acc: 0.2998.. Val loss: 1.8126.. Val acc: 0.5301\n",
            "Epoch 58/300.. Learning rate: 0.0455.. Train loss: 1.9231.. Train acc: 0.2947.. Val loss: 1.8036.. Val acc: 0.5167\n",
            "Epoch 59/300.. Learning rate: 0.0454.. Train loss: 1.9482.. Train acc: 0.2797.. Val loss: 1.8318.. Val acc: 0.5177\n",
            "Epoch 60/300.. Learning rate: 0.0452.. Train loss: 2.0082.. Train acc: 0.2894.. Val loss: 1.7702.. Val acc: 0.5329\n",
            "Epoch 61/300.. Learning rate: 0.0451.. Train loss: 1.9224.. Train acc: 0.3070.. Val loss: 1.7995.. Val acc: 0.5257\n",
            "Epoch 62/300.. Learning rate: 0.0449.. Train loss: 1.9431.. Train acc: 0.3412.. Val loss: 1.8243.. Val acc: 0.5201\n",
            "Epoch 63/300.. Learning rate: 0.0448.. Train loss: 1.9723.. Train acc: 0.2880.. Val loss: 1.8142.. Val acc: 0.5257\n",
            "Epoch 64/300.. Learning rate: 0.0446.. Train loss: 1.9348.. Train acc: 0.2874.. Val loss: 1.7886.. Val acc: 0.5267\n",
            "Epoch 65/300.. Learning rate: 0.0444.. Train loss: 1.8716.. Train acc: 0.3184.. Val loss: 1.8684.. Val acc: 0.5115\n",
            "Epoch 66/300.. Learning rate: 0.0443.. Train loss: 1.9279.. Train acc: 0.3038.. Val loss: 1.9284.. Val acc: 0.5048\n",
            "Epoch 67/300.. Learning rate: 0.0441.. Train loss: 1.8819.. Train acc: 0.3204.. Val loss: 1.6941.. Val acc: 0.5484\n",
            "Epoch 68/300.. Learning rate: 0.0439.. Train loss: 1.8898.. Train acc: 0.3192.. Val loss: 1.6934.. Val acc: 0.5516\n",
            "Epoch 69/300.. Learning rate: 0.0438.. Train loss: 1.8812.. Train acc: 0.3159.. Val loss: 1.7433.. Val acc: 0.5421\n",
            "Epoch 70/300.. Learning rate: 0.0436.. Train loss: 1.8182.. Train acc: 0.3222.. Val loss: 1.7814.. Val acc: 0.5222\n",
            "Epoch 71/300.. Learning rate: 0.0434.. Train loss: 1.8986.. Train acc: 0.3718.. Val loss: 1.7586.. Val acc: 0.5294\n",
            "Epoch 72/300.. Learning rate: 0.0432.. Train loss: 1.9244.. Train acc: 0.3279.. Val loss: 1.6789.. Val acc: 0.5532\n",
            "Epoch 73/300.. Learning rate: 0.0430.. Train loss: 1.8281.. Train acc: 0.3366.. Val loss: 1.7908.. Val acc: 0.5292\n",
            "Epoch 74/300.. Learning rate: 0.0429.. Train loss: 1.8485.. Train acc: 0.2886.. Val loss: 1.7005.. Val acc: 0.5528\n",
            "Epoch 75/300.. Learning rate: 0.0427.. Train loss: 1.8695.. Train acc: 0.3071.. Val loss: 1.8530.. Val acc: 0.5123\n",
            "Epoch 76/300.. Learning rate: 0.0425.. Train loss: 1.8495.. Train acc: 0.3196.. Val loss: 1.7585.. Val acc: 0.5369\n",
            "Epoch 77/300.. Learning rate: 0.0423.. Train loss: 1.8366.. Train acc: 0.3313.. Val loss: 1.6724.. Val acc: 0.5536\n",
            "Epoch 78/300.. Learning rate: 0.0421.. Train loss: 1.8241.. Train acc: 0.3208.. Val loss: 1.7673.. Val acc: 0.5437\n",
            "Epoch 79/300.. Learning rate: 0.0419.. Train loss: 1.7889.. Train acc: 0.3276.. Val loss: 1.9232.. Val acc: 0.4983\n",
            "Epoch 80/300.. Learning rate: 0.0417.. Train loss: 1.8098.. Train acc: 0.3115.. Val loss: 1.6981.. Val acc: 0.5510\n",
            "Epoch 81/300.. Learning rate: 0.0415.. Train loss: 1.9148.. Train acc: 0.2975.. Val loss: 1.6365.. Val acc: 0.5567\n",
            "Epoch 82/300.. Learning rate: 0.0413.. Train loss: 1.9049.. Train acc: 0.3177.. Val loss: 1.7370.. Val acc: 0.5506\n",
            "Epoch 83/300.. Learning rate: 0.0411.. Train loss: 1.8466.. Train acc: 0.2890.. Val loss: 1.9369.. Val acc: 0.5040\n",
            "Epoch 84/300.. Learning rate: 0.0409.. Train loss: 1.8945.. Train acc: 0.3193.. Val loss: 1.7406.. Val acc: 0.5516\n",
            "Epoch 85/300.. Learning rate: 0.0407.. Train loss: 1.7714.. Train acc: 0.3201.. Val loss: 1.7718.. Val acc: 0.5282\n",
            "Epoch 86/300.. Learning rate: 0.0405.. Train loss: 1.8627.. Train acc: 0.3514.. Val loss: 1.7919.. Val acc: 0.5274\n",
            "Epoch 87/300.. Learning rate: 0.0403.. Train loss: 1.8371.. Train acc: 0.3171.. Val loss: 1.7658.. Val acc: 0.5324\n",
            "Epoch 88/300.. Learning rate: 0.0401.. Train loss: 1.9381.. Train acc: 0.3149.. Val loss: 1.6747.. Val acc: 0.5638\n",
            "Epoch 89/300.. Learning rate: 0.0399.. Train loss: 1.7945.. Train acc: 0.3106.. Val loss: 1.7032.. Val acc: 0.5502\n",
            "Epoch 90/300.. Learning rate: 0.0397.. Train loss: 1.8679.. Train acc: 0.3275.. Val loss: 1.8600.. Val acc: 0.5217\n",
            "Epoch 91/300.. Learning rate: 0.0395.. Train loss: 1.7406.. Train acc: 0.3222.. Val loss: 1.8064.. Val acc: 0.5257\n",
            "Epoch 92/300.. Learning rate: 0.0393.. Train loss: 1.7575.. Train acc: 0.3099.. Val loss: 1.7969.. Val acc: 0.5393\n",
            "Epoch 93/300.. Learning rate: 0.0391.. Train loss: 1.8203.. Train acc: 0.3346.. Val loss: 1.7053.. Val acc: 0.5533\n",
            "Epoch 94/300.. Learning rate: 0.0388.. Train loss: 1.8282.. Train acc: 0.3353.. Val loss: 1.7814.. Val acc: 0.5365\n",
            "Epoch 95/300.. Learning rate: 0.0386.. Train loss: 1.7484.. Train acc: 0.3397.. Val loss: 1.7346.. Val acc: 0.5473\n",
            "Epoch 96/300.. Learning rate: 0.0384.. Train loss: 1.7769.. Train acc: 0.3049.. Val loss: 1.6763.. Val acc: 0.5511\n",
            "Epoch 97/300.. Learning rate: 0.0382.. Train loss: 1.7753.. Train acc: 0.3359.. Val loss: 1.7278.. Val acc: 0.5480\n",
            "Epoch 98/300.. Learning rate: 0.0380.. Train loss: 1.8447.. Train acc: 0.3183.. Val loss: 1.7260.. Val acc: 0.5451\n",
            "Epoch 99/300.. Learning rate: 0.0377.. Train loss: 1.6869.. Train acc: 0.3182.. Val loss: 1.6862.. Val acc: 0.5547\n",
            "Epoch 100/300.. Learning rate: 0.0375.. Train loss: 1.8087.. Train acc: 0.3337.. Val loss: 1.7379.. Val acc: 0.5499\n",
            "Epoch 101/300.. Learning rate: 0.0373.. Train loss: 1.7975.. Train acc: 0.3094.. Val loss: 1.6104.. Val acc: 0.5687\n",
            "Epoch 102/300.. Learning rate: 0.0370.. Train loss: 1.7588.. Train acc: 0.3036.. Val loss: 1.5832.. Val acc: 0.5807\n",
            "Epoch 103/300.. Learning rate: 0.0368.. Train loss: 1.7661.. Train acc: 0.3604.. Val loss: 1.8342.. Val acc: 0.5324\n",
            "Epoch 104/300.. Learning rate: 0.0366.. Train loss: 1.7549.. Train acc: 0.3476.. Val loss: 1.6050.. Val acc: 0.5698\n",
            "Epoch 105/300.. Learning rate: 0.0364.. Train loss: 1.7498.. Train acc: 0.3317.. Val loss: 1.8052.. Val acc: 0.5269\n",
            "Epoch 106/300.. Learning rate: 0.0361.. Train loss: 1.6779.. Train acc: 0.3486.. Val loss: 1.8557.. Val acc: 0.5273\n",
            "Epoch 107/300.. Learning rate: 0.0359.. Train loss: 1.7407.. Train acc: 0.3276.. Val loss: 1.6565.. Val acc: 0.5643\n",
            "Epoch 108/300.. Learning rate: 0.0356.. Train loss: 1.7472.. Train acc: 0.3256.. Val loss: 1.6552.. Val acc: 0.5614\n",
            "Epoch 109/300.. Learning rate: 0.0354.. Train loss: 1.6823.. Train acc: 0.3482.. Val loss: 1.6665.. Val acc: 0.5562\n",
            "Epoch 110/300.. Learning rate: 0.0352.. Train loss: 1.7012.. Train acc: 0.3242.. Val loss: 1.6156.. Val acc: 0.5753\n",
            "Epoch 111/300.. Learning rate: 0.0349.. Train loss: 1.7019.. Train acc: 0.3949.. Val loss: 1.6500.. Val acc: 0.5641\n",
            "Epoch 112/300.. Learning rate: 0.0347.. Train loss: 1.6451.. Train acc: 0.3491.. Val loss: 1.7129.. Val acc: 0.5514\n",
            "Epoch 113/300.. Learning rate: 0.0344.. Train loss: 1.6999.. Train acc: 0.3184.. Val loss: 1.5719.. Val acc: 0.5834\n",
            "Epoch 114/300.. Learning rate: 0.0342.. Train loss: 1.6838.. Train acc: 0.3389.. Val loss: 1.7021.. Val acc: 0.5511\n",
            "Epoch 115/300.. Learning rate: 0.0340.. Train loss: 1.6283.. Train acc: 0.3109.. Val loss: 1.6199.. Val acc: 0.5751\n",
            "Epoch 116/300.. Learning rate: 0.0337.. Train loss: 1.7519.. Train acc: 0.3205.. Val loss: 1.6554.. Val acc: 0.5701\n",
            "Epoch 117/300.. Learning rate: 0.0335.. Train loss: 1.7001.. Train acc: 0.3209.. Val loss: 1.6685.. Val acc: 0.5619\n",
            "Epoch 118/300.. Learning rate: 0.0332.. Train loss: 1.7221.. Train acc: 0.3325.. Val loss: 1.7112.. Val acc: 0.5528\n",
            "Epoch 119/300.. Learning rate: 0.0330.. Train loss: 1.6373.. Train acc: 0.3128.. Val loss: 1.6621.. Val acc: 0.5620\n",
            "Epoch 120/300.. Learning rate: 0.0327.. Train loss: 1.6028.. Train acc: 0.3549.. Val loss: 1.6963.. Val acc: 0.5567\n",
            "Epoch 121/300.. Learning rate: 0.0325.. Train loss: 1.5847.. Train acc: 0.3640.. Val loss: 1.6103.. Val acc: 0.5704\n",
            "Epoch 122/300.. Learning rate: 0.0322.. Train loss: 1.6231.. Train acc: 0.3486.. Val loss: 1.6070.. Val acc: 0.5764\n",
            "Epoch 123/300.. Learning rate: 0.0320.. Train loss: 1.6352.. Train acc: 0.3586.. Val loss: 1.6514.. Val acc: 0.5623\n",
            "Epoch 124/300.. Learning rate: 0.0317.. Train loss: 1.6361.. Train acc: 0.3442.. Val loss: 1.7037.. Val acc: 0.5565\n",
            "Epoch 125/300.. Learning rate: 0.0315.. Train loss: 1.7147.. Train acc: 0.3628.. Val loss: 1.5646.. Val acc: 0.5884\n",
            "Epoch 126/300.. Learning rate: 0.0312.. Train loss: 1.7074.. Train acc: 0.3408.. Val loss: 1.5504.. Val acc: 0.5951\n",
            "Epoch 127/300.. Learning rate: 0.0310.. Train loss: 1.5805.. Train acc: 0.3773.. Val loss: 1.6197.. Val acc: 0.5727\n",
            "Epoch 128/300.. Learning rate: 0.0307.. Train loss: 1.6263.. Train acc: 0.3398.. Val loss: 1.5874.. Val acc: 0.5831\n",
            "Epoch 129/300.. Learning rate: 0.0305.. Train loss: 1.6240.. Train acc: 0.3603.. Val loss: 1.5720.. Val acc: 0.5888\n",
            "Epoch 130/300.. Learning rate: 0.0302.. Train loss: 1.6896.. Train acc: 0.3431.. Val loss: 1.5957.. Val acc: 0.5825\n",
            "Epoch 131/300.. Learning rate: 0.0299.. Train loss: 1.5843.. Train acc: 0.3625.. Val loss: 1.6110.. Val acc: 0.5797\n",
            "Epoch 132/300.. Learning rate: 0.0297.. Train loss: 1.6009.. Train acc: 0.3676.. Val loss: 1.5996.. Val acc: 0.5780\n",
            "Epoch 133/300.. Learning rate: 0.0294.. Train loss: 1.5765.. Train acc: 0.3382.. Val loss: 1.5733.. Val acc: 0.5835\n",
            "Epoch 134/300.. Learning rate: 0.0292.. Train loss: 1.6221.. Train acc: 0.3567.. Val loss: 1.6035.. Val acc: 0.5830\n",
            "Epoch 135/300.. Learning rate: 0.0289.. Train loss: 1.6410.. Train acc: 0.3577.. Val loss: 1.5835.. Val acc: 0.5850\n",
            "Epoch 136/300.. Learning rate: 0.0287.. Train loss: 1.5480.. Train acc: 0.3770.. Val loss: 1.5973.. Val acc: 0.5880\n",
            "Epoch 137/300.. Learning rate: 0.0284.. Train loss: 1.5921.. Train acc: 0.3493.. Val loss: 1.5474.. Val acc: 0.5939\n",
            "Epoch 138/300.. Learning rate: 0.0281.. Train loss: 1.5784.. Train acc: 0.3445.. Val loss: 1.5578.. Val acc: 0.5882\n",
            "Epoch 139/300.. Learning rate: 0.0279.. Train loss: 1.6117.. Train acc: 0.3869.. Val loss: 1.6389.. Val acc: 0.5755\n",
            "Epoch 140/300.. Learning rate: 0.0276.. Train loss: 1.5196.. Train acc: 0.3565.. Val loss: 1.6153.. Val acc: 0.5806\n",
            "Epoch 141/300.. Learning rate: 0.0274.. Train loss: 1.5074.. Train acc: 0.3642.. Val loss: 1.6321.. Val acc: 0.5691\n",
            "Epoch 142/300.. Learning rate: 0.0271.. Train loss: 1.5532.. Train acc: 0.3855.. Val loss: 1.5904.. Val acc: 0.5853\n",
            "Epoch 143/300.. Learning rate: 0.0268.. Train loss: 1.5692.. Train acc: 0.3721.. Val loss: 1.5715.. Val acc: 0.5886\n",
            "Epoch 144/300.. Learning rate: 0.0266.. Train loss: 1.5055.. Train acc: 0.3625.. Val loss: 1.6067.. Val acc: 0.5856\n",
            "Epoch 145/300.. Learning rate: 0.0263.. Train loss: 1.4991.. Train acc: 0.3574.. Val loss: 1.5104.. Val acc: 0.5988\n",
            "Epoch 146/300.. Learning rate: 0.0260.. Train loss: 1.6495.. Train acc: 0.3784.. Val loss: 1.5950.. Val acc: 0.5796\n",
            "Epoch 147/300.. Learning rate: 0.0258.. Train loss: 1.4197.. Train acc: 0.3706.. Val loss: 1.6099.. Val acc: 0.5870\n",
            "Epoch 148/300.. Learning rate: 0.0255.. Train loss: 1.6535.. Train acc: 0.3413.. Val loss: 1.5438.. Val acc: 0.6011\n",
            "Epoch 149/300.. Learning rate: 0.0253.. Train loss: 1.4919.. Train acc: 0.3869.. Val loss: 1.5270.. Val acc: 0.6002\n",
            "Epoch 150/300.. Learning rate: 0.0250.. Train loss: 1.5324.. Train acc: 0.3744.. Val loss: 1.5777.. Val acc: 0.5875\n",
            "Epoch 151/300.. Learning rate: 0.0247.. Train loss: 1.5035.. Train acc: 0.3426.. Val loss: 1.5498.. Val acc: 0.5941\n",
            "Epoch 152/300.. Learning rate: 0.0245.. Train loss: 1.5325.. Train acc: 0.3463.. Val loss: 1.5454.. Val acc: 0.6025\n",
            "Epoch 153/300.. Learning rate: 0.0242.. Train loss: 1.4363.. Train acc: 0.4130.. Val loss: 1.5101.. Val acc: 0.5980\n",
            "Epoch 154/300.. Learning rate: 0.0240.. Train loss: 1.4888.. Train acc: 0.3651.. Val loss: 1.4930.. Val acc: 0.6050\n",
            "Epoch 155/300.. Learning rate: 0.0237.. Train loss: 1.4254.. Train acc: 0.3590.. Val loss: 1.5219.. Val acc: 0.6024\n",
            "Epoch 156/300.. Learning rate: 0.0234.. Train loss: 1.5553.. Train acc: 0.3881.. Val loss: 1.6338.. Val acc: 0.5826\n",
            "Epoch 157/300.. Learning rate: 0.0232.. Train loss: 1.5206.. Train acc: 0.3770.. Val loss: 1.5608.. Val acc: 0.5887\n",
            "Epoch 158/300.. Learning rate: 0.0229.. Train loss: 1.4300.. Train acc: 0.3813.. Val loss: 1.5481.. Val acc: 0.5934\n",
            "Epoch 159/300.. Learning rate: 0.0227.. Train loss: 1.4728.. Train acc: 0.4113.. Val loss: 1.4983.. Val acc: 0.6013\n",
            "Epoch 160/300.. Learning rate: 0.0224.. Train loss: 1.4452.. Train acc: 0.3920.. Val loss: 1.5804.. Val acc: 0.5903\n",
            "Epoch 161/300.. Learning rate: 0.0221.. Train loss: 1.5436.. Train acc: 0.3675.. Val loss: 1.5344.. Val acc: 0.6078\n",
            "Epoch 162/300.. Learning rate: 0.0219.. Train loss: 1.4430.. Train acc: 0.4026.. Val loss: 1.5738.. Val acc: 0.5955\n",
            "Epoch 163/300.. Learning rate: 0.0216.. Train loss: 1.4043.. Train acc: 0.3952.. Val loss: 1.4917.. Val acc: 0.6132\n",
            "Epoch 164/300.. Learning rate: 0.0214.. Train loss: 1.4604.. Train acc: 0.3888.. Val loss: 1.6066.. Val acc: 0.5906\n",
            "Epoch 165/300.. Learning rate: 0.0211.. Train loss: 1.4385.. Train acc: 0.3745.. Val loss: 1.4628.. Val acc: 0.6174\n",
            "Epoch 166/300.. Learning rate: 0.0208.. Train loss: 1.3571.. Train acc: 0.3694.. Val loss: 1.5571.. Val acc: 0.6001\n",
            "Epoch 167/300.. Learning rate: 0.0206.. Train loss: 1.4184.. Train acc: 0.3915.. Val loss: 1.4822.. Val acc: 0.6175\n",
            "Epoch 168/300.. Learning rate: 0.0203.. Train loss: 1.3549.. Train acc: 0.4092.. Val loss: 1.4916.. Val acc: 0.6104\n",
            "Epoch 169/300.. Learning rate: 0.0201.. Train loss: 1.4269.. Train acc: 0.3928.. Val loss: 1.5031.. Val acc: 0.6075\n",
            "Epoch 170/300.. Learning rate: 0.0198.. Train loss: 1.3484.. Train acc: 0.4108.. Val loss: 1.4945.. Val acc: 0.6166\n",
            "Epoch 171/300.. Learning rate: 0.0195.. Train loss: 1.3903.. Train acc: 0.4356.. Val loss: 1.5397.. Val acc: 0.5998\n",
            "Epoch 172/300.. Learning rate: 0.0193.. Train loss: 1.4620.. Train acc: 0.3594.. Val loss: 1.4462.. Val acc: 0.6218\n",
            "Epoch 173/300.. Learning rate: 0.0190.. Train loss: 1.3720.. Train acc: 0.3721.. Val loss: 1.4618.. Val acc: 0.6160\n",
            "Epoch 174/300.. Learning rate: 0.0188.. Train loss: 1.3018.. Train acc: 0.3979.. Val loss: 1.5142.. Val acc: 0.6076\n",
            "Epoch 175/300.. Learning rate: 0.0185.. Train loss: 1.3129.. Train acc: 0.3777.. Val loss: 1.5395.. Val acc: 0.6058\n",
            "Epoch 176/300.. Learning rate: 0.0183.. Train loss: 1.4146.. Train acc: 0.3821.. Val loss: 1.5405.. Val acc: 0.6072\n",
            "Epoch 177/300.. Learning rate: 0.0180.. Train loss: 1.2625.. Train acc: 0.3846.. Val loss: 1.5151.. Val acc: 0.6034\n",
            "Epoch 178/300.. Learning rate: 0.0178.. Train loss: 1.3117.. Train acc: 0.4027.. Val loss: 1.5592.. Val acc: 0.5978\n",
            "Epoch 179/300.. Learning rate: 0.0175.. Train loss: 1.3088.. Train acc: 0.3784.. Val loss: 1.4938.. Val acc: 0.6054\n",
            "Epoch 180/300.. Learning rate: 0.0173.. Train loss: 1.3290.. Train acc: 0.4168.. Val loss: 1.4571.. Val acc: 0.6176\n",
            "Epoch 181/300.. Learning rate: 0.0170.. Train loss: 1.3125.. Train acc: 0.4042.. Val loss: 1.4820.. Val acc: 0.6140\n",
            "Epoch 182/300.. Learning rate: 0.0168.. Train loss: 1.3107.. Train acc: 0.4314.. Val loss: 1.4672.. Val acc: 0.6128\n",
            "Epoch 183/300.. Learning rate: 0.0165.. Train loss: 1.4277.. Train acc: 0.3767.. Val loss: 1.4780.. Val acc: 0.6164\n",
            "Epoch 184/300.. Learning rate: 0.0163.. Train loss: 1.2742.. Train acc: 0.3909.. Val loss: 1.5184.. Val acc: 0.6092\n",
            "Epoch 185/300.. Learning rate: 0.0160.. Train loss: 1.3048.. Train acc: 0.4517.. Val loss: 1.5145.. Val acc: 0.6118\n",
            "Epoch 186/300.. Learning rate: 0.0158.. Train loss: 1.1644.. Train acc: 0.4129.. Val loss: 1.4302.. Val acc: 0.6252\n",
            "Epoch 187/300.. Learning rate: 0.0156.. Train loss: 1.1812.. Train acc: 0.4067.. Val loss: 1.4849.. Val acc: 0.6153\n",
            "Epoch 188/300.. Learning rate: 0.0153.. Train loss: 1.1498.. Train acc: 0.4264.. Val loss: 1.4820.. Val acc: 0.6156\n",
            "Epoch 189/300.. Learning rate: 0.0151.. Train loss: 1.1522.. Train acc: 0.4177.. Val loss: 1.5032.. Val acc: 0.6080\n",
            "Epoch 190/300.. Learning rate: 0.0148.. Train loss: 1.1070.. Train acc: 0.3938.. Val loss: 1.4911.. Val acc: 0.6184\n",
            "Epoch 191/300.. Learning rate: 0.0146.. Train loss: 1.1071.. Train acc: 0.3995.. Val loss: 1.4633.. Val acc: 0.6260\n",
            "Epoch 192/300.. Learning rate: 0.0144.. Train loss: 1.1829.. Train acc: 0.4377.. Val loss: 1.5125.. Val acc: 0.6193\n",
            "Epoch 193/300.. Learning rate: 0.0141.. Train loss: 1.1632.. Train acc: 0.4392.. Val loss: 1.4924.. Val acc: 0.6154\n",
            "Epoch 194/300.. Learning rate: 0.0139.. Train loss: 1.1038.. Train acc: 0.4129.. Val loss: 1.4823.. Val acc: 0.6208\n",
            "Epoch 195/300.. Learning rate: 0.0137.. Train loss: 1.2434.. Train acc: 0.4144.. Val loss: 1.4363.. Val acc: 0.6270\n",
            "Epoch 196/300.. Learning rate: 0.0134.. Train loss: 1.1578.. Train acc: 0.4164.. Val loss: 1.4850.. Val acc: 0.6239\n",
            "Epoch 197/300.. Learning rate: 0.0132.. Train loss: 1.1634.. Train acc: 0.4261.. Val loss: 1.4632.. Val acc: 0.6320\n",
            "Epoch 198/300.. Learning rate: 0.0130.. Train loss: 1.1705.. Train acc: 0.4219.. Val loss: 1.4679.. Val acc: 0.6292\n",
            "Epoch 199/300.. Learning rate: 0.0127.. Train loss: 1.1044.. Train acc: 0.3941.. Val loss: 1.4480.. Val acc: 0.6245\n",
            "Epoch 200/300.. Learning rate: 0.0125.. Train loss: 1.1343.. Train acc: 0.4526.. Val loss: 1.4762.. Val acc: 0.6183\n",
            "Epoch 201/300.. Learning rate: 0.0123.. Train loss: 1.0939.. Train acc: 0.4303.. Val loss: 1.4298.. Val acc: 0.6293\n",
            "Epoch 202/300.. Learning rate: 0.0121.. Train loss: 1.0501.. Train acc: 0.4665.. Val loss: 1.4585.. Val acc: 0.6229\n",
            "Epoch 203/300.. Learning rate: 0.0118.. Train loss: 1.0366.. Train acc: 0.4978.. Val loss: 1.4648.. Val acc: 0.6287\n",
            "Epoch 204/300.. Learning rate: 0.0116.. Train loss: 1.1382.. Train acc: 0.4289.. Val loss: 1.4519.. Val acc: 0.6399\n",
            "Epoch 205/300.. Learning rate: 0.0114.. Train loss: 1.1482.. Train acc: 0.4312.. Val loss: 1.4787.. Val acc: 0.6363\n",
            "Epoch 206/300.. Learning rate: 0.0112.. Train loss: 1.0304.. Train acc: 0.4143.. Val loss: 1.4440.. Val acc: 0.6291\n",
            "Epoch 207/300.. Learning rate: 0.0110.. Train loss: 1.0402.. Train acc: 0.4768.. Val loss: 1.4302.. Val acc: 0.6399\n",
            "Epoch 208/300.. Learning rate: 0.0107.. Train loss: 1.0353.. Train acc: 0.4232.. Val loss: 1.4995.. Val acc: 0.6294\n",
            "Epoch 209/300.. Learning rate: 0.0105.. Train loss: 1.0008.. Train acc: 0.3802.. Val loss: 1.4285.. Val acc: 0.6435\n",
            "Epoch 210/300.. Learning rate: 0.0103.. Train loss: 1.1525.. Train acc: 0.4638.. Val loss: 1.4377.. Val acc: 0.6340\n",
            "Epoch 211/300.. Learning rate: 0.0101.. Train loss: 1.0882.. Train acc: 0.3890.. Val loss: 1.4165.. Val acc: 0.6435\n",
            "Epoch 212/300.. Learning rate: 0.0099.. Train loss: 1.0973.. Train acc: 0.4297.. Val loss: 1.4402.. Val acc: 0.6359\n",
            "Epoch 213/300.. Learning rate: 0.0097.. Train loss: 1.0700.. Train acc: 0.4666.. Val loss: 1.3827.. Val acc: 0.6486\n",
            "Epoch 214/300.. Learning rate: 0.0095.. Train loss: 0.9728.. Train acc: 0.4557.. Val loss: 1.3750.. Val acc: 0.6576\n",
            "Epoch 215/300.. Learning rate: 0.0093.. Train loss: 1.1023.. Train acc: 0.4279.. Val loss: 1.4172.. Val acc: 0.6421\n",
            "Epoch 216/300.. Learning rate: 0.0091.. Train loss: 0.9964.. Train acc: 0.4628.. Val loss: 1.4154.. Val acc: 0.6405\n",
            "Epoch 217/300.. Learning rate: 0.0089.. Train loss: 0.9992.. Train acc: 0.4182.. Val loss: 1.4436.. Val acc: 0.6459\n",
            "Epoch 218/300.. Learning rate: 0.0087.. Train loss: 0.9185.. Train acc: 0.4365.. Val loss: 1.3777.. Val acc: 0.6523\n",
            "Epoch 219/300.. Learning rate: 0.0085.. Train loss: 1.0009.. Train acc: 0.4109.. Val loss: 1.3847.. Val acc: 0.6522\n",
            "Epoch 220/300.. Learning rate: 0.0083.. Train loss: 0.9557.. Train acc: 0.4636.. Val loss: 1.3969.. Val acc: 0.6552\n",
            "Epoch 221/300.. Learning rate: 0.0081.. Train loss: 1.0820.. Train acc: 0.4305.. Val loss: 1.4203.. Val acc: 0.6433\n",
            "Epoch 222/300.. Learning rate: 0.0079.. Train loss: 0.9240.. Train acc: 0.4523.. Val loss: 1.3718.. Val acc: 0.6528\n",
            "Epoch 223/300.. Learning rate: 0.0077.. Train loss: 0.9609.. Train acc: 0.4583.. Val loss: 1.3565.. Val acc: 0.6593\n",
            "Epoch 224/300.. Learning rate: 0.0075.. Train loss: 1.0171.. Train acc: 0.4402.. Val loss: 1.3847.. Val acc: 0.6495\n",
            "Epoch 225/300.. Learning rate: 0.0073.. Train loss: 0.8652.. Train acc: 0.4478.. Val loss: 1.3548.. Val acc: 0.6612\n",
            "Epoch 226/300.. Learning rate: 0.0071.. Train loss: 0.9512.. Train acc: 0.4703.. Val loss: 1.3481.. Val acc: 0.6614\n",
            "Epoch 227/300.. Learning rate: 0.0070.. Train loss: 0.9812.. Train acc: 0.4416.. Val loss: 1.4083.. Val acc: 0.6533\n",
            "Epoch 228/300.. Learning rate: 0.0068.. Train loss: 0.9094.. Train acc: 0.4316.. Val loss: 1.3683.. Val acc: 0.6591\n",
            "Epoch 229/300.. Learning rate: 0.0066.. Train loss: 0.9186.. Train acc: 0.5041.. Val loss: 1.3419.. Val acc: 0.6616\n",
            "Epoch 230/300.. Learning rate: 0.0064.. Train loss: 0.9859.. Train acc: 0.4367.. Val loss: 1.3840.. Val acc: 0.6611\n",
            "Epoch 231/300.. Learning rate: 0.0062.. Train loss: 0.9150.. Train acc: 0.5236.. Val loss: 1.3917.. Val acc: 0.6611\n",
            "Epoch 232/300.. Learning rate: 0.0061.. Train loss: 1.0469.. Train acc: 0.4659.. Val loss: 1.3440.. Val acc: 0.6614\n",
            "Epoch 233/300.. Learning rate: 0.0059.. Train loss: 0.9503.. Train acc: 0.4637.. Val loss: 1.3493.. Val acc: 0.6614\n",
            "Epoch 234/300.. Learning rate: 0.0057.. Train loss: 0.9711.. Train acc: 0.5033.. Val loss: 1.3525.. Val acc: 0.6664\n",
            "Epoch 235/300.. Learning rate: 0.0056.. Train loss: 0.9494.. Train acc: 0.4565.. Val loss: 1.3167.. Val acc: 0.6745\n",
            "Epoch 236/300.. Learning rate: 0.0054.. Train loss: 0.8320.. Train acc: 0.4711.. Val loss: 1.3268.. Val acc: 0.6719\n",
            "Epoch 237/300.. Learning rate: 0.0052.. Train loss: 0.9205.. Train acc: 0.4689.. Val loss: 1.3666.. Val acc: 0.6621\n",
            "Epoch 238/300.. Learning rate: 0.0051.. Train loss: 0.8438.. Train acc: 0.4641.. Val loss: 1.3347.. Val acc: 0.6641\n",
            "Epoch 239/300.. Learning rate: 0.0049.. Train loss: 0.8899.. Train acc: 0.4576.. Val loss: 1.3384.. Val acc: 0.6680\n",
            "Epoch 240/300.. Learning rate: 0.0048.. Train loss: 0.8630.. Train acc: 0.4850.. Val loss: 1.2999.. Val acc: 0.6744\n",
            "Epoch 241/300.. Learning rate: 0.0046.. Train loss: 0.8511.. Train acc: 0.4783.. Val loss: 1.3038.. Val acc: 0.6710\n",
            "Epoch 242/300.. Learning rate: 0.0045.. Train loss: 0.7889.. Train acc: 0.5422.. Val loss: 1.3099.. Val acc: 0.6759\n",
            "Epoch 243/300.. Learning rate: 0.0043.. Train loss: 0.9154.. Train acc: 0.4592.. Val loss: 1.3381.. Val acc: 0.6722\n",
            "Epoch 244/300.. Learning rate: 0.0042.. Train loss: 0.8410.. Train acc: 0.4558.. Val loss: 1.3000.. Val acc: 0.6772\n",
            "Epoch 245/300.. Learning rate: 0.0040.. Train loss: 1.0223.. Train acc: 0.4335.. Val loss: 1.3236.. Val acc: 0.6710\n",
            "Epoch 246/300.. Learning rate: 0.0039.. Train loss: 0.8827.. Train acc: 0.4738.. Val loss: 1.3151.. Val acc: 0.6787\n",
            "Epoch 247/300.. Learning rate: 0.0038.. Train loss: 0.8141.. Train acc: 0.4486.. Val loss: 1.3002.. Val acc: 0.6766\n",
            "Epoch 248/300.. Learning rate: 0.0036.. Train loss: 0.8789.. Train acc: 0.4738.. Val loss: 1.2856.. Val acc: 0.6815\n",
            "Epoch 249/300.. Learning rate: 0.0035.. Train loss: 0.8673.. Train acc: 0.4799.. Val loss: 1.2691.. Val acc: 0.6839\n",
            "Epoch 250/300.. Learning rate: 0.0034.. Train loss: 0.9214.. Train acc: 0.4984.. Val loss: 1.2804.. Val acc: 0.6794\n",
            "Epoch 251/300.. Learning rate: 0.0032.. Train loss: 0.9149.. Train acc: 0.4780.. Val loss: 1.2896.. Val acc: 0.6816\n",
            "Epoch 252/300.. Learning rate: 0.0031.. Train loss: 0.8914.. Train acc: 0.4604.. Val loss: 1.2934.. Val acc: 0.6822\n",
            "Epoch 253/300.. Learning rate: 0.0030.. Train loss: 0.8299.. Train acc: 0.5000.. Val loss: 1.2653.. Val acc: 0.6875\n",
            "Epoch 254/300.. Learning rate: 0.0028.. Train loss: 0.8668.. Train acc: 0.4785.. Val loss: 1.2873.. Val acc: 0.6819\n",
            "Epoch 255/300.. Learning rate: 0.0027.. Train loss: 0.7613.. Train acc: 0.5121.. Val loss: 1.2679.. Val acc: 0.6869\n",
            "Epoch 256/300.. Learning rate: 0.0026.. Train loss: 0.8450.. Train acc: 0.4827.. Val loss: 1.2651.. Val acc: 0.6813\n",
            "Epoch 257/300.. Learning rate: 0.0025.. Train loss: 0.8516.. Train acc: 0.4608.. Val loss: 1.3028.. Val acc: 0.6808\n",
            "Epoch 258/300.. Learning rate: 0.0024.. Train loss: 0.7475.. Train acc: 0.4996.. Val loss: 1.3106.. Val acc: 0.6834\n",
            "Epoch 259/300.. Learning rate: 0.0023.. Train loss: 0.8870.. Train acc: 0.4085.. Val loss: 1.2619.. Val acc: 0.6846\n",
            "Epoch 260/300.. Learning rate: 0.0022.. Train loss: 0.8460.. Train acc: 0.4729.. Val loss: 1.2730.. Val acc: 0.6835\n",
            "Epoch 261/300.. Learning rate: 0.0021.. Train loss: 0.8575.. Train acc: 0.4596.. Val loss: 1.2952.. Val acc: 0.6823\n",
            "Epoch 262/300.. Learning rate: 0.0020.. Train loss: 0.8887.. Train acc: 0.4876.. Val loss: 1.2872.. Val acc: 0.6823\n",
            "Epoch 263/300.. Learning rate: 0.0019.. Train loss: 0.8608.. Train acc: 0.4751.. Val loss: 1.3130.. Val acc: 0.6831\n",
            "Epoch 264/300.. Learning rate: 0.0018.. Train loss: 0.9253.. Train acc: 0.4838.. Val loss: 1.2351.. Val acc: 0.6935\n",
            "Epoch 265/300.. Learning rate: 0.0017.. Train loss: 0.8858.. Train acc: 0.4222.. Val loss: 1.2848.. Val acc: 0.6843\n",
            "Epoch 266/300.. Learning rate: 0.0016.. Train loss: 0.8663.. Train acc: 0.5062.. Val loss: 1.2558.. Val acc: 0.6887\n",
            "Epoch 267/300.. Learning rate: 0.0015.. Train loss: 0.7777.. Train acc: 0.4491.. Val loss: 1.2325.. Val acc: 0.6910\n",
            "Epoch 268/300.. Learning rate: 0.0014.. Train loss: 0.8008.. Train acc: 0.4841.. Val loss: 1.2614.. Val acc: 0.6897\n",
            "Epoch 269/300.. Learning rate: 0.0013.. Train loss: 0.8566.. Train acc: 0.4730.. Val loss: 1.2755.. Val acc: 0.6919\n",
            "Epoch 270/300.. Learning rate: 0.0012.. Train loss: 0.7751.. Train acc: 0.4925.. Val loss: 1.2411.. Val acc: 0.6911\n",
            "Epoch 271/300.. Learning rate: 0.0011.. Train loss: 0.8661.. Train acc: 0.4938.. Val loss: 1.2461.. Val acc: 0.6923\n",
            "Epoch 272/300.. Learning rate: 0.0011.. Train loss: 0.7620.. Train acc: 0.5132.. Val loss: 1.2611.. Val acc: 0.6938\n",
            "Epoch 273/300.. Learning rate: 0.0010.. Train loss: 0.8533.. Train acc: 0.4491.. Val loss: 1.2334.. Val acc: 0.6938\n",
            "Epoch 274/300.. Learning rate: 0.0009.. Train loss: 0.8060.. Train acc: 0.4295.. Val loss: 1.2398.. Val acc: 0.6939\n",
            "Epoch 275/300.. Learning rate: 0.0009.. Train loss: 0.7427.. Train acc: 0.4766.. Val loss: 1.2566.. Val acc: 0.6924\n",
            "Epoch 276/300.. Learning rate: 0.0008.. Train loss: 0.7468.. Train acc: 0.5050.. Val loss: 1.2382.. Val acc: 0.6981\n",
            "Epoch 277/300.. Learning rate: 0.0007.. Train loss: 0.8402.. Train acc: 0.5058.. Val loss: 1.2662.. Val acc: 0.6919\n",
            "Epoch 278/300.. Learning rate: 0.0007.. Train loss: 0.7500.. Train acc: 0.5091.. Val loss: 1.2241.. Val acc: 0.6969\n",
            "Epoch 279/300.. Learning rate: 0.0006.. Train loss: 0.8201.. Train acc: 0.4673.. Val loss: 1.2837.. Val acc: 0.6892\n",
            "Epoch 280/300.. Learning rate: 0.0005.. Train loss: 0.8275.. Train acc: 0.5525.. Val loss: 1.2239.. Val acc: 0.6969\n",
            "Epoch 281/300.. Learning rate: 0.0005.. Train loss: 0.7888.. Train acc: 0.4577.. Val loss: 1.2186.. Val acc: 0.6989\n",
            "Epoch 282/300.. Learning rate: 0.0004.. Train loss: 0.8036.. Train acc: 0.4618.. Val loss: 1.2642.. Val acc: 0.6957\n",
            "Epoch 283/300.. Learning rate: 0.0004.. Train loss: 0.7403.. Train acc: 0.4753.. Val loss: 1.2242.. Val acc: 0.6959\n",
            "Epoch 284/300.. Learning rate: 0.0004.. Train loss: 0.8391.. Train acc: 0.4625.. Val loss: 1.2787.. Val acc: 0.6896\n",
            "Epoch 285/300.. Learning rate: 0.0003.. Train loss: 0.9186.. Train acc: 0.4560.. Val loss: 1.2423.. Val acc: 0.6959\n",
            "Epoch 286/300.. Learning rate: 0.0003.. Train loss: 0.8205.. Train acc: 0.4890.. Val loss: 1.2258.. Val acc: 0.6963\n",
            "Epoch 287/300.. Learning rate: 0.0002.. Train loss: 0.7637.. Train acc: 0.4693.. Val loss: 1.2380.. Val acc: 0.6927\n",
            "Epoch 288/300.. Learning rate: 0.0002.. Train loss: 0.8394.. Train acc: 0.4803.. Val loss: 1.2423.. Val acc: 0.6943\n",
            "Epoch 289/300.. Learning rate: 0.0002.. Train loss: 0.7635.. Train acc: 0.5075.. Val loss: 1.2408.. Val acc: 0.6970\n",
            "Epoch 290/300.. Learning rate: 0.0001.. Train loss: 0.8680.. Train acc: 0.4815.. Val loss: 1.2181.. Val acc: 0.6992\n",
            "Epoch 291/300.. Learning rate: 0.0001.. Train loss: 0.7415.. Train acc: 0.4285.. Val loss: 1.2164.. Val acc: 0.6976\n",
            "Epoch 292/300.. Learning rate: 0.0001.. Train loss: 0.8470.. Train acc: 0.4465.. Val loss: 1.2383.. Val acc: 0.6984\n",
            "Epoch 293/300.. Learning rate: 0.0001.. Train loss: 0.8120.. Train acc: 0.4732.. Val loss: 1.2275.. Val acc: 0.6948\n",
            "Epoch 294/300.. Learning rate: 0.0001.. Train loss: 0.8236.. Train acc: 0.4754.. Val loss: 1.2299.. Val acc: 0.6970\n",
            "Epoch 295/300.. Learning rate: 0.0000.. Train loss: 0.7864.. Train acc: 0.4785.. Val loss: 1.2382.. Val acc: 0.6948\n",
            "Epoch 296/300.. Learning rate: 0.0000.. Train loss: 0.8265.. Train acc: 0.4996.. Val loss: 1.2291.. Val acc: 0.6956\n",
            "Epoch 297/300.. Learning rate: 0.0000.. Train loss: 0.7608.. Train acc: 0.4669.. Val loss: 1.2386.. Val acc: 0.6928\n",
            "Epoch 298/300.. Learning rate: 0.0000.. Train loss: 0.8434.. Train acc: 0.4512.. Val loss: 1.2633.. Val acc: 0.6956\n",
            "Epoch 299/300.. Learning rate: 0.0000.. Train loss: 0.7590.. Train acc: 0.4965.. Val loss: 1.2782.. Val acc: 0.6917\n",
            "Epoch 300/300.. Learning rate: 0.0000.. Train loss: 0.8321.. Train acc: 0.4841.. Val loss: 1.2223.. Val acc: 0.6983\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_462fb85b-bfba-445f-849c-d7cd7d992111\", \"Accuracy and Loss with mix up of alpha = 0.2.png\", 51486)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss:  tensor(0.0017, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Test acc:  0.7066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ProbabilityDensityFunction(alpha):\n",
        "  randomVariable = beta(alpha, alpha)\n",
        "  observedValue = np.linspace(0, 1, 1000)\n",
        "  plt.plot(observedValue, randomVariable.pdf(observedValue), 'r-', lw=2.6)\n",
        "  plt.title(f'Beta Distribution with alpha=beta={alpha}')\n",
        "  plt.xlabel('Lambda')\n",
        "  plt.ylabel('Probability Density')\n",
        "  plt.ylim(0, 3)\n",
        "  plt.savefig('ProbabilityDensityFunction.png')\n",
        "  files.download('ProbabilityDensityFunction.png')\n",
        "\n",
        "# Plot the beta distribution\n",
        "ProbabilityDensityFunction(0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Ru07ALyHItYc",
        "outputId": "bb6f6580-461d-4dd1-cab0-8c51ca683225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9e3d61c3-655c-49dd-b729-adacfb66f2f3\", \"ProbabilityDensityFunction.png\", 24717)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}